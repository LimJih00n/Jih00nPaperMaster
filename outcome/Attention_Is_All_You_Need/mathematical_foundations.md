# Transformer ì´í•´ë¥¼ ìœ„í•œ ìˆ˜í•™ì  ê¸°ì´ˆ - ì™„ë²½ ê°€ì´ë“œ

## ğŸ“š ëª©ì°¨
1. [ì„ í˜•ëŒ€ìˆ˜ ê¸°ì´ˆ](#1-ì„ í˜•ëŒ€ìˆ˜-ê¸°ì´ˆ)
2. [í™•ë¥ ë¡ ê³¼ ì •ë³´ì´ë¡ ](#2-í™•ë¥ ë¡ ê³¼-ì •ë³´ì´ë¡ )
3. [Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™](#3-attention-ë©”ì»¤ë‹ˆì¦˜ì˜-ìˆ˜í•™)
4. [ìµœì í™” ì´ë¡ ](#4-ìµœì í™”-ì´ë¡ )
5. [ì‹¤ìŠµ ì˜ˆì œ](#5-ì‹¤ìŠµ-ì˜ˆì œ)

---

## 1. ì„ í˜•ëŒ€ìˆ˜ ê¸°ì´ˆ

### 1.1 ë²¡í„°ì™€ í–‰ë ¬ ê¸°ë³¸

#### ë²¡í„° (Vector)
ë²¡í„°ëŠ” ìˆ«ìë“¤ì˜ ìˆœì„œìˆëŠ” ë°°ì—´ì…ë‹ˆë‹¤.

```
ë‹¨ì–´ "cat"ì˜ ì„ë² ë”© ë²¡í„° ì˜ˆì‹œ:
v = [0.2, -0.5, 0.8, 0.1]  (4ì°¨ì› ë²¡í„°)

ì‹¤ì œ Transformerì—ì„œëŠ”:
v âˆˆ â„^512 ë˜ëŠ” â„^768 (512 ë˜ëŠ” 768ì°¨ì›)
```

#### í–‰ë ¬ (Matrix)
í–‰ë ¬ì€ ë²¡í„°ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
ë¬¸ì¥ "I love cats"ì˜ ì„ë² ë”© í–‰ë ¬:
     [0.2  -0.5   0.8]  â† "I"ì˜ ì„ë² ë”©
X =  [0.3   0.7  -0.2]  â† "love"ì˜ ì„ë² ë”©
     [0.1  -0.4   0.9]  â† "cats"ì˜ ì„ë² ë”©

í¬ê¸°: 3Ã—3 (3ê°œ ë‹¨ì–´, 3ì°¨ì› ì„ë² ë”©)
```

### 1.2 í•µì‹¬ ì—°ì‚°

#### í–‰ë ¬ ê³±ì…ˆ (Matrix Multiplication)
**ì´ê²ƒì´ Transformerì˜ í•µì‹¬ì…ë‹ˆë‹¤!**

```
A Ã— B = C

ì˜ˆì‹œ:
[1 2]   [5 6]   [1Ã—5+2Ã—7  1Ã—6+2Ã—8]   [19 22]
[3 4] Ã— [7 8] = [3Ã—5+4Ã—7  3Ã—6+4Ã—8] = [43 50]

Transformerì—ì„œì˜ ì˜ë¯¸:
- Query Ã— Key^T = Attention Score
- ê° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ê³„ì‚°
```

#### ì „ì¹˜ í–‰ë ¬ (Transpose)
í–‰ê³¼ ì—´ì„ ë°”ê¾¸ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.

```
     [1 2 3]        [1 4]
A =  [4 5 6]   A^T = [2 5]
                     [3 6]

Transformerì—ì„œ:
Key^Të¥¼ ë§Œë“¤ì–´ Queryì™€ ê³±í•˜ê¸° ìœ„í•´ ì‚¬ìš©
```

#### ë‚´ì  (Dot Product)
ë‘ ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

```
v1 Â· v2 = v1[0]Ã—v2[0] + v1[1]Ã—v2[1] + ...

ì˜ˆì‹œ:
[1, 2, 3] Â· [4, 5, 6] = 1Ã—4 + 2Ã—5 + 3Ã—6 = 32

ì˜ë¯¸:
- ê°’ì´ í´ìˆ˜ë¡ ë‘ ë²¡í„°ê°€ ìœ ì‚¬
- Attention Score ê³„ì‚°ì˜ ê¸°ì´ˆ
```

### 1.3 ì°¨ì›ê³¼ í¬ê¸°

```
Transformerì˜ ì£¼ìš” ì°¨ì›:
- d_model = 512: ëª¨ë¸ì˜ íˆë“  ì°¨ì›
- d_k = 64: Query/Key ë²¡í„° ì°¨ì›
- d_v = 64: Value ë²¡í„° ì°¨ì›
- h = 8: Attention í—¤ë“œ ìˆ˜

ê´€ê³„: d_model = h Ã— d_k
      512 = 8 Ã— 64
```

---

## 2. í™•ë¥ ë¡ ê³¼ ì •ë³´ì´ë¡ 

### 2.1 Softmax í•¨ìˆ˜
**Attention ê°€ì¤‘ì¹˜ë¥¼ ë§Œë“œëŠ” í•µì‹¬ í•¨ìˆ˜**

```
softmax(x_i) = exp(x_i) / Î£(exp(x_j))

ì˜ˆì‹œ:
ì…ë ¥: [2.0, 1.0, 0.1]
ê³„ì‚°:
- exp(2.0) = 7.39
- exp(1.0) = 2.72  
- exp(0.1) = 1.11
- í•©ê³„ = 11.22

ì¶œë ¥: [0.66, 0.24, 0.10]  (í™•ë¥ ë¡œ ë³€í™˜ë¨)

ì˜ë¯¸:
- ëª¨ë“  ê°’ì˜ í•© = 1.0
- í° ê°’ì€ ë” í¬ê²Œ, ì‘ì€ ê°’ì€ ë” ì‘ê²Œ ë§Œë“¦
- Attentionì—ì„œ "ì£¼ëª©í•  ë‹¨ì–´" ê²°ì •
```

### 2.2 Cross-Entropy Loss
ëª¨ë¸ í•™ìŠµì˜ ëª©í‘œ í•¨ìˆ˜ì…ë‹ˆë‹¤.

```
L = -Î£(y_true Ã— log(y_pred))

ì˜ˆì‹œ:
ì •ë‹µ: "cat" (ì›-í•« ë²¡í„°: [0, 1, 0])
ì˜ˆì¸¡: [0.2, 0.7, 0.1]

Loss = -(0Ã—log(0.2) + 1Ã—log(0.7) + 0Ã—log(0.1))
     = -log(0.7) = 0.36

ì˜ë¯¸:
- ì˜ˆì¸¡ì´ ì •í™•í• ìˆ˜ë¡ Lossê°€ ì‘ìŒ
- ì™„ë²½í•œ ì˜ˆì¸¡: Loss = 0
```

### 2.3 í™•ë¥  ë¶„í¬

```
ì–¸ì–´ ëª¨ë¸ì˜ ì¶œë ¥:
P(ë‹¤ìŒ ë‹¨ì–´ = "cat") = 0.3
P(ë‹¤ìŒ ë‹¨ì–´ = "dog") = 0.2
P(ë‹¤ìŒ ë‹¨ì–´ = "bird") = 0.1
...
í•©ê³„ = 1.0

Perplexity (í˜¼ë€ë„):
PPL = exp(í‰ê·  Loss)
ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸
```

---

## 3. Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™

### 3.1 Scaled Dot-Product Attention
**Transformerì˜ í•µì‹¬ ê³µì‹**

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

#### ë‹¨ê³„ë³„ ë¶„í•´:

**Step 1: Query, Key, Value ìƒì„±**
```
ì…ë ¥ Xê°€ ì£¼ì–´ì¡Œì„ ë•Œ:
Q = X Ã— W_Q  (Query í–‰ë ¬)
K = X Ã— W_K  (Key í–‰ë ¬)
V = X Ã— W_V  (Value í–‰ë ¬)

ì˜ˆì‹œ (3ê°œ ë‹¨ì–´, 4ì°¨ì›):
X = [[x1], [x2], [x3]]  (3Ã—4)
W_Q, W_K, W_VëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ (4Ã—4)
```

**Step 2: Attention Score ê³„ì‚°**
```
Scores = Q Ã— K^T

ì˜ˆì‹œ:
     K1  K2  K3
Q1 [ 10   5   2]  â† Q1ì´ ê° Kì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€
Q2 [  3  12   4]
Q3 [  1   4  15]

ì˜ë¯¸: Q1ì€ K1ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ìŒ (10)
```

**Step 3: ìŠ¤ì¼€ì¼ë§**
```
Scaled_Scores = Scores / âˆšd_k

ì™œ âˆšd_kë¡œ ë‚˜ëˆ„ë‚˜?
- d_kê°€ í´ìˆ˜ë¡ ë‚´ì  ê°’ì´ ì»¤ì§
- Softmaxê°€ í¬í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€
- Gradient vanishing ë°©ì§€

ì˜ˆì‹œ (d_k = 64):
Scores / âˆš64 = Scores / 8
```

**Step 4: Softmax ì ìš©**
```
Attention_Weights = softmax(Scaled_Scores)

     K1    K2    K3
Q1 [0.85  0.10  0.05]  â† Q1ì€ K1ì— 85% ì£¼ëª©
Q2 [0.15  0.70  0.15]  â† Q2ëŠ” K2ì— 70% ì£¼ëª©
Q3 [0.05  0.20  0.75]  â† Q3ëŠ” K3ì— 75% ì£¼ëª©
```

**Step 5: Value ê°€ì¤‘í•©**
```
Output = Attention_Weights Ã— V

ê° Queryì— ëŒ€í•´:
Output_1 = 0.85Ã—V1 + 0.10Ã—V2 + 0.05Ã—V3
(V1ì˜ ì •ë³´ë¥¼ 85% ë°˜ì˜)
```

### 3.2 Multi-Head Attention
**ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì£¼ëª©**

```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Ã— W_O

ê° í—¤ë“œ:
head_i = Attention(QÃ—W_Q^i, KÃ—W_K^i, VÃ—W_V^i)

8ê°œ í—¤ë“œ ì˜ˆì‹œ:
- Head 1: ë¬¸ë²•ì  ê´€ê³„ í•™ìŠµ
- Head 2: ì˜ë¯¸ì  ìœ ì‚¬ì„± í•™ìŠµ
- Head 3: ìœ„ì¹˜ ê´€ê³„ í•™ìŠµ
- ...
```

### 3.3 Positional Encoding
**ìˆœì„œ ì •ë³´ ì¶”ê°€**

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

ì˜ˆì‹œ (pos=0, d_model=4):
PE[0] = [sin(0/1), cos(0/1), sin(0/100), cos(0/100)]
      = [0, 1, 0, 1]

pos=1:
PE[1] = [sin(1/1), cos(1/1), sin(1/100), cos(1/100)]
      = [0.84, 0.54, 0.01, 0.99]

íŠ¹ì§•:
- ê° ìœ„ì¹˜ë§ˆë‹¤ ê³ ìœ í•œ íŒ¨í„´
- ìƒëŒ€ ìœ„ì¹˜ ê³„ì‚° ê°€ëŠ¥
- í•™ìŠµ ì—†ì´ ì„ì˜ ê¸¸ì´ ì²˜ë¦¬
```

---

## 4. ìµœì í™” ì´ë¡ 

### 4.1 Adam Optimizer

```
Adam ì—…ë°ì´íŠ¸ ê·œì¹™:
m_t = Î²1 Ã— m_(t-1) + (1-Î²1) Ã— gradient
v_t = Î²2 Ã— v_(t-1) + (1-Î²2) Ã— gradientÂ²
mÌ‚_t = m_t / (1 - Î²1^t)
vÌ‚_t = v_t / (1 - Î²2^t)
Î¸_t = Î¸_(t-1) - Î± Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)

Transformer ì„¤ì •:
- Î²1 = 0.9 (momentum)
- Î²2 = 0.98 (RMSprop)
- Îµ = 10^-9 (numerical stability)
```

### 4.2 Learning Rate Schedule

```
lr = d_model^(-0.5) Ã— min(step^(-0.5), step Ã— warmup^(-1.5))

Warmup ì˜ˆì‹œ (warmup=4000):
- Step 1000: lr ì¦ê°€ ì¤‘
- Step 4000: lr ìµœëŒ€
- Step 8000: lr ê°ì†Œ ì‹œì‘

ì´ìœ :
- ì´ˆê¸°: ì‘ì€ lrë¡œ ì•ˆì •ì  ì‹œì‘
- ì¤‘ê°„: ë¹ ë¥¸ í•™ìŠµ
- í›„ê¸°: ë¯¸ì„¸ ì¡°ì •
```

### 4.3 Gradient Clipping

```
if ||gradient|| > threshold:
    gradient = gradient Ã— (threshold / ||gradient||)

ì˜ˆì‹œ:
- Gradient norm = 15
- Threshold = 1.0
- Clipped gradient = gradient Ã— (1.0/15)

íš¨ê³¼: Gradient explosion ë°©ì§€
```

---

## 5. ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: ê°„ë‹¨í•œ Attention ê³„ì‚°

```python
import numpy as np

# 3ê°œ ë‹¨ì–´, 4ì°¨ì› ì„ë² ë”©
Q = np.array([[1, 0, 1, 0],
              [0, 1, 0, 1],
              [1, 1, 0, 0]])

K = np.array([[1, 1, 0, 0],
              [0, 0, 1, 1],
              [1, 0, 1, 0]])

V = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12]])

# Step 1: QK^T ê³„ì‚°
scores = np.matmul(Q, K.T)
print("Attention Scores:")
print(scores)
# [[2, 1, 2],
#  [1, 2, 1],
#  [2, 0, 1]]

# Step 2: ìŠ¤ì¼€ì¼ë§
d_k = 4
scaled_scores = scores / np.sqrt(d_k)

# Step 3: Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

attention_weights = softmax(scaled_scores)
print("\nAttention Weights:")
print(attention_weights)

# Step 4: Value ê°€ì¤‘í•©
output = np.matmul(attention_weights, V)
print("\nOutput:")
print(output)
```

### ì˜ˆì œ 2: Positional Encoding ì‹œê°í™”

```python
def get_positional_encoding(seq_len, d_model):
    PE = np.zeros((seq_len, d_model))
    
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            PE[pos, i] = np.sin(pos / (10000 ** (i/d_model)))
            if i+1 < d_model:
                PE[pos, i+1] = np.cos(pos / (10000 ** (i/d_model)))
    
    return PE

# 10ê°œ ìœ„ì¹˜, 8ì°¨ì›
PE = get_positional_encoding(10, 8)

# ê° ìœ„ì¹˜ì˜ ê³ ìœ  íŒ¨í„´ í™•ì¸
import matplotlib.pyplot as plt
plt.imshow(PE, cmap='RdBu')
plt.xlabel('Dimension')
plt.ylabel('Position')
plt.title('Positional Encoding Pattern')
plt.colorbar()
```

### ì˜ˆì œ 3: Multi-Head ë¶„í•´

```python
# 512ì°¨ì›ì„ 8ê°œ í—¤ë“œë¡œ ë¶„í• 
d_model = 512
num_heads = 8
d_k = d_model // num_heads  # 64

# ì…ë ¥ (10ê°œ ë‹¨ì–´, 512ì°¨ì›)
X = np.random.randn(10, d_model)

# 8ê°œ í—¤ë“œë¡œ reshape
X_multihead = X.reshape(10, num_heads, d_k)

print(f"ì›ë³¸ í¬ê¸°: {X.shape}")
print(f"Multi-head í¬ê¸°: {X_multihead.shape}")
# ì›ë³¸: (10, 512)
# Multi-head: (10, 8, 64)

# ê° í—¤ë“œëŠ” 64ì°¨ì› ê³µê°„ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ attention ê³„ì‚°
```

---

## ğŸ“ í•µì‹¬ ìš”ì•½

### ê¼­ ì´í•´í•´ì•¼ í•  ìˆ˜í•™ ê°œë…

1. **í–‰ë ¬ ê³±ì…ˆ**: Attentionì˜ ëª¨ë“  ê³„ì‚° ê¸°ì´ˆ
2. **Softmax**: ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
3. **Scaled Dot-Product**: Attentionì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜
4. **Gradientì™€ Backpropagation**: í•™ìŠµì˜ ì›ë¦¬

### ìˆ˜í•™ ê³µë¶€ ë¡œë“œë§µ

1. **Week 1-2**: ì„ í˜•ëŒ€ìˆ˜ ê¸°ì´ˆ (ë²¡í„°, í–‰ë ¬, ë‚´ì )
2. **Week 3-4**: í™•ë¥ ë¡  (í™•ë¥ ë¶„í¬, ì¡°ê±´ë¶€ í™•ë¥ )
3. **Week 5-6**: ë¯¸ì ë¶„ (í¸ë¯¸ë¶„, ì²´ì¸ë£°)
4. **Week 7-8**: Transformer ìˆ˜ì‹ ì§ì ‘ êµ¬í˜„

### ì¶”ì²œ í•™ìŠµ ìë£Œ

- **Khan Academy**: ì„ í˜•ëŒ€ìˆ˜, ë¯¸ì ë¶„ ê¸°ì´ˆ
- **3Blue1Brown**: ì‹œê°ì  ìˆ˜í•™ ì´í•´
- **Andrew Ng's Course**: ë”¥ëŸ¬ë‹ ìˆ˜í•™
- **The Annotated Transformer**: ì½”ë“œë¡œ ë°°ìš°ëŠ” ìˆ˜í•™

### ì‹¤ìŠµ íŒ

1. **NumPyë¡œ ì§ì ‘ êµ¬í˜„**: ìˆ˜ì‹ì„ ì½”ë“œë¡œ ì˜®ê²¨ë³´ê¸°
2. **ì‘ì€ ì˜ˆì œë¶€í„°**: 3Ã—3 í–‰ë ¬ë¡œ ì‹œì‘
3. **ì‹œê°í™”**: matplotlibìœ¼ë¡œ attention íŒ¨í„´ ê·¸ë¦¬ê¸°
4. **ë””ë²„ê¹…**: ê° ë‹¨ê³„ì˜ shape í™•ì¸ ìŠµê´€í™”