# STEPER - êµ¬í˜„ ê°€ì´ë“œ

## ğŸ” ë‹¨ê³„ë³„ ë¯¸ë‹ˆ êµ¬í˜„

### Step 1: ê¸°ë³¸ STEPER í”„ë ˆì„ì›Œí¬ êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Dict, Tuple

class STEPERFramework(nn.Module):
    def __init__(self, model_name: str = "meta-llama/Llama-3.1-8B-Instruct"):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Difficulty-aware íŒŒë¼ë¯¸í„° (í•™ìŠµ ê°€ëŠ¥)
        self.sigma_init = nn.Parameter(torch.tensor(1.0))    # Reasoning Initialization
        self.sigma_exp = nn.Parameter(torch.tensor(1.0))     # Reasoning Expansion  
        self.sigma_agg = nn.Parameter(torch.tensor(1.0))     # Reasoning Aggregation
        
        print(f"âœ… STEPER ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.parameters()):,}")
        
    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.model(input_ids=input_ids, 
                           attention_mask=attention_mask, 
                           labels=labels)
        return outputs

# ì˜ˆì‹œ ì‚¬ìš©
steper_model = STEPERFramework()
```

### Step 2: Step-wise ë°ì´í„°ì…‹ êµ¬ì„±ê¸°

```python
class StepwiseDatasetBuilder:
    def __init__(self, teacher_model, retriever, max_steps: int = 5):
        self.teacher_model = teacher_model
        self.retriever = retriever  # BM25 ë˜ëŠ” ë‹¤ë¥¸ retrieval ëª¨ë¸
        self.max_steps = max_steps
        
    def create_stepwise_sample(self, question: str, answer: str) -> List[Dict]:
        """ë‹¨ì¼ QA ìŒì„ step-wise ìƒ˜í”Œë“¤ë¡œ ë³€í™˜"""
        stepwise_samples = []
        
        # Step 1: Reasoning Initialization
        P1 = self.retriever.search(question, k=4)
        R1 = self.teacher_model.generate_rationale(question, P1)
        
        stepwise_samples.append({
            'type': 'reasoning_initialization',
            'input': f"Question: {question}\nPassages: {P1}",
            'output': R1,
            'step': 1
        })
        
        # Steps 2 to S-1: Reasoning Expansion
        passages_cumulative = P1
        rationales_cumulative = [R1]
        
        for step in range(2, self.max_steps):
            # ì´ì „ rationale ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰
            query = self._construct_step_query(question, rationales_cumulative)
            new_passages = self.retriever.search(query, k=4)
            passages_cumulative.extend(new_passages)
            
            # ìƒˆë¡œìš´ rationale ìƒì„±
            context = f"Question: {question}\nPassages: {passages_cumulative}\nPrevious reasoning: {' '.join(rationales_cumulative)}"
            R_step = self.teacher_model.generate_rationale(context)
            
            # Answer flag ì²´í¬ (ì¡°ê¸° ì¢…ë£Œ)
            if "So the answer is:" in R_step:
                break
                
            rationales_cumulative.append(R_step)
            stepwise_samples.append({
                'type': 'reasoning_expansion', 
                'input': context,
                'output': R_step,
                'step': step
            })
            
        # Final Step: Reasoning Aggregation
        final_context = f"Question: {question}\nPassages: {passages_cumulative}\nReasoning chain: {' '.join(rationales_cumulative)}"
        final_output = f"{' '.join(rationales_cumulative)} So the answer is: {answer}"
        
        stepwise_samples.append({
            'type': 'reasoning_aggregation',
            'input': final_context, 
            'output': final_output,
            'step': len(rationales_cumulative) + 1
        })
        
        return stepwise_samples
    
    def _construct_step_query(self, question: str, previous_rationales: List[str]) -> str:
        """ì´ì „ ì¶”ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        last_rationale = previous_rationales[-1] if previous_rationales else ""
        # ê°„ë‹¨í•œ êµ¬í˜„: ë§ˆì§€ë§‰ rationaleì˜ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ
        return f"{question} {last_rationale}"

# ì˜ˆì‹œ ì‚¬ìš©
dataset_builder = StepwiseDatasetBuilder(teacher_model, retriever)
sample = dataset_builder.create_stepwise_sample(
    question="Jim Halsey guided the career of the musician who hosted what country variety show?",
    answer="Hee Haw"
)

print("ğŸ“‹ ìƒì„±ëœ Step-wise ìƒ˜í”Œë“¤:")
for i, s in enumerate(sample):
    print(f"Step {s['step']} ({s['type']}): {s['output'][:50]}...")
```

### Step 3: Multi-Task Loss êµ¬í˜„

```python
class STEPERLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)
        
    def forward(self, model_outputs, batch_data, sigma_params):
        """
        Args:
            model_outputs: ëª¨ë¸ ì¶œë ¥ 
            batch_data: ë°°ì¹˜ ë°ì´í„° (step_type ì •ë³´ í¬í•¨)
            sigma_params: (sigma_init, sigma_exp, sigma_agg) ë‚œì´ë„ íŒŒë¼ë¯¸í„°
        """
        sigma_init, sigma_exp, sigma_agg = sigma_params
        
        total_loss = 0.0
        step_losses = {'init': [], 'exp': [], 'agg': []}
        
        for i, sample in enumerate(batch_data):
            step_type = sample['step_type']  # 'reasoning_initialization', 'reasoning_expansion', 'reasoning_aggregation'
            
            # ê°œë³„ ìƒ˜í”Œ loss ê³„ì‚°
            sample_loss = self.ce_loss(model_outputs.logits[i], sample['labels'])
            
            # Step typeë³„ loss ëˆ„ì 
            if step_type == 'reasoning_initialization':
                step_losses['init'].append(sample_loss)
            elif step_type == 'reasoning_expansion':
                step_losses['exp'].append(sample_loss) 
            elif step_type == 'reasoning_aggregation':
                step_losses['agg'].append(sample_loss)
        
        # ê° íƒ€ì…ë³„ í‰ê·  loss ê³„ì‚°
        L_init = torch.stack(step_losses['init']).mean() if step_losses['init'] else 0.0
        L_exp = torch.stack(step_losses['exp']).mean() if step_losses['exp'] else 0.0  
        L_agg = torch.stack(step_losses['agg']).mean() if step_losses['agg'] else 0.0
        
        # Difficulty-aware weighting ì ìš©
        weighted_loss = (
            (1.0 / (2 * sigma_init**2)) * L_init + torch.log(sigma_init) +
            (1.0 / (2 * sigma_exp**2)) * L_exp + torch.log(sigma_exp) +  
            (1.0 / (2 * sigma_agg**2)) * L_agg + torch.log(sigma_agg)
        )
        
        return {
            'total_loss': weighted_loss,
            'L_init': L_init,
            'L_exp': L_exp, 
            'L_agg': L_agg,
            'sigma_init': sigma_init.item(),
            'sigma_exp': sigma_exp.item(),
            'sigma_agg': sigma_agg.item()
        }

# ì˜ˆì‹œ ì‚¬ìš© 
loss_fn = STEPERLoss()
```

### Step 4: í›ˆë ¨ ë£¨í”„ êµ¬í˜„

```python
def train_steper(model, dataloader, num_epochs=2, lr=5e-6):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    loss_fn = STEPERLoss()
    
    model.train()
    
    for epoch in range(num_epochs):
        epoch_losses = []
        sigma_history = {'init': [], 'exp': [], 'agg': []}
        
        for batch_idx, batch in enumerate(dataloader):
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'], 
                labels=batch['labels']
            )
            
            # Loss ê³„ì‚°
            sigma_params = (model.sigma_init, model.sigma_exp, model.sigma_agg)
            loss_dict = loss_fn(outputs, batch['step_data'], sigma_params)
            
            # Backward pass
            loss_dict['total_loss'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()
            
            # Logging
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}")
                print(f"  Total Loss: {loss_dict['total_loss']:.4f}")
                print(f"  Ïƒ_init: {loss_dict['sigma_init']:.3f}, Ïƒ_exp: {loss_dict['sigma_exp']:.3f}, Ïƒ_agg: {loss_dict['sigma_agg']:.3f}")
                
            epoch_losses.append(loss_dict['total_loss'].item())
            sigma_history['init'].append(loss_dict['sigma_init'])
            sigma_history['exp'].append(loss_dict['sigma_exp'])
            sigma_history['agg'].append(loss_dict['sigma_agg'])
            
        scheduler.step()
        print(f"âœ… Epoch {epoch} ì™„ë£Œ, Average Loss: {np.mean(epoch_losses):.4f}")
        
    return model, sigma_history

# í›ˆë ¨ ì‹¤í–‰
trained_model, sigma_hist = train_steper(steper_model, train_dataloader)
```

### Step 5: ì¶”ë¡  ë° ì‹œê°í™”

```python
def steper_inference(model, tokenizer, question: str, retriever, max_steps: int = 5):
    """STEPER ëª¨ë¸ë¡œ ë‹¨ê³„ë³„ ì¶”ë¡  ìˆ˜í–‰"""
    model.eval()
    
    reasoning_chain = []
    passages_cumulative = []
    
    with torch.no_grad():
        for step in range(1, max_steps + 1):
            # ê²€ìƒ‰ ìˆ˜í–‰
            if step == 1:
                query = question
            else:
                query = f"{question} {reasoning_chain[-1]}"  # ì´ì „ ì¶”ë¡  ê¸°ë°˜
                
            new_passages = retriever.search(query, k=4)
            passages_cumulative.extend(new_passages)
            
            # ì…ë ¥ êµ¬ì„±
            if step == 1:
                context = f"Question: {question}\nPassages: {' '.join(passages_cumulative[:4])}\nAnswer step by step:"
            else:
                context = f"Question: {question}\nPassages: {' '.join(passages_cumulative)}\nPrevious reasoning: {' '.join(reasoning_chain)}\nContinue reasoning:"
                
            # í† í¬ë‚˜ì´ì§• ë° ìƒì„±
            inputs = tokenizer(context, return_tensors="pt", max_length=1024, truncation=True)
            outputs = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
            
            # ë””ì½”ë”©
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            new_reasoning = generated_text[len(context):].strip()
            reasoning_chain.append(new_reasoning)
            
            print(f"ğŸ” Step {step}: {new_reasoning}")
            
            # ì¢…ë£Œ ì¡°ê±´ ì²´í¬
            if "So the answer is:" in new_reasoning:
                break
                
    return reasoning_chain, passages_cumulative

# ì˜ˆì‹œ ì‚¬ìš©
question = "Jim Halsey guided the career of the musician who hosted what country variety show?"
reasoning_chain, passages = steper_inference(trained_model, tokenizer, question, retriever)

print("\nğŸ“ ìµœì¢… ì¶”ë¡  ì²´ì¸:")
for i, reasoning in enumerate(reasoning_chain, 1):
    print(f"Step {i}: {reasoning}")
```

### Step 6: ì„±ëŠ¥ ë¶„ì„ ë° ì‹œê°í™”

```python
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_sigma_evolution(sigma_history):
    """Sigma íŒŒë¼ë¯¸í„° ë³€í™” ì‹œê°í™”"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Sigma ê°’ ë³€í™” ì¶”ì´
    steps = range(len(sigma_history['init']))
    ax1.plot(steps, sigma_history['init'], label='Ïƒ_init (Initialization)', color='blue')
    ax1.plot(steps, sigma_history['exp'], label='Ïƒ_exp (Expansion)', color='orange') 
    ax1.plot(steps, sigma_history['agg'], label='Ïƒ_agg (Aggregation)', color='green')
    ax1.set_xlabel('Training Steps')
    ax1.set_ylabel('Sigma Values')
    ax1.set_title('Difficulty Parameter Evolution')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ìµœì¢… sigma ê°’ ë¹„êµ
    final_sigmas = [sigma_history['init'][-1], sigma_history['exp'][-1], sigma_history['agg'][-1]]
    labels = ['Initialization', 'Expansion', 'Aggregation']
    colors = ['blue', 'orange', 'green']
    
    bars = ax2.bar(labels, final_sigmas, color=colors, alpha=0.7)
    ax2.set_ylabel('Final Sigma Value')
    ax2.set_title('Task Difficulty Comparison')
    
    # ê°’ í‘œì‹œ
    for bar, value in zip(bars, final_sigmas):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    # í•´ì„ ì¶œë ¥
    most_difficult = labels[np.argmax(final_sigmas)]
    least_difficult = labels[np.argmin(final_sigmas)]
    print(f"ğŸ” ë¶„ì„ ê²°ê³¼:")
    print(f"  ê°€ì¥ ì–´ë ¤ìš´ íƒœìŠ¤í¬: {most_difficult} (Ïƒ={max(final_sigmas):.3f})")
    print(f"  ê°€ì¥ ì‰¬ìš´ íƒœìŠ¤í¬: {least_difficult} (Ïƒ={min(final_sigmas):.3f})")

def evaluate_step_performance(model, test_dataloader):
    """ë‹¨ê³„ë³„ ì„±ëŠ¥ í‰ê°€"""
    model.eval()
    
    step_accuracies = {'init': [], 'exp': [], 'agg': []}
    
    with torch.no_grad():
        for batch in test_dataloader:
            outputs = model(batch['input_ids'], batch['attention_mask'])
            predictions = torch.argmax(outputs.logits, dim=-1)
            
            for i, sample in enumerate(batch['step_data']):
                step_type = sample['step_type']
                correct = (predictions[i] == sample['labels']).float().mean()
                
                if step_type == 'reasoning_initialization':
                    step_accuracies['init'].append(correct.item())
                elif step_type == 'reasoning_expansion':
                    step_accuracies['exp'].append(correct.item())
                elif step_type == 'reasoning_aggregation':
                    step_accuracies['agg'].append(correct.item())
    
    # ê²°ê³¼ ì¶œë ¥
    for step_type, accuracies in step_accuracies.items():
        avg_acc = np.mean(accuracies)
        print(f"ğŸ“Š {step_type.capitalize()} Accuracy: {avg_acc:.3f}")
    
    return step_accuracies

# ë¶„ì„ ì‹¤í–‰
analyze_sigma_evolution(sigma_hist)
step_perfs = evaluate_step_performance(trained_model, test_dataloader)
```

## ğŸ“Š ì„±ëŠ¥ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§
```python
# ì •ìƒì ì¸ í•™ìŠµ ì§€í‘œ
training_checkpoints = {
    "epoch_0": {
        "total_loss": 8.5, 
        "sigma_init": 1.0, "sigma_exp": 1.0, "sigma_agg": 1.0,
        "step_accuracy": {"init": 0.3, "exp": 0.2, "agg": 0.4}
    },
    "epoch_1": {
        "total_loss": 4.2,
        "sigma_init": 0.8, "sigma_exp": 1.2, "sigma_agg": 0.9, 
        "step_accuracy": {"init": 0.6, "exp": 0.4, "agg": 0.7}
    },
    "epoch_2": {
        "total_loss": 1.8,
        "sigma_init": 0.7, "sigma_exp": 1.4, "sigma_agg": 0.8,
        "step_accuracy": {"init": 0.8, "exp": 0.6, "agg": 0.9}
    }
}

print("ğŸ¯ í•™ìŠµ ì§„í–‰ë„ ì²´í¬:")
print("âœ… Loss ê°ì†Œ: 8.5 â†’ 4.2 â†’ 1.8")
print("âœ… Sigma ì ì‘: Expansionì´ ê°€ì¥ ì–´ë ¤ìš´ íƒœìŠ¤í¬ë¡œ ì¸ì‹ë¨")
print("âœ… ì„±ëŠ¥ í–¥ìƒ: ëª¨ë“  ë‹¨ê³„ì—ì„œ ì§€ì†ì  ê°œì„ ")
```

## ğŸš€ ìµœì í™” íŒ

### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **Gradient Checkpointing**: ë©”ëª¨ë¦¬ 50% ì ˆì•½
- **Mixed Precision**: FP16 ì‚¬ìš©ìœ¼ë¡œ ì†ë„ 2ë°° í–¥ìƒ
- **DeepSpeed ZeRO**: ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¶„ì‚° í•™ìŠµ

### 2. ìˆ˜ë ´ ì•ˆì •ì„±
- **Gradient Clipping**: `max_norm=1.0`ìœ¼ë¡œ í­ì£¼ ë°©ì§€
- **Warmup Scheduler**: ì´ˆê¸° ë¶ˆì•ˆì •ì„± ì™„í™”
- **Early Stopping**: Validation loss ëª¨ë‹ˆí„°ë§

### 3. ì„±ëŠ¥ ìµœì í™”
- **Teacher Forcing**: í›ˆë ¨ ì‹œ ì‹¤ì œ ì´ì „ ë‹¨ê³„ ì‚¬ìš©
- **Curriculum Learning**: ì‰¬ìš´ ìƒ˜í”Œë¶€í„° ì ì§„ì  í•™ìŠµ
- **Data Augmentation**: ë‹¤ì–‘í•œ ì¶”ë¡  ê²½ë¡œ ìƒì„±

ì´ êµ¬í˜„ ê°€ì´ë“œë¥¼ í†µí•´ STEPERì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤ì œ ì‘ë™í•˜ëŠ” ì½”ë“œë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯