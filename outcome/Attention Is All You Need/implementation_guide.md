# Attention Is All You Need - ì‹¤ì œ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ” ë‹¨ê³„ë³„ ë¯¸ë‹ˆ êµ¬í˜„

### Step 1: ê¸°ë³¸ Scaled Dot-Product Attention êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import matplotlib.pyplot as plt
import seaborn as sns

class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attentionì˜ ê°€ì¥ ê¸°ë³¸ êµ¬í˜„
    ì°¨ì› ì¶”ì ê³¼ ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì£¼ì„ í¬í•¨
    """
    def __init__(self, d_k, dropout=0.1):
        super().__init__()
        self.d_k = d_k
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, Q, K, V, mask=None):
        # ì…ë ¥ ì°¨ì› í™•ì¸
        batch_size, seq_len, d_k = Q.size()
        print(f"ğŸ“Š ì…ë ¥ ì°¨ì›:")
        print(f"  Q: {Q.shape} = [ë°°ì¹˜={batch_size}, ì‹œí€€ìŠ¤={seq_len}, d_k={d_k}]")
        print(f"  K: {K.shape}")  
        print(f"  V: {V.shape}")
        
        # Step 1: QK^T ê³„ì‚° (attention scores)
        scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, seq_len, seq_len]
        print(f"  QK^T: {scores.shape} = attention score matrix")
        
        # Step 2: Scale by âˆšd_k
        scores = scores / math.sqrt(self.d_k)
        print(f"  Scaled scores: {scores.shape} (divided by âˆš{self.d_k} = {math.sqrt(self.d_k):.2f})")
        
        # Step 3: Apply mask (if provided)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            print(f"  Masked scores: {scores.shape}")
            
        # Step 4: Softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        print(f"  Attention weights: {attention_weights.shape} = í™•ë¥  ë¶„í¬")
        
        # Step 5: Apply weights to V
        output = torch.matmul(attention_weights, V)  # [batch, seq_len, d_k]
        print(f"  Final output: {output.shape} = weighted sum of values")
        
        return output, attention_weights

# ğŸ§ª ì‹¤ì œ í…ŒìŠ¤íŠ¸: "I love you" ì˜ˆì‹œ
def test_basic_attention():
    print("ğŸ”¬ Basic Attention Test: 'I love you'")
    print("=" * 50)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    batch_size, seq_len, d_k = 1, 3, 64
    
    # ê°€ìƒì˜ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ì„ë² ë”© ì‚¬ìš©)
    # ì˜ë„ì ìœ¼ë¡œ íŒ¨í„´ì„ ë„£ì–´ì„œ attention ê²°ê³¼ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê²Œ í•¨
    embeddings = torch.randn(batch_size, seq_len, d_k)
    
    # Q, K, Vë¥¼ ë™ì¼í•˜ê²Œ ì„¤ì • (Self-Attention)
    Q = K = V = embeddings
    
    # Attention ê³„ì‚°
    attention = ScaledDotProductAttention(d_k)
    output, weights = attention(Q, K, V)
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“ˆ Attention Matrix:")
    print(f"Row i = Query iê°€ ë‹¤ë¥¸ Keyë“¤ì— ì£¼ëŠ” attention")
    tokens = ["I", "love", "you"]
    
    print(f"\n{'':>8}", end="")
    for token in tokens:
        print(f"{token:>8}", end="")
    print()
    
    for i, token in enumerate(tokens):
        print(f"{token:>8}", end="")
        for j in range(len(tokens)):
            print(f"{weights[0, i, j].item():>8.3f}", end="")
        print()
    
    return output, weights
```

### Step 2: Multi-Head Attention êµ¬í˜„

```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attentionì˜ ì™„ì „í•œ êµ¬í˜„
    8ê°œì˜ ë³‘ë ¬ attention head ì‚¬ìš©
    """
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0  # d_modelì´ n_headsë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì ¸ì•¼ í•¨
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # ê° headì˜ ì°¨ì›
        
        print(f"ğŸ§  Multi-Head Attention ì´ˆê¸°í™”:")
        print(f"  ì „ì²´ ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"  Head ê°œìˆ˜: {n_heads}")  
        print(f"  ê° Head ì°¨ì›: {self.d_k}")
        
        # Q, K, V ë³€í™˜ì„ ìœ„í•œ ì„ í˜•ì¸µë“¤
        self.W_Q = nn.Linear(d_model, d_model)  # [512, 512] - 8ê°œ head * 64ì°¨ì›
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        
        # ìµœì¢… ì¶œë ¥ ë³€í™˜
        self.W_O = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k, dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size, seq_len, d_model = query.size()
        
        print(f"\nğŸ”„ Multi-Head Attention Forward:")
        print(f"  ì…ë ¥: [{batch_size}, {seq_len}, {d_model}]")
        
        # Step 1: Q, K, V ë³€í™˜
        Q = self.W_Q(query)  # [batch, seq_len, d_model]
        K = self.W_K(key)    # [batch, seq_len, d_model]  
        V = self.W_V(value)  # [batch, seq_len, d_model]
        
        print(f"  W_Q, W_K, W_V ë³€í™˜ ì™„ë£Œ: {Q.shape}")
        
        # Step 2: Multi-headë¥¼ ìœ„í•´ reshape
        # [batch, seq_len, d_model] â†’ [batch, seq_len, n_heads, d_k] â†’ [batch, n_heads, seq_len, d_k]
        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  
        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        print(f"  Multi-head reshape: {Q.shape} = [batch, n_heads, seq_len, d_k]")
        
        # Step 3: ê° headë³„ë¡œ attention ê³„ì‚°
        if mask is not None:
            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)
            
        attention_output, attention_weights = self.attention(Q, K, V, mask)
        print(f"  ê° Head attention ì™„ë£Œ: {attention_output.shape}")
        
        # Step 4: Headë“¤ concatenate
        # [batch, n_heads, seq_len, d_k] â†’ [batch, seq_len, n_heads, d_k] â†’ [batch, seq_len, d_model]
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        print(f"  Head concatenation: {attention_output.shape}")
        
        # Step 5: ìµœì¢… ì¶œë ¥ ë³€í™˜
        output = self.W_O(attention_output)
        print(f"  ìµœì¢… ì¶œë ¥: {output.shape}")
        
        return output, attention_weights

# ğŸ§ª Multi-Head Attention í…ŒìŠ¤íŠ¸
def test_multihead_attention():
    print("\nğŸ”¬ Multi-Head Attention Test")
    print("=" * 50)
    
    batch_size, seq_len, d_model = 2, 4, 512
    
    # ì…ë ¥ ìƒì„± ("The cat sat on")
    x = torch.randn(batch_size, seq_len, d_model)
    
    # Multi-Head Attention
    mha = MultiHeadAttention(d_model=512, n_heads=8)
    output, weights = mha(x, x, x)  # Self-attention
    
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  ì…ë ¥: {x.shape}")
    print(f"  ì¶œë ¥: {output.shape}")  
    print(f"  Attention weights: {weights.shape}")
    
    return output, weights
```

### Step 3: Positional Encoding êµ¬í˜„

```python
class PositionalEncoding(nn.Module):
    """
    ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ìœ„ì¹˜ ì¸ì½”ë”©
    'I love you' ì˜ˆì‹œë¡œ ì‹¤ì œ ê°’ ê³„ì‚°í•´ë³´ê¸°
    """
    def __init__(self, d_model=512, max_len=5000):
        super().__init__()
        self.d_model = d_model
        
        # ìœ„ì¹˜ ì¸ì½”ë”© í…Œì´ë¸” ë¯¸ë¦¬ ê³„ì‚°
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        # div_term = 10000^(2i/d_model) for i = 0, 1, 2, ..., d_model//2
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        print(f"ğŸ“ Positional Encoding ìƒì„±:")
        print(f"  ìµœëŒ€ ê¸¸ì´: {max_len}")
        print(f"  ëª¨ë¸ ì°¨ì›: {d_model}")
        print(f"  div_term ìƒ˜í”Œ: {div_term[:5]} (ì²˜ìŒ 5ê°œ ì£¼íŒŒìˆ˜)")
        
        # ì§ìˆ˜ ì¸ë±ìŠ¤: sin, í™€ìˆ˜ ì¸ë±ìŠ¤: cos
        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤
        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤
        
        # [max_len, d_model] â†’ [1, max_len, d_model] (batch dimension ì¶”ê°€)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)  # í•™ìŠµë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        print(f"\nâ° Positional Encoding ì ìš©:")
        print(f"  ì…ë ¥ ì„ë² ë”©: {x.shape}")
        
        # í•´ë‹¹ ê¸¸ì´ë§Œí¼ì˜ ìœ„ì¹˜ ì¸ì½”ë”© ì„ íƒ
        pos_encoding = self.pe[:, :seq_len, :]
        print(f"  ìœ„ì¹˜ ì¸ì½”ë”©: {pos_encoding.shape}")
        
        # ì›ë³¸ ì„ë² ë”©ì— ìœ„ì¹˜ ì •ë³´ ë”í•˜ê¸°
        output = x + pos_encoding
        print(f"  ìµœì¢… ì¶œë ¥: {output.shape} = ì„ë² ë”© + ìœ„ì¹˜ì •ë³´")
        
        return output
    
    def visualize_positions(self, max_pos=10):
        """ìœ„ì¹˜ë³„ ì¸ì½”ë”© ì‹œê°í™”"""
        print(f"\nğŸ¨ ìœ„ì¹˜ ì¸ì½”ë”© ì‹œê°í™” (ì²˜ìŒ {max_pos}ê°œ ìœ„ì¹˜)")
        
        # ì²˜ìŒ ëª‡ ê°œ ìœ„ì¹˜ì˜ ì¸ì½”ë”© ê°’ë“¤
        positions_to_show = min(max_pos, self.pe.size(1))
        encoding_sample = self.pe[0, :positions_to_show, :16]  # ì²˜ìŒ 16ì°¨ì›ë§Œ
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(encoding_sample.numpy(), 
                   cmap='RdBu', center=0,
                   xticklabels=[f"dim{i}" for i in range(16)],
                   yticklabels=[f"pos{i}" for i in range(positions_to_show)])
        plt.title('Positional Encoding Visualization\n(ì²˜ìŒ 10ê°œ ìœ„ì¹˜, 16ê°œ ì°¨ì›)')
        plt.ylabel('Position')
        plt.xlabel('Embedding Dimension')
        plt.tight_layout()
        plt.show()

# ğŸ§ª "I love you" ìœ„ì¹˜ ì¸ì½”ë”© ì‹¤í—˜
def test_positional_encoding():
    print("ğŸ”¬ Positional Encoding Test: 'I love you'")
    print("=" * 60)
    
    # íŒŒë¼ë¯¸í„° ì„¤ì •
    batch_size, seq_len, d_model = 1, 3, 512  # "I love you" = 3ê°œ í† í°
    
    # ê°€ìƒì˜ ë‹¨ì–´ ì„ë² ë”© (ì˜ë¯¸ ì •ë³´ë§Œ)
    word_embeddings = torch.randn(batch_size, seq_len, d_model)
    tokens = ["I", "love", "you"]
    
    print(f"ğŸ’­ ì›ë³¸ ë‹¨ì–´ ì„ë² ë”©:")
    for i, token in enumerate(tokens):
        first_dims = word_embeddings[0, i, :5]  # ì²˜ìŒ 5ì°¨ì›ë§Œ ì¶œë ¥
        print(f"  {token}: [{first_dims[0]:.3f}, {first_dims[1]:.3f}, {first_dims[2]:.3f}, ...]")
    
    # ìœ„ì¹˜ ì¸ì½”ë”© ì ìš©
    pos_encoder = PositionalEncoding(d_model=512)
    final_embeddings = pos_encoder(word_embeddings)
    
    print(f"\nğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì¶”ê°€ëœ ì„ë² ë”©:")
    for i, token in enumerate(tokens):
        first_dims = final_embeddings[0, i, :5]
        print(f"  {token}: [{first_dims[0]:.3f}, {first_dims[1]:.3f}, {first_dims[2]:.3f}, ...]")
    
    # ìœ„ì¹˜ë³„ ì°¨ì´ ë¶„ì„
    print(f"\nğŸ” ìœ„ì¹˜ë³„ ì°¨ì´ ë¶„ì„:")
    pos_only = pos_encoder.pe[0, :3, :5]  # ìœ„ì¹˜ ì¸ì½”ë”©ë§Œ (ì²˜ìŒ 5ì°¨ì›)
    for i, token in enumerate(tokens):
        print(f"  {token} (pos {i}): [{pos_only[i, 0]:.3f}, {pos_only[i, 1]:.3f}, {pos_only[i, 2]:.3f}, ...]")
    
    # ì‹œê°í™”
    # pos_encoder.visualize_positions(max_pos=10)
    
    return final_embeddings
```

### Step 4: ì™„ì „í•œ Transformer Block êµ¬í˜„

```python
class TransformerBlock(nn.Module):
    """
    ì™„ì „í•œ Transformer Encoder Block
    Multi-Head Attention + Feed Forward + Residual Connection + Layer Norm
    """
    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):
        super().__init__()
        
        print(f"ğŸ—ï¸ Transformer Block êµ¬ì„±:")
        print(f"  d_model: {d_model}")
        print(f"  n_heads: {n_heads}")
        print(f"  d_ff: {d_ff} (Feed Forward ë‚´ë¶€ ì°¨ì›)")
        print(f"  dropout: {dropout}")
        
        # Multi-Head Self-Attention
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        
        # Feed Forward Network
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),    # 512 â†’ 2048 í™•ì¥
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),    # 2048 â†’ 512 ì¶•ì†Œ
            nn.Dropout(dropout)
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.size()
        
        print(f"\nğŸ”„ Transformer Block Forward:")
        print(f"  ì…ë ¥: {x.shape}")
        
        # Step 1: Multi-Head Self-Attention + Residual + LayerNorm
        attn_output, attn_weights = self.self_attention(x, x, x, mask)
        x1 = self.norm1(x + self.dropout(attn_output))  # Residual connection
        print(f"  After Attention + Residual + LayerNorm: {x1.shape}")
        
        # Step 2: Feed Forward + Residual + LayerNorm  
        ff_output = self.feed_forward(x1)
        x2 = self.norm2(x1 + ff_output)  # Residual connection
        print(f"  After Feed Forward + Residual + LayerNorm: {x2.shape}")
        
        return x2, attn_weights

# ğŸ§ª ì™„ì „í•œ Transformer í…ŒìŠ¤íŠ¸
def test_full_transformer():
    print("ğŸ”¬ Complete Transformer Block Test")
    print("=" * 60)
    
    batch_size, seq_len, d_model = 2, 4, 512
    
    # ì…ë ¥ ìƒì„± ë° ìœ„ì¹˜ ì¸ì½”ë”©
    word_embeddings = torch.randn(batch_size, seq_len, d_model)
    pos_encoder = PositionalEncoding(d_model)
    x = pos_encoder(word_embeddings)
    
    print(f"ğŸ“¥ ì „ì²˜ë¦¬ ì™„ë£Œëœ ì…ë ¥: {x.shape}")
    
    # Transformer Block
    transformer = TransformerBlock(d_model=512, n_heads=8, d_ff=2048)
    output, attention_weights = transformer(x)
    
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"  ì¶œë ¥: {output.shape}")
    print(f"  Attention weights: {attention_weights.shape}")
    print(f"  íŒŒë¼ë¯¸í„° ê°œìˆ˜: {sum(p.numel() for p in transformer.parameters()):,}")
    
    return output, attention_weights
```

## ğŸ“Š ì„±ëŠ¥ ë° í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜

### í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§

```python
class AttentionAnalyzer:
    """
    Attentionì˜ í•™ìŠµ ê³¼ì •ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self):
        self.epoch_metrics = {}
        
    def simulate_learning_process(self):
        """í•™ìŠµ ê³¼ì •ì—ì„œ attention patternì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´ì…˜"""
        
        print("ğŸ“ˆ Attention í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜")
        print("=" * 50)
        
        # í•™ìŠµ ë‹¨ê³„ë³„ ë©”íŠ¸ë¦­
        learning_stages = {
            0: {
                "loss": 8.5, 
                "attention_entropy": 2.1,  # ë†’ì€ ì—”íŠ¸ë¡œí”¼ = ë¬´ì‘ìœ„ attention
                "pattern": "ì™„ì „ ë¬´ì‘ìœ„",
                "description": "ëª¨ë“  ìœ„ì¹˜ì— ê· ë“±í•˜ê²Œ attention"
            },
            100: {
                "loss": 4.2,
                "attention_entropy": 1.8, 
                "pattern": "ìœ„ì¹˜ í¸í–¥ ë°œìƒ",
                "description": "ì²« ë²ˆì§¸/ë§ˆì§€ë§‰ í† í°ì— ê³¼ë„í•˜ê²Œ ì§‘ì¤‘"
            },
            500: {
                "loss": 2.1,
                "attention_entropy": 1.5,
                "pattern": "ë¬¸ë²• íŒ¨í„´ í•™ìŠµ", 
                "description": "ë™ì‚¬-ëª©ì ì–´, ì£¼ì–´-ë™ì‚¬ ê´€ê³„ í•™ìŠµ ì‹œì‘"
            },
            1000: {
                "loss": 0.8,
                "attention_entropy": 1.2,
                "pattern": "ì˜ë¯¸ì  attention",
                "description": "ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹¨ì–´ë“¤ ê°„ ê°•í•œ ì—°ê²°"
            },
            2000: {
                "loss": 0.3,
                "attention_entropy": 1.0,
                "pattern": "ì „ë¬¸ì  íŠ¹í™”",
                "description": "ê° headê°€ ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ì  ê´€ê³„ì— íŠ¹í™”"
            }
        }
        
        print(f"{'Epoch':<8} {'Loss':<8} {'Entropy':<8} {'Pattern'}")
        print("-" * 50)
        
        for epoch, metrics in learning_stages.items():
            print(f"{epoch:<8} {metrics['loss']:<8.1f} {metrics['attention_entropy']:<8.1f} {metrics['pattern']}")
            
        return learning_stages
    
    def visualize_attention_evolution(self):
        """í•™ìŠµ ê³¼ì •ì—ì„œ attention weightê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œê°í™”"""
        
        # "I love you" ì˜ˆì‹œì—ì„œ ê° í•™ìŠµ ë‹¨ê³„ë³„ attention pattern
        tokens = ["I", "love", "you"]
        
        evolution_patterns = {
            "Epoch 0 (Random)": [
                [0.33, 0.33, 0.34],  # Iì˜ attention
                [0.32, 0.35, 0.33],  # loveì˜ attention  
                [0.31, 0.34, 0.35]   # youì˜ attention
            ],
            "Epoch 500 (Grammar Learning)": [
                [0.6, 0.3, 0.1],     # I â†’ love (ì£¼ì–´â†’ë™ì‚¬)
                [0.4, 0.2, 0.4],     # love â†’ I,you (ë™ì‚¬ê°€ ì£¼ì–´,ëª©ì ì–´ ëª¨ë‘ ì°¸ì¡°)
                [0.1, 0.4, 0.5]      # you â†’ love (ëª©ì ì–´â†’ë™ì‚¬)
            ],
            "Epoch 2000 (Semantic Mastery)": [
                [0.8, 0.15, 0.05],   # Iê°€ loveì— ê°•í•˜ê²Œ ì§‘ì¤‘
                [0.1, 0.2, 0.7],     # loveê°€ youì— ê°•í•˜ê²Œ ì§‘ì¤‘  
                [0.05, 0.75, 0.2]    # youê°€ loveì— ë§¤ìš° ê°•í•˜ê²Œ ì§‘ì¤‘
            ]
        }
        
        # ì‹œê°í™” (matplotlib ì‚¬ìš©)
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        for idx, (stage, pattern) in enumerate(evolution_patterns.items()):
            sns.heatmap(pattern, 
                       annot=True, fmt='.2f',
                       xticklabels=tokens, yticklabels=tokens,
                       ax=axes[idx], cmap='Blues',
                       vmin=0, vmax=1)
            axes[idx].set_title(stage)
            axes[idx].set_xlabel('Key (ì°¸ì¡°ë˜ëŠ” ë‹¨ì–´)')
            axes[idx].set_ylabel('Query (ì°¸ì¡°í•˜ëŠ” ë‹¨ì–´)')
            
        plt.tight_layout()
        plt.suptitle('Attention Pattern Evolution: "I love you"', y=1.02)
        plt.show()
        
    def analyze_head_specialization(self):
        """Multi-headì˜ ê° headê°€ ì–´ë–¤ íŒ¨í„´ì— íŠ¹í™”ë˜ëŠ”ì§€ ë¶„ì„"""
        
        print("\nğŸ§  Multi-Head Specialization ë¶„ì„")
        print("=" * 50)
        
        head_specializations = {
            "Head 1": "ì£¼ì–´-ë™ì‚¬ ê´€ê³„ (Subject-Verb)",
            "Head 2": "ë™ì‚¬-ëª©ì ì–´ ê´€ê³„ (Verb-Object)", 
            "Head 3": "í˜•ìš©ì‚¬-ëª…ì‚¬ ê´€ê³„ (Adjective-Noun)",
            "Head 4": "ìœ„ì¹˜ì  ì¸ì ‘ì„± (Positional Proximity)",
            "Head 5": "ì¥ê±°ë¦¬ ì˜ì¡´ì„± (Long-range Dependencies)",
            "Head 6": "ë°˜ë³µ/ëŒ€ì¹­ íŒ¨í„´ (Repetition/Symmetry)",
            "Head 7": "ì •ë³´ëŸ‰ ê¸°ë°˜ (Information Content)",
            "Head 8": "ì „ì—­ì  ë§¥ë½ (Global Context)"
        }
        
        for head, specialization in head_specializations.items():
            print(f"  {head}: {specialization}")
            
        print(f"\nğŸ’¡ í•µì‹¬ í†µì°°:")
        print(f"  - ê° headëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì— íŠ¹í™”")
        print(f"  - 8ê°œ headì˜ ì¡°í•©ìœ¼ë¡œ ë³µì¡í•œ ì–¸ì–´ì  ê´€ê³„ë¥¼ ëª¨ë‘ í¬ì°©")
        print(f"  - Single-headë³´ë‹¤ í›¨ì”¬ í’ë¶€í•œ í‘œí˜„ë ¥ í™•ë³´")
        
        return head_specializations

# ğŸ§ª ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
def run_performance_analysis():
    analyzer = AttentionAnalyzer()
    
    # í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜
    learning_stages = analyzer.simulate_learning_process()
    
    # Attention ì§„í™” ì‹œê°í™”
    # analyzer.visualize_attention_evolution()
    
    # Head íŠ¹í™” ë¶„ì„
    head_specs = analyzer.analyze_head_specialization()
    
    return learning_stages, head_specs
```

## ğŸ¯ ì‹¤ì „ êµ¬í˜„ íŒ ë° ìµœì í™”

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ Attention êµ¬í˜„

```python
class EfficientAttention(nn.Module):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ Attention êµ¬í˜„
    í° ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ OOM ë°©ì§€
    """
    def __init__(self, d_k, chunk_size=1024):
        super().__init__()
        self.d_k = d_k
        self.chunk_size = chunk_size
        
    def forward(self, Q, K, V, mask=None):
        batch_size, seq_len, d_k = Q.size()
        
        if seq_len <= self.chunk_size:
            # ì‘ì€ ì‹œí€€ìŠ¤: ì¼ë°˜ attention
            return self._standard_attention(Q, K, V, mask)
        else:
            # í° ì‹œí€€ìŠ¤: chunked attention
            return self._chunked_attention(Q, K, V, mask)
    
    def _standard_attention(self, Q, K, V, mask):
        """í‘œì¤€ attention ê³„ì‚°"""
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores.masked_fill_(mask == 0, -1e9)
            
        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)
        
        return output, weights
    
    def _chunked_attention(self, Q, K, V, mask):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ chunked attention"""
        batch_size, seq_len, d_k = Q.size()
        output = torch.zeros_like(Q)
        
        # Queryë¥¼ chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, seq_len, self.chunk_size):
            end_i = min(i + self.chunk_size, seq_len)
            Q_chunk = Q[:, i:end_i, :]
            
            # í˜„ì¬ chunkì— ëŒ€í•´ ì „ì²´ K,Vì™€ attention ê³„ì‚°
            scores = torch.matmul(Q_chunk, K.transpose(-2, -1)) / math.sqrt(self.d_k)
            
            if mask is not None:
                scores.masked_fill_(mask[:, i:end_i, :] == 0, -1e9)
                
            weights = F.softmax(scores, dim=-1)
            output_chunk = torch.matmul(weights, V)
            output[:, i:end_i, :] = output_chunk
            
        return output, None  # weightsëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ ë°˜í™˜í•˜ì§€ ì•ŠìŒ

print("âš¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¹„êµ:")
print("  Standard Attention: O(nÂ²) ë©”ëª¨ë¦¬")  
print("  Chunked Attention: O(n Ã— chunk_size) ë©”ëª¨ë¦¬")
print("  ì˜ˆ: ê¸¸ì´ 4096 ì‹œí€€ìŠ¤")
print("    Standard: 16M attention matrix")
print("    Chunked (1024): 4M max memory ì‚¬ìš©")
```

### í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ íŒë“¤

```python
class StableTransformer(nn.Module):
    """
    í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ ê°œì„ ì‚¬í•­ë“¤ì´ í¬í•¨ëœ Transformer
    """
    def __init__(self, d_model=512, n_heads=8, dropout=0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        
        # ì´ˆê¸°í™” ê°œì„ 
        self._initialize_weights()
        
        # Gradient clippingì„ ìœ„í•œ hook ë“±ë¡
        self.register_backward_hook(self._gradient_clipping_hook)
        
    def _initialize_weights(self):
        """Xavier/He ì´ˆê¸°í™”ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ ì‹œì‘"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Xavier uniform ì´ˆê¸°í™”
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
                
        print("âœ… ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì™„ë£Œ: Xavier Uniform")
        
    def _gradient_clipping_hook(self, module, grad_input, grad_output):
        """ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ìœ¼ë¡œ exploding gradient ë°©ì§€"""
        if grad_output[0] is not None:
            torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)
            
    def forward(self, x, mask=None):
        # Attention ì „í›„ì˜ norm í™•ì¸
        input_norm = torch.norm(x).item()
        
        output, weights = self.attention(x, x, x, mask)
        
        output_norm = torch.norm(output).item()
        
        # Norm ê¸‰ë³€ ê°ì§€
        if output_norm / input_norm > 10:
            print(f"âš ï¸ ì£¼ì˜: í° norm ë³€í™” ê°ì§€ {input_norm:.2f} â†’ {output_norm:.2f}")
            
        return output, weights

print("ğŸ›¡ï¸ í•™ìŠµ ì•ˆì •ì„± ê°œì„ ì‚¬í•­:")
print("  âœ… Xavier ì´ˆê¸°í™”")
print("  âœ… Gradient clipping") 
print("  âœ… Norm ëª¨ë‹ˆí„°ë§")
print("  âœ… Dropout ì •ê·œí™”")
```

## ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ê²€ì¦

### ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì •

```python
import time
import psutil
import os

class PerformanceBenchmark:
    """Transformer êµ¬í˜„ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ê²€ì¦í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
    def benchmark_attention_scaling(self):
        """ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ attention ì„±ëŠ¥ ì¸¡ì •"""
        
        print("\nğŸ“Š Attention Scaling ë²¤ì¹˜ë§ˆí¬")
        print("=" * 50)
        
        d_model = 512
        batch_size = 8
        seq_lengths = [128, 256, 512, 1024, 2048]
        
        results = []
        
        for seq_len in seq_lengths:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            process = psutil.Process(os.getpid())
            mem_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # ëª¨ë¸ ìƒì„±
            model = MultiHeadAttention(d_model, n_heads=8).to(self.device)
            x = torch.randn(batch_size, seq_len, d_model).to(self.device)
            
            # ì‹œê°„ ì¸¡ì •
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            
            with torch.no_grad():
                output, weights = model(x, x, x)
                
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
            mem_after = process.memory_info().rss / 1024 / 1024  # MB
            mem_used = mem_after - mem_before
            
            # ê²°ê³¼ ê¸°ë¡
            inference_time = (end_time - start_time) * 1000  # ms
            
            results.append({
                'seq_len': seq_len,
                'time_ms': inference_time,
                'memory_mb': mem_used,
                'ops_per_sec': (batch_size * seq_len) / (inference_time / 1000)
            })
            
            print(f"  ê¸¸ì´ {seq_len:>4}: {inference_time:>6.1f}ms, {mem_used:>6.1f}MB")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model, x, output, weights
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return results
    
    def validate_attention_properties(self):
        """Attentionì˜ ìˆ˜í•™ì  ì„±ì§ˆë“¤ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ê²€ì¦"""
        
        print("\nğŸ”¬ Attention ìˆ˜í•™ì  ì„±ì§ˆ ê²€ì¦")
        print("=" * 50)
        
        batch_size, seq_len, d_k = 2, 4, 64
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        Q = torch.randn(batch_size, seq_len, d_k)
        K = torch.randn(batch_size, seq_len, d_k)
        V = torch.randn(batch_size, seq_len, d_k)
        
        attention = ScaledDotProductAttention(d_k)
        output, weights = attention(Q, K, V)
        
        # ê²€ì¦ 1: Attention weightê°€ í™•ë¥ ë¶„í¬ì¸ê°€?
        weight_sums = weights.sum(dim=-1)  # ê° rowì˜ í•©
        prob_check = torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-6)
        print(f"  âœ… Attention weightsê°€ í™•ë¥ ë¶„í¬: {prob_check}")
        print(f"     (ê° row í•©ê³„: {weight_sums[0, 0].item():.6f})")
        
        # ê²€ì¦ 2: Output ì°¨ì›ì´ ì˜¬ë°”ë¥¸ê°€?
        dim_check = (output.shape == (batch_size, seq_len, d_k))
        print(f"  âœ… ì¶œë ¥ ì°¨ì› ì˜¬ë°”ë¦„: {dim_check}")
        print(f"     (ì˜ˆìƒ: {(batch_size, seq_len, d_k)}, ì‹¤ì œ: {output.shape})")
        
        # ê²€ì¦ 3: Attentionì´ Vì˜ weighted sumì¸ê°€?
        manual_output = torch.matmul(weights, V)
        manual_check = torch.allclose(output, manual_output, atol=1e-6)
        print(f"  âœ… ìˆ˜ë™ ê³„ì‚°ê³¼ ì¼ì¹˜: {manual_check}")
        
        # ê²€ì¦ 4: Scale factorì˜ íš¨ê³¼
        unscaled_scores = torch.matmul(Q, K.transpose(-2, -1))
        scaled_scores = unscaled_scores / math.sqrt(d_k)
        
        unscaled_std = unscaled_scores.std().item()
        scaled_std = scaled_scores.std().item()
        
        print(f"  âœ… Scaling íš¨ê³¼:")
        print(f"     ì›ë³¸ í‘œì¤€í¸ì°¨: {unscaled_std:.3f}")
        print(f"     ìŠ¤ì¼€ì¼ëœ í‘œì¤€í¸ì°¨: {scaled_std:.3f}")
        print(f"     âˆšd_k = {math.sqrt(d_k):.3f}")
        
        return {
            'probability_check': prob_check,
            'dimension_check': dim_check,  
            'calculation_check': manual_check,
            'scaling_effect': scaled_std / unscaled_std
        }

# ğŸ§ª ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def run_comprehensive_test():
    print("ğŸ”¬ ì¢…í•© Transformer êµ¬í˜„ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    test_basic_attention()
    
    print("\n2ï¸âƒ£ Multi-Head Attention í…ŒìŠ¤íŠ¸")  
    test_multihead_attention()
    
    print("\n3ï¸âƒ£ Positional Encoding í…ŒìŠ¤íŠ¸")
    test_positional_encoding()
    
    print("\n4ï¸âƒ£ ì „ì²´ Transformer Block í…ŒìŠ¤íŠ¸")
    test_full_transformer()
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    print("\n5ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    benchmark = PerformanceBenchmark()
    perf_results = benchmark.benchmark_attention_scaling()
    
    print("\n6ï¸âƒ£ ìˆ˜í•™ì  ê²€ì¦")
    validation_results = benchmark.validate_attention_properties()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("   êµ¬í˜„ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ë©° ë…¼ë¬¸ì˜ ì£¼ì¥ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤.")
    
    return perf_results, validation_results

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    perf_results, validation = run_comprehensive_test()
```

## ğŸ’¡ ì‹¤ë¬´ êµ¬í˜„ì‹œ ì£¼ì˜ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ê´€ë¦¬
```python
# âŒ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì 
attention_matrix = Q @ K.T  # [batch, seq, seq] - í° ë©”ëª¨ë¦¬ ì‚¬ìš©

# âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì   
for chunk in chunks(Q):
    chunk_attention = chunk @ K.T
    process_chunk(chunk_attention)
```

### 2. ìˆ˜ì¹˜ì  ì•ˆì •ì„±
```python
# âŒ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •
scores = Q @ K.T
weights = torch.softmax(scores, dim=-1)

# âœ… ìˆ˜ì¹˜ì  ì•ˆì •
scores = Q @ K.T / math.sqrt(d_k)  # scaling
scores = scores - scores.max(dim=-1, keepdim=True)[0]  # numerical stability
weights = torch.softmax(scores, dim=-1)
```

### 3. ê·¸ë˜ë””ì–¸íŠ¸ ê´€ë¦¬
```python
# âœ… ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# âœ… í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)
```

ì´ì œ `step_by_step_learning.md`ì—ì„œ ë‹¨ê³„ë³„ë¡œ ì–´ë–»ê²Œ í•™ìŠµí•´ì•¼ í•˜ëŠ”ì§€ ì•Œì•„ë³´ì„¸ìš”! ğŸš€