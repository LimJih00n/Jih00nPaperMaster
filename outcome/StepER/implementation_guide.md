# StepER êµ¬í˜„ ì‹¤ìŠµ ê°€ì´ë“œ

## ğŸš€ Step-by-Step êµ¬í˜„ íŠœí† ë¦¬ì–¼

### ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch
import torch.nn as nn
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
import numpy as np

# RAG ê´€ë ¨
from rank_bm25 import BM25Okapi  # BM25 retriever
import faiss  # ë²¡í„° ê²€ìƒ‰ìš© (ì„ íƒ)
```

---

## 1ï¸âƒ£ ë°ì´í„°ì…‹ êµ¬ì¶•

### Step 1: Teacher Modelë¡œ ì¶”ë¡  ê²½ë¡œ ìƒì„±

```python
def generate_stepwise_data(question, documents, teacher_model, tokenizer):
    """
    Teacher ëª¨ë¸ì„ ì‚¬ìš©í•´ ë‹¨ê³„ë³„ ì¶”ë¡  ë°ì´í„° ìƒì„±
    """
    stepwise_data = {
        'first_step': None,
        'mid_steps': [],
        'final_step': None
    }
    
    # 1ë‹¨ê³„: ì´ˆê¸° ì¶”ë¡  (Reasoning Initialization)
    prompt_step1 = f"""
    Question: {question}
    Documents: {documents[0]}  # ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©
    
    Based on the document, start reasoning about the question.
    Reasoning:
    """
    
    first_reasoning = teacher_model.generate(prompt_step1)
    stepwise_data['first_step'] = {
        'input': prompt_step1,
        'output': first_reasoning,
        'documents_used': documents[0]
    }
    
    # 2-4ë‹¨ê³„: ì¶”ë¡  í™•ì¥ (Reasoning Expansion)
    accumulated_reasoning = first_reasoning
    for step in range(2, 5):  # 2, 3, 4ë‹¨ê³„
        prompt_mid = f"""
        Question: {question}
        Previous reasoning: {accumulated_reasoning}
        New documents: {documents[step-1]}
        
        Continue the reasoning with new information.
        Reasoning:
        """
        
        mid_reasoning = teacher_model.generate(prompt_mid)
        stepwise_data['mid_steps'].append({
            'input': prompt_mid,
            'output': mid_reasoning,
            'step': step
        })
        accumulated_reasoning += " " + mid_reasoning
    
    # 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ (Reasoning Aggregation)
    prompt_final = f"""
    Question: {question}
    All reasoning: {accumulated_reasoning}
    All documents: {' '.join(documents)}
    
    Provide the final answer.
    So the answer is:
    """
    
    final_answer = teacher_model.generate(prompt_final)
    stepwise_data['final_step'] = {
        'input': prompt_final,
        'output': final_answer
    }
    
    return stepwise_data
```

### Step 2: ë°ì´í„° í•„í„°ë§

```python
def filter_correct_samples(stepwise_data, ground_truth):
    """
    ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œë§Œ í•„í„°ë§
    """
    # ìµœì¢… ë‹µë³€ì—ì„œ ë‹µ ì¶”ì¶œ
    predicted_answer = extract_answer(stepwise_data['final_step']['output'])
    
    # ì •ë‹µê³¼ ë¹„êµ
    if predicted_answer.lower().strip() == ground_truth.lower().strip():
        return stepwise_data
    else:
        return None  # í‹€ë¦° ë‹µë³€ì€ ì œì™¸
```

---

## 2ï¸âƒ£ StepER ëª¨ë¸ í´ë˜ìŠ¤ êµ¬í˜„

### Multi-task Learningì„ ìœ„í•œ ì»¤ìŠ¤í…€ ëª¨ë¸

```python
class StepERModel(nn.Module):
    """
    StepER: 3ê°€ì§€ ì¶”ë¡  ëŠ¥ë ¥ì„ í•™ìŠµí•˜ëŠ” ëª¨ë¸
    """
    def __init__(self, base_model_name="meta-llama/Llama-3.1-8B"):
        super().__init__()
        
        # ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ ë¡œë“œ
        self.base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # ë‚œì´ë„ íŒŒë¼ë¯¸í„° (Ïƒ) ì´ˆê¸°í™”
        self.sigma_init = nn.Parameter(torch.tensor(1.0))  # ì´ˆê¸°í™” ë‚œì´ë„
        self.sigma_exp = nn.Parameter(torch.tensor(1.0))   # í™•ì¥ ë‚œì´ë„
        self.sigma_agg = nn.Parameter(torch.tensor(1.0))   # ì§‘ê³„ ë‚œì´ë„
        
    def forward(self, input_ids, attention_mask, task_type):
        """
        task_type: 'init', 'exp', 'agg' ì¤‘ í•˜ë‚˜
        """
        # ê¸°ë³¸ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids  # Teacher forcing
        )
        
        return outputs.loss, task_type
    
    def compute_weighted_loss(self, losses_dict):
        """
        ë‚œì´ë„ ì¸ì‹ ê°€ì¤‘ì¹˜ ì ìš© (ìˆ˜ì‹ 4)
        """
        weighted_loss = 0
        
        for task_type, loss in losses_dict.items():
            if task_type == 'init':
                sigma = self.sigma_init
            elif task_type == 'exp':
                sigma = self.sigma_exp
            else:  # 'agg'
                sigma = self.sigma_agg
            
            # ìˆ˜ì‹: (1/2ÏƒÂ²) * L + log(Ïƒ)
            weighted_loss += (1 / (2 * sigma**2)) * loss + torch.log(sigma)
        
        return weighted_loss
```

---

## 3ï¸âƒ£ í•™ìŠµ ë£¨í”„ êµ¬í˜„

### Custom Trainer for Multi-task Learning

```python
class StepERTrainer:
    def __init__(self, model, train_dataset, eval_dataset):
        self.model = model
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=5e-6  # ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ learning rate
        )
        
    def train_step(self, batch):
        """
        í•œ ë²ˆì˜ í•™ìŠµ ìŠ¤í…
        """
        self.model.train()
        losses_dict = {}
        
        # (a) Reasoning Initialization í•™ìŠµ
        if 'first_step' in batch:
            loss_init, _ = self.model(
                input_ids=batch['first_step_ids'],
                attention_mask=batch['first_step_mask'],
                task_type='init'
            )
            losses_dict['init'] = loss_init
        
        # (b) Reasoning Expansion í•™ìŠµ
        for step_data in batch.get('mid_steps', []):
            loss_exp, _ = self.model(
                input_ids=step_data['ids'],
                attention_mask=step_data['mask'],
                task_type='exp'
            )
            if 'exp' not in losses_dict:
                losses_dict['exp'] = 0
            losses_dict['exp'] += loss_exp
        
        # (c) Reasoning Aggregation í•™ìŠµ
        if 'final_step' in batch:
            loss_agg, _ = self.model(
                input_ids=batch['final_step_ids'],
                attention_mask=batch['final_step_mask'],
                task_type='agg'
            )
            losses_dict['agg'] = loss_agg
        
        # ë‚œì´ë„ ì¸ì‹ ê°€ì¤‘ì¹˜ ì ìš©
        total_loss = self.model.compute_weighted_loss(losses_dict)
        
        # ì—­ì „íŒŒ
        total_loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        return {
            'total_loss': total_loss.item(),
            'sigma_init': self.model.sigma_init.item(),
            'sigma_exp': self.model.sigma_exp.item(),
            'sigma_agg': self.model.sigma_agg.item()
        }
    
    def train(self, num_epochs=2):
        """
        ì „ì²´ í•™ìŠµ ê³¼ì •
        """
        for epoch in range(num_epochs):
            print(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
            
            for batch_idx, batch in enumerate(self.train_dataset):
                metrics = self.train_step(batch)
                
                if batch_idx % 100 == 0:
                    print(f"Batch {batch_idx}: Loss={metrics['total_loss']:.4f}")
                    print(f"  Ïƒ values - Init: {metrics['sigma_init']:.2f}, "
                          f"Exp: {metrics['sigma_exp']:.2f}, "
                          f"Agg: {metrics['sigma_agg']:.2f}")
            
            # í‰ê°€
            self.evaluate()
    
    def evaluate(self):
        """
        ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€
        """
        self.model.eval()
        # í‰ê°€ ì½”ë“œ...
```

---

## 4ï¸âƒ£ ì¶”ë¡  ì‹œ ì‚¬ìš©ë²•

### Multi-step ì¶”ë¡  ì‹¤í–‰

```python
def stepwise_inference(question, model, retriever, num_steps=5):
    """
    í•™ìŠµëœ StepER ëª¨ë¸ë¡œ ë‹¨ê³„ë³„ ì¶”ë¡ 
    """
    all_documents = []
    all_reasoning = []
    
    # Step 1: ì´ˆê¸° ì¶”ë¡ 
    docs_1 = retriever.search(question, k=4)
    prompt_1 = f"Question: {question}\nDocuments: {docs_1}\nReasoning:"
    
    reasoning_1 = model.generate(prompt_1, task_type='init')
    all_documents.extend(docs_1)
    all_reasoning.append(reasoning_1)
    
    # Step 2-4: ì¶”ë¡  í™•ì¥
    for step in range(2, num_steps):
        # ì´ì „ ì¶”ë¡ ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•´ ìƒˆ ë¬¸ì„œ ê²€ìƒ‰
        query = all_reasoning[-1]  # ë§ˆì§€ë§‰ ì¶”ë¡ 
        new_docs = retriever.search(query, k=4)
        
        prompt = f"""
        Question: {question}
        Previous: {' '.join(all_reasoning)}
        New docs: {new_docs}
        Continue reasoning:
        """
        
        reasoning = model.generate(prompt, task_type='exp')
        all_documents.extend(new_docs)
        all_reasoning.append(reasoning)
        
        # "So the answer is" ë‚˜ì˜¤ë©´ ì¡°ê¸° ì¢…ë£Œ
        if "So the answer is" in reasoning:
            break
    
    # Step 5: ìµœì¢… ë‹µë³€
    prompt_final = f"""
    Question: {question}
    All reasoning: {' '.join(all_reasoning)}
    All documents: {' '.join(all_documents)}
    So the answer is:
    """
    
    final_answer = model.generate(prompt_final, task_type='agg')
    
    return {
        'question': question,
        'reasoning_steps': all_reasoning,
        'final_answer': final_answer,
        'documents_used': all_documents
    }
```

---

## 5ï¸âƒ£ ì‹¤ì „ íŒê³¼ ì£¼ì˜ì‚¬í•­

### ğŸ’¡ êµ¬í˜„ ì‹œ í•µì‹¬ í¬ì¸íŠ¸

```python
# 1. ë°°ì¹˜ ì²˜ë¦¬ ì‹œ íŒ¨ë”© ì£¼ì˜
def collate_stepwise_batch(samples):
    """
    ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ì²˜ë¦¬
    """
    # ê° íƒœìŠ¤í¬ë³„ë¡œ ê·¸ë£¹í™”
    init_samples = [s['first_step'] for s in samples if 'first_step' in s]
    exp_samples = []
    for s in samples:
        exp_samples.extend(s.get('mid_steps', []))
    agg_samples = [s['final_step'] for s in samples if 'final_step' in s]
    
    # ê°ê° íŒ¨ë”©
    # ...

# 2. ë©”ëª¨ë¦¬ ìµœì í™”
def gradient_checkpointing_setup(model):
    """
    Gradient checkpointingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    """
    model.gradient_checkpointing_enable()
    
# 3. Mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    loss = model(inputs)
scaler.scale(loss).backward()
```

### âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë°ì´í„° í’ˆì§ˆ**: Teacher ëª¨ë¸ì˜ ì¶”ë¡ ì´ í‹€ë¦¬ë©´ ì•ˆ ë¨
2. **ë‹¨ê³„ ìˆ˜ ì¡°ì ˆ**: ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ S ê°’ ì¡°ì • í•„ìš”
3. **Retriever ì„±ëŠ¥**: BM25ë³´ë‹¤ Dense retriever ì‚¬ìš© ê³ ë ¤
4. **Ïƒ ì´ˆê¸°ê°’**: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ í•™ìŠµ ë¶ˆì•ˆì •

---

## ğŸ“ˆ ì˜ˆìƒ ê²°ê³¼

```python
# í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§
"""
Epoch 1/2
Batch 0: Loss=2.3451
  Ïƒ values - Init: 0.95, Exp: 1.12, Agg: 1.45
  
Batch 100: Loss=1.8234  
  Ïƒ values - Init: 0.72, Exp: 1.08, Agg: 1.67
  
# Ïƒ ê°’ í•´ì„:
# - Initì´ ê°€ì¥ ì‘ìŒ = ê°€ì¥ ì‰¬ìš´ íƒœìŠ¤í¬
# - Aggê°€ ê°€ì¥ í¼ = ê°€ì¥ ì–´ë ¤ìš´ íƒœìŠ¤í¬
"""
```

ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´ ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!