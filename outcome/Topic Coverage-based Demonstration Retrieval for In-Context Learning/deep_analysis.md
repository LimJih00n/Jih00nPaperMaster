# Topic Coverage-based Demonstration Retrieval for In-Context Learning - 4-Layer ì™„ì „ë¶„í•´ ë¶„ì„

## ğŸ”¬ 4-Layer Deep Analysis Framework ì ìš©

### ğŸ“ Layer 1: ëª¨ë¸ ì•„í‚¤í…ì²˜ ì™„ì „ë¶„í•´
**"ë°ì´í„°ê°€ Xâ†’Yê¹Œì§€ ì–´ë–¤ ë³€í™˜ì„ ê±°ì¹˜ëŠ”ê°€?"**

#### ğŸŒŠ ë°ì´í„° í”Œë¡œìš° ì¶”ì 

**Stage 1: Topical Knowledge Assessment (ì‚¬ì „ ì²˜ë¦¬ ë‹¨ê³„)**
```python
# ì…ë ¥ ë°ì´í„° ë³€í™˜ ê³¼ì •
Raw_Demonstrations = [
    "Herbivores are animals that primarily eat plants",
    "Carnivores hunt and eat other animals", 
    "Photosynthesis converts sunlight into energy"
]
    â†“ [Embedding Model: all-mpnet-base-v2]
Embeddings = torch.tensor([
    [0.1, -0.3, 0.8, ...],  # [768] - herbivore demo embedding
    [-0.2, 0.9, -0.1, ...], # [768] - carnivore demo embedding  
    [0.6, 0.2, -0.4, ...]   # [768] - photosynthesis demo embedding
])  # Shape: [num_demos, 768]
    â†“ [Topic Mining: BM25 + Semantic Matching + GPT-4o]
Core_Topics = {
    demo_1: {"herbivore": 0.9, "animal": 0.6, "plant": 0.4},
    demo_2: {"carnivore": 0.8, "predator": 0.7, "animal": 0.5}, 
    demo_3: {"photosynthesis": 0.95, "energy": 0.6, "plant": 0.8}
}
    â†“ [Distinctiveness-aware Soft Labels]
Training_Targets = torch.tensor([
    [0, 0.9, 0, 0.6, ...],  # herbivore=0.9, animal=0.6 for demo_1
    [0.8, 0, 0.7, 0.5, ...], # carnivore=0.8, predator=0.7 for demo_2
    [0, 0, 0, 0.6, 0.95, 0.8, ...] # photosynthesis=0.95, etc.
])  # Shape: [num_demos, num_topics]
    â†“ [3-Layer MLP Training]
Topic_Predictor = f(e_d) â†’ tÌ‚_d âˆˆ [0,1]^|T|
```

**Stage 2: Test-time Retrieval (ì‹¤ì‹œê°„ ì¶”ë¡ )**
```python
# í…ŒìŠ¤íŠ¸ ì‹œê°„ ë°ì´í„° ë³€í™˜
Test_Question = "Non-human organisms that mainly consume plants are known as what?"
    â†“ [Same Embedding Model] 
Test_Embedding = torch.tensor([0.3, -0.1, 0.7, ...])  # [768]
    â†“ [Trained Topic Predictor]
Required_Topics = torch.tensor([0.87, 0.91, 0.90, ...])  # [num_topics]
                              # herbivore=0.87, carnivore=0.91, omnivore=0.90

# ê° í›„ë³´ ì‹œì—°ì— ëŒ€í•´
For each candidate_d in candidate_pool:
    Candidate_Embedding = encode(candidate_d)  # [768]
    â†“ [Topic Predictor]
    Covered_Topics = f(Candidate_Embedding)   # [num_topics]  
    â†“ [Relevance Score Computation]
    Relevance = âŸ¨Required_Topics âŠ˜ Model_Knowledge, Covered_TopicsâŸ©  # scalar
    â†“ [Cumulative Coverage Update]  
    Updated_Coverage = max(0, New_Coverage - Previous_Coverage)  # [num_topics]

# ìµœì¢… ì¶œë ¥
Selected_Demonstrations = [d_1, d_2, ..., d_k]  # ì„ íƒëœ kê°œ ì‹œì—°
```

#### ğŸ“Š ì°¨ì›ë³„ ë³€í™˜ ë¶„ì„

```python
dimension_tracking = {
    "ì…ë ¥_ë‹¨ê³„": {
        "raw_text": "variable length string",
        "embedding": "[batch_size, 768]",
        "topic_labels": "[batch_size, num_topics]"
    },
    
    "ëª¨ë¸_ë‚´ë¶€": {
        "mlp_layer1": "[768] â†’ [512] + ReLU + Dropout(0.1)",
        "mlp_layer2": "[512] â†’ [512] + ReLU + Dropout(0.1)",  
        "mlp_layer3": "[512] â†’ [num_topics] + Sigmoid"
    },
    
    "ì¶œë ¥_ë‹¨ê³„": {
        "topic_distribution": "[num_topics] âˆˆ [0,1]",
        "relevance_score": "scalar âˆˆ â„",
        "selected_demos": "List[str] of length k"
    }
}

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ (num_topics=1000, batch_size=32)
memory_breakdown = {
    "embeddings": "32 Ã— 768 Ã— 4 bytes = 98.3 KB",
    "topic_predictions": "32 Ã— 1000 Ã— 4 bytes = 128 KB", 
    "model_parameters": "768Ã—512 + 512Ã—512 + 512Ã—1000 = 1.2M params â‰ˆ 4.8 MB",
    "topical_knowledge": "1000 Ã— 4 bytes = 4 KB"
}
```

#### ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„ ì² í•™ ë¶„ì„

**ì™œ ì´ëŸ° êµ¬ì¡°ë¡œ ì„¤ê³„í–ˆì„ê¹Œ?**

1. **2-Stage Pipelineì˜ í•„ìš”ì„±**
```python
# Alternative 1: End-to-end í•™ìŠµ
ë¬¸ì œì  = {
    "ë°ì´í„°_ë¶€ì¡±": "ê° í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ìµœì  ì‹œì—° ë¼ë²¨ë§ í•„ìš” (í˜„ì‹¤ì  ë¶ˆê°€ëŠ¥)",
    "ì¼ë°˜í™”_ì–´ë ¤ì›€": "íŠ¹ì • í…ŒìŠ¤íŠ¸-ì‹œì—° ìŒì—ë§Œ ìµœì í™”",
    "í™•ì¥ì„±_ë¬¸ì œ": "ìƒˆ ë„ë©”ì¸ë§ˆë‹¤ ì¬í›ˆë ¨ í•„ìš”"
}

# TopicKì˜ 2-Stage ì ‘ê·¼
ì¥ì  = {
    "ëª¨ë“ˆí™”": "í† í”½ ì˜ˆì¸¡ê³¼ ê²€ìƒ‰ ë¡œì§ ë¶„ë¦¬ â†’ ê°ê° ìµœì í™” ê°€ëŠ¥",
    "ì¬ì‚¬ìš©ì„±": "í•œë²ˆ í›ˆë ¨ëœ í† í”½ ì˜ˆì¸¡ê¸°ë¥¼ ì—¬ëŸ¬ íƒœìŠ¤í¬ì— í™œìš©",
    "í•´ì„ì„±": "í† í”½ ë ˆë²¨ì—ì„œ ì„ íƒ ì´ìœ  ëª…í™•íˆ íŒŒì•… ê°€ëŠ¥"
}
```

2. **ê²½ëŸ‰ Topic Predictorì˜ ê·¼ê±°**
```python
# Alternative: ëŒ€í˜• ì–¸ì–´ëª¨ë¸ ì§ì ‘ í™œìš©
ëŒ€í˜•_ëª¨ë¸_ë¬¸ì œ = {
    "ì†ë„": "BERT-large ê¸°ì¤€ ~100ms vs MLP ~1ms",
    "ë©”ëª¨ë¦¬": "1.3GB vs 5MB (260ë°° ì°¨ì´)",
    "ë°°í¬": "ì„œë²„ ìì› ë§ì´ í•„ìš” vs ì—£ì§€ ë””ë°”ì´ìŠ¤ ê°€ëŠ¥"
}

# 3-Layer MLP ì„ íƒ ê·¼ê±°
mlp_ì¥ì  = {
    "ì¶©ë¶„í•œ_í‘œí˜„ë ¥": "ë¹„ì„ í˜• ë³€í™˜ 2ë²ˆìœ¼ë¡œ ë³µì¡í•œ í† í”½ ë§¤í•‘ ê°€ëŠ¥",
    "ê³¼ì í•©_ë°©ì§€": "Dropoutìœ¼ë¡œ ì •ê·œí™”, ë„ˆë¬´ ê¹Šì§€ ì•Šì•„ ì•ˆì •ì ",
    "ë¹ ë¥¸_ì¶”ë¡ ": "í–‰ë ¬ ê³±ì…ˆ 3ë²ˆìœ¼ë¡œ ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ ì²˜ë¦¬"
}
```

### ğŸ¯ Layer 2: íŒŒë¼ë¯¸í„° ì§„í™” ë¶„ì„
**"ë¬´ì—‡ì„ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ê°€?"**

#### ğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜

**ì´ˆê¸°í™” â†’ í•™ìŠµ ì¤‘ê°„ â†’ ìˆ˜ë ´ ê³¼ì • ì¶”ì **

```python
# Epoch 0: ëœë¤ ì´ˆê¸°í™”
W1_init = torch.randn(768, 512) * 0.01  # Xavier ì´ˆê¸°í™”
W2_init = torch.randn(512, 512) * 0.01
W3_init = torch.randn(512, 1000) * 0.01

# ì˜ˆì¸¡ ê²°ê³¼: ê±°ì˜ ëœë¤ (sigmoid â†’ ëª¨ë“  í† í”½ ~0.5)
ì´ˆê¸°_ì˜ˆì¸¡ = {
    "herbivore_demo": {"herbivore": 0.52, "carnivore": 0.48, "plant": 0.51},
    "ì •í™•ë„": "ê±°ì˜ ìš°ì—° ìˆ˜ì¤€ (~50%)",
    "í† í”½_ë¶„ë³„": "ì˜ë¯¸ìˆëŠ” íŒ¨í„´ ì—†ìŒ"
}

# Epoch 100: íŒ¨í„´ í•™ìŠµ ì‹œì‘  
í•™ìŠµ_ì¤‘ê°„ = {
    "W1": "ì…ë ¥ ì„ë² ë”©ì˜ ì˜ë¯¸ë¡ ì  íŠ¹ì„± í¬ì°© ì‹œì‘",
    "W2": "í† í”½ ê°„ ìƒê´€ê´€ê³„ í•™ìŠµ (herbivore â†” plant ê°•í•œ ì—°ê²°)",
    "W3": "ê° í† í”½ë³„ ë¶„ë¥˜ ê²½ê³„ í˜•ì„±"
}

ì¤‘ê°„_ì˜ˆì¸¡ = {
    "herbivore_demo": {"herbivore": 0.78, "carnivore": 0.23, "plant": 0.65},
    "ì •í™•ë„": "~75% (ìƒë‹¹í•œ ê°œì„ )",
    "í† í”½_ë¶„ë³„": "ê´€ë ¨ í† í”½ë“¤ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘"
}

# Epoch 1000: ìˆ˜ë ´ ìƒíƒœ
ìˆ˜ë ´_ìƒíƒœ = {
    "W1": "ì„ë² ë”© ê³µê°„ì˜ ì˜ë¯¸ë¡ ì  êµ¬ì¡° ì™„ì „ í•™ìŠµ",
    "W2": "í† í”½ ê³„ì¸µêµ¬ì¡° ë‚´ì¬í™” (ë™ë¬¼ > ì´ˆì‹ë™ë¬¼ > êµ¬ì²´ì  íŠ¹ì„±)",
    "W3": "ê° í† í”½ì— ëŒ€í•œ ì •í™•í•œ ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ í™•ë¦½"
}

ìµœì¢…_ì˜ˆì¸¡ = {
    "herbivore_demo": {"herbivore": 0.92, "carnivore": 0.08, "plant": 0.73},
    "ì •í™•ë„": "~90% (ì¸ê°„ ìˆ˜ì¤€ ê·¼ì ‘)",
    "í† í”½_ë¶„ë³„": "ì„¸ë°€í•œ í† í”½ êµ¬ë¶„ + ê´€ë ¨ì„± íŒŒì•…"
}
```

#### ğŸ”¬ íŒŒë¼ë¯¸í„°ë³„ ì—­í•  ë¶„ì„

**ê° ë ˆì´ì–´ê°€ í•™ìŠµí•˜ëŠ” íŠ¹í™”ëœ ê¸°ëŠ¥**

```python
def analyze_learned_representations():
    """í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë“¤ì˜ ì˜ë¯¸ í•´ì„"""
    
    # W1 (First Layer): ì„ë² ë”© â†’ ì˜ë¯¸ì  íŠ¹ì„± ì¶”ì¶œ
    W1_analysis = {
        "ë‰´ëŸ°_1": "ë™ë¬¼ ê´€ë ¨ ë‹¨ì–´ë“¤ì— ê°•í•˜ê²Œ ë°˜ì‘ (animal, organism, creature)",
        "ë‰´ëŸ°_2": "ë¨¹ì´ ê´€ë ¨ íŒ¨í„´ ê°ì§€ (eat, consume, feed, digest)",
        "ë‰´ëŸ°_3": "ì‹ë¬¼ íŠ¹ì„± ì¸ì‹ (plant, vegetation, photosynthesis)",
        "ë‰´ëŸ°_N": "ê°ê°ì´ íŠ¹ì • ì˜ë¯¸ ì˜ì—­ì˜ feature detector ì—­í• "
    }
    
    # W2 (Second Layer): íŠ¹ì„± ì¡°í•© â†’ í† í”½ ì›í˜• ìƒì„±
    W2_analysis = {
        "ì¡°í•©_íŒ¨í„´": "W1ì˜ íŠ¹ì„±ë“¤ì„ ì¡°í•©í•˜ì—¬ í† í”½ ì›í˜• ìƒì„±",
        "ì˜ˆì‹œ": "ë™ë¬¼íŠ¹ì„± + ì‹ë¬¼ì„­ì·¨íŠ¹ì„± â†’ ì´ˆì‹ë™ë¬¼ ì›í˜•",
        "ìƒí˜¸ì‘ìš©": "í† í”½ ê°„ ìœ ì‚¬ì„±/ì°¨ì´ì  í•™ìŠµ (carnivore vs herbivore)",
        "ê³„ì¸µì„±": "ìƒìœ„-í•˜ìœ„ í† í”½ ê´€ê³„ ë‚´ì¬í™”"
    }
    
    # W3 (Output Layer): í† í”½ ì›í˜• â†’ í™•ë¥  ë¶„í¬
    W3_analysis = {
        "ë¶„ë¥˜_ê²½ê³„": "ê° í† í”½ì— ëŒ€í•œ ì´ì§„ ë¶„ë¥˜ê¸° ì—­í• ",
        "ë³´ì •": "í† í”½ë³„ ë¹ˆë„/ì¤‘ìš”ë„ì— ë”°ë¥¸ threshold ìë™ ì¡°ì •",
        "ìƒí˜¸ë°°íƒ€ì„±": "mutually exclusive í† í”½ë“¤ ê°„ ê²½ìŸ í•™ìŠµ"
    }
    
    return W1_analysis, W2_analysis, W3_analysis
```

#### âš¡ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë¶„ì„

**ì—­ì „íŒŒì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì–´ë–»ê²Œ íë¥´ëŠ”ì§€ ë‹¨ê³„ë³„ ì¶”ì **

```python
def trace_gradient_flow():
    """ê·¸ë˜ë””ì–¸íŠ¸ ì—­ì „íŒŒ ê³¼ì • ìƒì„¸ ë¶„ì„"""
    
    # Forward Pass
    forward_flow = """
    e_d [768] 
    â†’ h1 = ReLU(W1 @ e_d) [512]
    â†’ h2 = ReLU(W2 @ h1) [512]  
    â†’ logits = W3 @ h2 [1000]
    â†’ tÌ‚_d = sigmoid(logits) [1000]
    â†’ loss = BCE(tÌ‚_d, t_d)
    """
    
    # Backward Pass  
    backward_flow = """
    âˆ‚L/âˆ‚tÌ‚_d = (tÌ‚_d - t_d) / (tÌ‚_d * (1 - tÌ‚_d))  [1000]
    
    â†“ sigmoid ì—­ì „íŒŒ
    âˆ‚L/âˆ‚logits = âˆ‚L/âˆ‚tÌ‚_d * tÌ‚_d * (1 - tÌ‚_d)  [1000]
    
    â†“ W3 ì—­ì „íŒŒ  
    âˆ‚L/âˆ‚W3 = âˆ‚L/âˆ‚logits @ h2.T  [1000, 512]
    âˆ‚L/âˆ‚h2 = W3.T @ âˆ‚L/âˆ‚logits  [512]
    
    â†“ ReLU ì—­ì „íŒŒ (h2 > 0ì¼ ë•Œë§Œ í†µê³¼)
    âˆ‚L/âˆ‚h2_pre = âˆ‚L/âˆ‚h2 * (h2 > 0)  [512]
    
    â†“ W2 ì—­ì „íŒŒ
    âˆ‚L/âˆ‚W2 = âˆ‚L/âˆ‚h2_pre @ h1.T  [512, 512]  
    âˆ‚L/âˆ‚h1 = W2.T @ âˆ‚L/âˆ‚h2_pre  [512]
    
    â†“ ReLU + W1 ì—­ì „íŒŒ
    âˆ‚L/âˆ‚W1 = (âˆ‚L/âˆ‚h1 * (h1 > 0)) @ e_d.T  [512, 768]
    """
    
    # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ë¶„ì„
    gradient_magnitude = {
        "âˆ‚L/âˆ‚W3": "ê°€ì¥ í° ê·¸ë˜ë””ì–¸íŠ¸ - ì§ì ‘ì ì¸ ì¶œë ¥ ì˜í–¥",
        "âˆ‚L/âˆ‚W2": "ì¤‘ê°„ í¬ê¸° - ReLUë¡œ ì¸í•œ ì¼ë¶€ ì‹ í˜¸ ì†Œì‹¤",
        "âˆ‚L/âˆ‚W1": "ê°€ì¥ ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ - vanishing í˜„ìƒ ì£¼ì˜"
    }
    
    return backward_flow, gradient_magnitude

# ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
def gradient_clipping():
    """ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼ ë°©ì§€"""
    
    total_norm = torch.sqrt(sum(p.grad.data.norm() ** 2 for p in model.parameters()))
    
    if total_norm > max_grad_norm:  # ì¼ë°˜ì ìœ¼ë¡œ 1.0
        for p in model.parameters():
            p.grad.data *= (max_grad_norm / total_norm)
            
    return "ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì •ê·œí™” ì™„ë£Œ"
```

### ğŸ¨ Layer 3: ì¶œë ¥ ìƒì„± ë©”ì»¤ë‹ˆì¦˜
**"ìµœì¢… ë‹µì„ ì–´ë–»ê²Œ ë§Œë“œëŠ”ê°€?"**

#### ğŸ” êµ¬ì²´ì  ì˜ˆì‹œë¡œ ì¶œë ¥ ê³¼ì • ì¶”ì 

**ì˜ˆì‹œ: "Non-human organisms that mainly consume plants are known as what?" ì§ˆë¬¸ ì²˜ë¦¬**

```python
def trace_output_generation():
    """TopicKê°€ ì–´ë–»ê²Œ ì‹œì—°ì„ ì„ íƒí•˜ëŠ”ì§€ ë‹¨ê³„ë³„ ì¶”ì """
    
    # Step 1: í…ŒìŠ¤íŠ¸ ì…ë ¥ í† í”½ ë¶„ì„
    test_input = "Non-human organisms that mainly consume plants are known as what?"
    test_embedding = sentence_transformer.encode(test_input)  # [768]
    
    required_topics = topic_predictor(test_embedding)  # [1000]
    print("í•„ìš” í† í”½ (ìƒìœ„ 5ê°œ):")
    print(f"herbivore: 0.87, carnivore: 0.91, omnivore: 0.90, plant: 0.34, animal: 0.18")
    
    # Step 2: í›„ë³´ ì‹œì—°ë“¤ì˜ í† í”½ ë¶„í¬ ê³„ì‚°
    candidates = [
        "Herbivores are animals that primarily eat plants",
        "Carnivores hunt and eat other animals", 
        "Omnivores eat both plants and animals",
        "Photosynthesis converts sunlight into energy"
    ]
    
    candidate_topics = {}
    for i, cand in enumerate(candidates):
        cand_embedding = sentence_transformer.encode(cand)
        cand_topics = topic_predictor(cand_embedding)
        candidate_topics[f"cand_{i}"] = cand_topics
    
    # Step 3: ëª¨ë¸ ì§€ì‹ ê°€ì¤‘ ê´€ë ¨ì„± ê³„ì‚°
    model_knowledge = torch.tensor([0.75, 0.72, 0.85, 0.77, 0.78, ...])  # [1000]
    
    relevance_scores = {}
    for cand_id, cand_topic_dist in candidate_topics.items():
        # r(x,d) = âŸ¨tÌ‚_x âŠ˜ tÌ‚_LM, tÌ‚_dâŸ©
        knowledge_weighted_test = required_topics / torch.clamp(model_knowledge, min=1e-8)
        relevance = torch.dot(knowledge_weighted_test, cand_topic_dist)
        relevance_scores[cand_id] = relevance.item()
    
    print("\nê´€ë ¨ì„± ì ìˆ˜:")
    print(f"cand_0 (Herbivore): {relevance_scores['cand_0']:.3f}")  # ë†’ì€ ì ìˆ˜ ì˜ˆìƒ
    print(f"cand_1 (Carnivore): {relevance_scores['cand_1']:.3f}")  
    print(f"cand_2 (Omnivore): {relevance_scores['cand_2']:.3f}")
    print(f"cand_3 (Photosynthesis): {relevance_scores['cand_3']:.3f}")  # ë‚®ì€ ì ìˆ˜ ì˜ˆìƒ
    
    # Step 4: ëˆ„ì  ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ì„ íƒ ê³¼ì •
    selected = []
    remaining = list(range(len(candidates)))
    cumulative_coverage = torch.zeros(1000)  # [num_topics]
    
    for round_idx in range(3):  # 3ê°œ ì‹œì—° ì„ íƒ
        best_score = -float('inf')
        best_cand = None
        
        for cand_idx in remaining:
            # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
            base_score = relevance_scores[f'cand_{cand_idx}']
            
            # ëˆ„ì  ì»¤ë²„ë¦¬ì§€ ê³ ë ¤ (2ë¼ìš´ë“œë¶€í„°)
            if round_idx > 0:
                overlap_penalty = torch.dot(
                    candidate_topics[f'cand_{cand_idx}'], 
                    cumulative_coverage
                ).item() * 0.3  # ì˜¤ë²„ë© í˜ë„í‹°
                
                adjusted_score = base_score - overlap_penalty
            else:
                adjusted_score = base_score
            
            if adjusted_score > best_score:
                best_score = adjusted_score
                best_cand = cand_idx
        
        # ìµœê³  ì ìˆ˜ í›„ë³´ ì„ íƒ
        selected.append(best_cand)
        remaining.remove(best_cand)
        
        # ëˆ„ì  ì»¤ë²„ë¦¬ì§€ ì—…ë°ì´íŠ¸  
        cumulative_coverage = torch.max(
            cumulative_coverage, 
            candidate_topics[f'cand_{best_cand}']
        )
        
        print(f"\nRound {round_idx + 1}: ì„ íƒëœ ì‹œì—° {best_cand}")
        print(f"ë‚´ìš©: {candidates[best_cand]}")
        print(f"ì ìˆ˜: {best_score:.3f}")
        print(f"ëˆ„ì  ì»¤ë²„ë¦¬ì§€: {cumulative_coverage.sum().item():.1f}")
    
    return selected
```

#### ğŸ“Š í™•ë¥  ë¶„í¬ í˜•ì„± ê³¼ì •

**í† í”½ë³„ í™•ë¥ ì´ ì–´ë–»ê²Œ ìµœì¢… ì„ íƒìœ¼ë¡œ ì´ì–´ì§€ëŠ”ì§€**

```python
def analyze_probability_formation():
    """í† í”½ í™•ë¥  â†’ ê´€ë ¨ì„± ì ìˆ˜ â†’ ìµœì¢… ì„ íƒ ë³€í™˜ ê³¼ì •"""
    
    # 1. í† í”½ ì˜ˆì¸¡ê¸° ì¶œë ¥ (sigmoid í›„)
    raw_logits = torch.tensor([2.1, -0.8, 1.5, 0.3, ...])  # MLP ì¶œë ¥
    topic_probs = torch.sigmoid(raw_logits)  # [0.89, 0.31, 0.82, 0.57, ...]
    
    # 2. ëª¨ë¸ ì§€ì‹ ê°€ì¤‘ì¹˜ ì ìš©
    model_knowledge = torch.tensor([0.75, 0.85, 0.60, 0.90, ...])
    knowledge_weighted = topic_probs / model_knowledge  # [1.19, 0.36, 1.37, 0.63, ...]
    
    # 3. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìš”êµ¬ì‚¬í•­ê³¼ ë‚´ì 
    test_requirements = torch.tensor([0.87, 0.15, 0.92, 0.23, ...])
    final_relevance = torch.dot(knowledge_weighted, test_requirements)  # scalar
    
    # 4. ì˜ë¯¸ì  ìœ ì‚¬ë„ì™€ ê²°í•©
    semantic_similarity = 0.73  # cosine similarity
    combined_score = final_relevance + 0.5 * semantic_similarity
    
    print("í™•ë¥  í˜•ì„± ê³¼ì •:")
    print(f"Raw Logits â†’ Sigmoid: {raw_logits[0]:.2f} â†’ {topic_probs[0]:.3f}")
    print(f"Knowledge Weighted: {topic_probs[0]:.3f} / {model_knowledge[0]:.2f} = {knowledge_weighted[0]:.3f}")
    print(f"Test Alignment: {knowledge_weighted[0]:.3f} * {test_requirements[0]:.3f} = {(knowledge_weighted[0] * test_requirements[0]):.3f}")
    print(f"Final Relevance: {final_relevance:.3f}")
    print(f"Combined Score: {combined_score:.3f}")
    
    return combined_score

def analyze_selection_mechanism():
    """ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ì˜ í™•ë¥ ì  vs ê²°ì •ì  íŠ¹ì„±"""
    
    selection_properties = {
        "greedy_nature": {
            "íŠ¹ì„±": "ê° ë¼ìš´ë“œì—ì„œ ìµœê³  ì ìˆ˜ ì‹œì—°ì„ í™•ì •ì ìœ¼ë¡œ ì„ íƒ",
            "ì¥ì ": "ë¹ ë¥´ê³  ì•ˆì •ì , ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼",
            "ë‹¨ì ": "ì§€ì—­ ìµœì í•´ì— ë¹ ì§ˆ ê°€ëŠ¥ì„±"
        },
        
        "diversity_mechanism": {
            "íŠ¹ì„±": "ëˆ„ì  ì»¤ë²„ë¦¬ì§€ë¡œ ë‹¤ì–‘ì„± ë³´ì¥",  
            "íš¨ê³¼": "í›„ì† ì„ íƒì—ì„œ ì¤‘ë³µ í† í”½ í˜ë„í‹° ì ìš©",
            "ê²°ê³¼": "ì „ì²´ì ìœ¼ë¡œ ê· í˜•ì¡íŒ í† í”½ ì»¤ë²„ë¦¬ì§€"
        },
        
        "adaptation_potential": {
            "ì•„ì´ë””ì–´": "í™•ë¥ ì  ì„ íƒìœ¼ë¡œ exploration ì¶”ê°€",
            "êµ¬í˜„": "softmax temperatureë¡œ top-kì—ì„œ í™•ë¥ ì  ìƒ˜í”Œë§",
            "íš¨ê³¼": "ë” ë‹¤ì–‘í•œ ì‹œì—° ì¡°í•© íƒìƒ‰ ê°€ëŠ¥"
        }
    }
    
    return selection_properties
```

### ğŸ“Š Layer 4: ì†ì‹¤í•¨ìˆ˜ì™€ ìµœì í™”
**"ì–¼ë§ˆë‚˜ í‹€ë ¸ê³  ì–´ë–»ê²Œ ê°œì„ í•˜ëŠ”ê°€?"**

#### ğŸ¯ ì†ì‹¤í•¨ìˆ˜ ì„¤ê³„ ì² í•™

**ì™œ ì´ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì„ íƒí–ˆëŠ”ê°€?**

```python
def analyze_loss_function_design():
    """TopicKì˜ ì†ì‹¤í•¨ìˆ˜ ì„¤ê³„ ê·¼ê±° ë¶„ì„"""
    
    # ì„ íƒëœ ì†ì‹¤: Binary Cross-Entropy with Soft Labels
    chosen_loss = """
    L_TP = -âˆ‘_d [âˆ‘_{tâˆˆT_d} t_{d,t} log tÌ‚_{d,t} + âˆ‘_{tâˆ‰T_d} log(1 - tÌ‚_{d,t})]
    """
    
    design_rationale = {
        "BCE_ì„ íƒ_ì´ìœ ": {
            "ì í•©ì„±": "ê° í† í”½ì„ ë…ë¦½ì ì¸ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ëª¨ë¸ë§",
            "ìœ ì—°ì„±": "í•œ ì‹œì—°ì´ ì—¬ëŸ¬ í† í”½ì„ ë™ì‹œì— ê°€ì§ˆ ìˆ˜ ìˆìŒ",
            "í™•ë¥ _í•´ì„": "í† í”½ ë©¤ë²„ì‹­ì„ í™•ë¥ ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•´ì„"
        },
        
        "Soft_Label_í•„ìš”ì„±": {
            "í˜„ì‹¤_ë°˜ì˜": "í† í”½ ë©¤ë²„ì‹­ì´ binaryê°€ ì•„ë‹Œ continuous",
            "êµ¬ë³„ì„±_ë°˜ì˜": "ì¼ë°˜ì  í† í”½ë³´ë‹¤ íŠ¹ìˆ˜í•œ í† í”½ì— ë†’ì€ ê°€ì¤‘ì¹˜",
            "í•™ìŠµ_ì•ˆì •ì„±": "hard labelë³´ë‹¤ gradientê°€ ë¶€ë“œëŸ½ê²Œ íë¦„"
        },
        
        "Alternative_ë¹„êµ": {
            "CrossEntropy": "ìƒí˜¸ë°°íƒ€ì  í† í”½ ê°€ì • â†’ ë¶€ì ì ˆ",
            "MSE": "í™•ë¥  í•´ì„ ì–´ë ¤ì›€, outlierì— ë¯¼ê°",
            "Focal Loss": "class imbalance ì‹¬í•˜ì§€ ì•Šì•„ ë¶ˆí•„ìš”"
        }
    }
    
    return design_rationale

# ì†ì‹¤í•¨ìˆ˜ ë™ì‘ ì‹œë®¬ë ˆì´ì…˜
def simulate_loss_behavior():
    """ì†ì‹¤ ê°’ì´ í•™ìŠµ ê³¼ì •ì—ì„œ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€"""
    
    # ì´ˆê¸° ìƒíƒœ (ëœë¤ ì˜ˆì¸¡)
    initial_state = {
        "prediction": torch.tensor([0.5, 0.5, 0.5, 0.5]),  # ëª¨ë“  í† í”½ 0.5 í™•ë¥ 
        "target": torch.tensor([0.9, 0.0, 0.7, 0.3]),     # ì‹¤ì œ í† í”½ ë¶„í¬
        "loss": 0.693  # -log(0.5) â‰ˆ 0.693 for all topics
    }
    
    # í•™ìŠµ ì¤‘ê°„ (íŒ¨í„´ í•™ìŠµ ì‹œì‘)
    intermediate_state = {
        "prediction": torch.tensor([0.78, 0.23, 0.65, 0.42]),
        "target": torch.tensor([0.9, 0.0, 0.7, 0.3]),
        "loss": 0.234  # ìƒë‹¹í•œ ê°ì†Œ
    }
    
    # ìˆ˜ë ´ ìƒíƒœ (ê±°ì˜ ì •í™•í•œ ì˜ˆì¸¡)
    converged_state = {
        "prediction": torch.tensor([0.92, 0.08, 0.73, 0.28]),
        "target": torch.tensor([0.9, 0.0, 0.7, 0.3]),
        "loss": 0.045  # ë§¤ìš° ë‚®ì€ ì†ì‹¤
    }
    
    return initial_state, intermediate_state, converged_state
```

#### âš™ï¸ ìµœì í™” ì „ëµ ë¶„ì„

**í•™ìŠµ ì¤‘ ì†ì‹¤ê°’ ë³€í™”ì™€ ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒì˜ ì—°ê²°**

```python
def analyze_optimization_dynamics():
    """ìµœì í™” ê³¼ì •ì˜ ì—­í•™ ê´€ê³„ ë¶„ì„"""
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ì „ëµ
    learning_schedule = {
        "warmup_phase": {
            "epochs": "0-10",
            "lr": "0 â†’ 1e-4 (linear warmup)",
            "ëª©ì ": "ì´ˆê¸° í° ê·¸ë˜ë””ì–¸íŠ¸ë¡œ ì¸í•œ ë¶ˆì•ˆì •ì„± ë°©ì§€"
        },
        
        "steady_phase": {  
            "epochs": "10-80",
            "lr": "1e-4 (constant)",
            "ëª©ì ": "ì•ˆì •ì ì¸ íŒ¨í„´ í•™ìŠµ"
        },
        
        "fine_tuning_phase": {
            "epochs": "80-100", 
            "lr": "1e-4 â†’ 1e-5 (cosine decay)",
            "ëª©ì ": "ì„¸ë°€í•œ í† í”½ ê²½ê³„ ì¡°ì •"
        }
    }
    
    # ì†ì‹¤ vs ì„±ëŠ¥ ìƒê´€ê´€ê³„
    loss_performance_correlation = {
        "training_loss": {
            "epoch_0": {"loss": 0.693, "topic_accuracy": 0.50},
            "epoch_20": {"loss": 0.420, "topic_accuracy": 0.68}, 
            "epoch_50": {"loss": 0.180, "topic_accuracy": 0.82},
            "epoch_100": {"loss": 0.089, "topic_accuracy": 0.91}
        },
        
        "downstream_performance": {
            "topic_accuracy_0.50": {"icl_accuracy": 0.45, "coverage": 0.32},
            "topic_accuracy_0.68": {"icl_accuracy": 0.52, "coverage": 0.48},
            "topic_accuracy_0.82": {"icl_accuracy": 0.61, "coverage": 0.67},
            "topic_accuracy_0.91": {"icl_accuracy": 0.68, "coverage": 0.84}
        }
    }
    
    # ìµœì í™” ì¥ì• ë¬¼ê³¼ í•´ê²°ì±…
    optimization_challenges = {
        "vanishing_gradients": {
            "ë¬¸ì œ": "ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ˆê¸° ë ˆì´ì–´ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤",
            "í•´ê²°": "ì ì ˆí•œ ì´ˆê¸°í™” + ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘",
            "ëª¨ë‹ˆí„°ë§": "ê° ë ˆì´ì–´ë³„ ê·¸ë˜ë””ì–¸íŠ¸ norm ì¶”ì "
        },
        
        "class_imbalance": {
            "ë¬¸ì œ": "ì¼ë¶€ í† í”½ì´ ë§¤ìš° ë“œë¬¼ê²Œ ë“±ì¥",
            "í•´ê²°": "distinctiveness-aware soft labelingìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•´ê²°",
            "íš¨ê³¼": "ë“œë¬¸ í† í”½ì— ìë™ìœ¼ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬"
        },
        
        "overfitting": {
            "ë¬¸ì œ": "ì‘ì€ í† í”½ ë°ì´í„°ì— ê³¼ì í•©",
            "í•´ê²°": "Dropout(0.1) + early stopping", 
            "ê²€ì¦": "held-out validation setìœ¼ë¡œ ëª¨ë‹ˆí„°ë§"
        }
    }
    
    return learning_schedule, loss_performance_correlation, optimization_challenges

def trace_parameter_updates():
    """íŒŒë¼ë¯¸í„°ê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸ë˜ëŠ”ì§€ ì¶”ì """
    
    # Adam optimizer ë™ì‘ ê³¼ì •
    adam_dynamics = {
        "momentum_estimation": "m_t = Î²â‚ * m_{t-1} + (1-Î²â‚) * g_t",
        "variance_estimation": "v_t = Î²â‚‚ * v_{t-1} + (1-Î²â‚‚) * g_tÂ²", 
        "bias_correction": "mÌ‚_t = m_t / (1-Î²â‚^t), vÌ‚_t = v_t / (1-Î²â‚‚^t)",
        "parameter_update": "Î¸_t = Î¸_{t-1} - Î± * mÌ‚_t / (âˆšvÌ‚_t + Îµ)"
    }
    
    # ì‹¤ì œ íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ ë¶„ì„
    parameter_change_analysis = {
        "W1_changes": {
            "ì´ˆê¸°": "í° ë³€í™”ëŸ‰ (0.01~0.1), ì„ë² ë”© ê³µê°„ íƒìƒ‰",
            "ì¤‘ê°„": "ì¤‘ê°„ ë³€í™”ëŸ‰ (0.001~0.01), íŒ¨í„´ ì„¸ë¶„í™”", 
            "í›„ê¸°": "ì‘ì€ ë³€í™”ëŸ‰ (0.0001~0.001), ë¯¸ì„¸ ì¡°ì •"
        },
        
        "W3_changes": {
            "íŠ¹ì§•": "W1ë³´ë‹¤ ë” í° ë³€í™”ëŸ‰, ì§ì ‘ì ì¸ ì¶œë ¥ ì˜í–¥",
            "íŒ¨í„´": "íŠ¹ì • í† í”½ì— ëŒ€í•œ ê°•í•œ ê°€ì¤‘ì¹˜ í˜•ì„±",
            "ì•ˆì •ì„±": "í›„ê¸°ì—ë„ ìƒëŒ€ì ìœ¼ë¡œ í° ì—…ë°ì´íŠ¸ ìœ ì§€"
        }
    }
    
    return adam_dynamics, parameter_change_analysis
```

#### ğŸ”„ ì„±ëŠ¥ í–¥ìƒ ë©”ì»¤ë‹ˆì¦˜

**ì†ì‹¤ ê°ì†Œê°€ ì–´ë–»ê²Œ ì‹¤ì œ ê²€ìƒ‰ ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§€ëŠ”ê°€?**

```python
def connect_loss_to_performance():
    """ì†ì‹¤í•¨ìˆ˜ ìµœì í™” â†’ ì‹¤ì œ ì„±ëŠ¥ í–¥ìƒ ì—°ê²°ê³ ë¦¬"""
    
    improvement_pathway = {
        "Step_1_í† í”½_ë¶„ë¥˜_ì •í™•ë„": {
            "ë³€í™”": "BCE loss ê°ì†Œ â†’ í† í”½ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ",
            "ì¸¡ì •": "F1-score: 0.50 â†’ 0.91",
            "ì˜ë¯¸": "ì‹œì—°ì˜ í† í”½ì„ ì •í™•íˆ íŒŒì•…"
        },
        
        "Step_2_ê´€ë ¨ì„±_ì ìˆ˜_í’ˆì§ˆ": {
            "ë³€í™”": "ì •í™•í•œ í† í”½ ì˜ˆì¸¡ â†’ ë” ì •í™•í•œ ê´€ë ¨ì„± ì ìˆ˜",
            "ì¸¡ì •": "ê´€ë ¨ì„±-ì‹¤ì œì„±ëŠ¥ ìƒê´€ê³„ìˆ˜: 0.23 â†’ 0.78", 
            "ì˜ë¯¸": "ì ìˆ˜ ë†’ì€ ì‹œì—°ì´ ì‹¤ì œë¡œ ë„ì›€ë¨"
        },
        
        "Step_3_ì‹œì—°_ì„ íƒ_í’ˆì§ˆ": {
            "ë³€í™”": "ì •í™•í•œ ê´€ë ¨ì„± ì ìˆ˜ â†’ ë” ë‚˜ì€ ì‹œì—° ì„ íƒ",
            "ì¸¡ì •": "ì„ íƒëœ ì‹œì—°ì˜ í‰ê·  ìœ ìš©ì„±: 0.52 â†’ 0.84",
            "ì˜ë¯¸": "í…ŒìŠ¤íŠ¸ ì…ë ¥ì— ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ì‹œì—° ì„ íƒ"
        },
        
        "Step_4_ICL_ì„±ëŠ¥_í–¥ìƒ": {
            "ë³€í™”": "ì¢‹ì€ ì‹œì—° ì„ íƒ â†’ ICL íƒœìŠ¤í¬ ì„±ëŠ¥ í–¥ìƒ",
            "ì¸¡ì •": "ìµœì¢… ì •í™•ë„: 44.34% â†’ 46.19% (ConE ëŒ€ë¹„)",
            "ì˜ë¯¸": "ì‚¬ìš©ìì—ê²Œ ê°€ì‹œì ì¸ ì„±ëŠ¥ ê°œì„ "
        }
    }
    
    # ì„±ëŠ¥ í–¥ìƒì˜ ë³µí•©ì  íš¨ê³¼
    compound_effects = {
        "í† í”½_ì»¤ë²„ë¦¬ì§€_íš¨ê³¼": {
            "before": "ì¤‘ë³µë˜ê±°ë‚˜ ê´€ë ¨ì—†ëŠ” ì‹œì—° ì„ íƒ",
            "after": "ì²´ê³„ì ì¸ ì§€ì‹ ì˜ì—­ ì»¤ë²„ë¦¬ì§€",
            "ê²°ê³¼": "ë” í¬ê´„ì ì¸ í•™ìŠµ ì‹ í˜¸ ì œê³µ"
        },
        
        "ëª¨ë¸_ì ì‘_íš¨ê³¼": {
            "before": "ëª¨ë¸ ì§€ì‹ ìƒíƒœ ë¬´ì‹œ",
            "after": "ëª¨ë¸ì´ ì•½í•œ ë¶€ë¶„ ìš°ì„  ë³´ê°•",
            "ê²°ê³¼": "íš¨ìœ¨ì ì¸ ì§€ì‹ ì „ì´"
        },
        
        "ë‹¤ì–‘ì„±_ë³´ì¥_íš¨ê³¼": {
            "before": "ìœ ì‚¬í•œ ì‹œì—°ë“¤ì˜ ì¤‘ë³µ ì„ íƒ", 
            "after": "ëˆ„ì  ì»¤ë²„ë¦¬ì§€ë¡œ ë‹¤ì–‘ì„± ë³´ì¥",
            "ê²°ê³¼": "ë” í’ë¶€í•œ í•™ìŠµ ë§¥ë½ ì œê³µ"
        }
    }
    
    return improvement_pathway, compound_effects
```

## ğŸ¯ 4-Layer ë¶„ì„ ì¢…í•©

### ğŸ”— ë ˆì´ì–´ ê°„ ìƒí˜¸ì‘ìš©

```python
cross_layer_interactions = {
    "Layer1_to_Layer2": {
        "ì—°ê²°": "ì•„í‚¤í…ì²˜ ì„¤ê³„ â†’ íŒŒë¼ë¯¸í„° í•™ìŠµ ë°©í–¥ ê²°ì •",
        "ì˜ˆì‹œ": "3-layer MLP êµ¬ì¡°ê°€ hierarchical feature learning ìœ ë„"
    },
    
    "Layer2_to_Layer3": {
        "ì—°ê²°": "í•™ìŠµëœ íŒŒë¼ë¯¸í„° â†’ ì¶œë ¥ ìƒì„± í’ˆì§ˆ ê²°ì •",
        "ì˜ˆì‹œ": "ì •í™•í•œ í† í”½ ë¶„ë¥˜ê¸° â†’ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ì„± ì ìˆ˜"
    },
    
    "Layer3_to_Layer4": {
        "ì—°ê²°": "ì¶œë ¥ í’ˆì§ˆ â†’ ì†ì‹¤ ê³„ì‚° ë° ë‹¤ìŒ í•™ìŠµ ë°©í–¥",
        "ì˜ˆì‹œ": "ì¢‹ì€ ì‹œì—° ì„ íƒ â†’ ë‚®ì€ downstream task loss"
    },
    
    "Layer4_to_Layer1": {
        "ì—°ê²°": "ìµœì í™” ê²°ê³¼ â†’ ì•„í‚¤í…ì²˜ í‰ê°€ ë° ê°œì„ ",
        "ì˜ˆì‹œ": "í•™ìŠµ ì•ˆì •ì„± ë¬¸ì œ â†’ ë ˆì´ì–´ ìˆ˜ë‚˜ activation ì¡°ì •"
    }
}
```

### ğŸ¨ ì„¤ê³„ì˜ ì¼ê´€ì„±

**ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì¼ê´€ëœ ì„¤ê³„ ì² í•™: "íš¨ìœ¨ì„± + í•´ì„ì„± + í™•ì¥ì„±"**

```python
design_consistency = {
    "íš¨ìœ¨ì„±": {
        "Layer1": "ê²½ëŸ‰ MLPë¡œ ë¹ ë¥¸ ì¶”ë¡ ",
        "Layer2": "ì‚¬ì „ ê³„ì‚° ê°€ëŠ¥í•œ topical knowledge", 
        "Layer3": "LLM ì¶”ë¡  ì—†ëŠ” ì‹¤ì‹œê°„ ì„ íƒ",
        "Layer4": "ê°„ë‹¨í•œ BCEë¡œ ë¹ ë¥¸ í•™ìŠµ"
    },
    
    "í•´ì„ì„±": {
        "Layer1": "í† í”½ ë‹¨ìœ„ë¡œ ë¶„í•´ ê°€ëŠ¥í•œ êµ¬ì¡°",
        "Layer2": "ê° íŒŒë¼ë¯¸í„°ì˜ ì—­í•  ëª…í™•",
        "Layer3": "ì„ íƒ ì´ìœ ë¥¼ í† í”½ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥",
        "Layer4": "ì†ì‹¤ì´ ì‹¤ì œ ì„±ëŠ¥ê³¼ ì§ê²°"
    },
    
    "í™•ì¥ì„±": {
        "Layer1": "ìƒˆ ë„ë©”ì¸ì— topic miningë§Œìœ¼ë¡œ ì ìš©",
        "Layer2": "pre-trained embedding í™œìš©",
        "Layer3": "ëª¨ë“  LLMì— ì ìš© ê°€ëŠ¥í•œ ë°©ì‹",
        "Layer4": "ë‹¤ì–‘í•œ downstream taskì— ì ìš©"
    }
}
```

ì´ 4-Layer ì™„ì „ë¶„í•´ë¥¼ í†µí•´ TopicKì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œê°€ ì–´ë–»ê²Œ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìµœì¢… ì„±ëŠ¥ì„ ë§Œë“¤ì–´ë‚´ëŠ”ì§€ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!