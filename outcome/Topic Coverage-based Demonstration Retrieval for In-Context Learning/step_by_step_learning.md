# Topic Coverage-based Demonstration Retrieval for In-Context Learning - ë‹¨ê³„ë³„ í•™ìŠµ ê²½ë¡œ

## ğŸƒâ€â™‚ï¸ ë‹¨ê³„ë³„ í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Level 1: ê¸°ë³¸ ì´í•´ (30ë¶„) - TopicKì˜ í•µì‹¬ ì•„ì´ë””ì–´ íŒŒì•…

#### âœ… í•™ìŠµ ëª©í‘œ
- [ ] In-Context Learningì˜ ì‹œì—° ì„ íƒ ë¬¸ì œ ì´í•´
- [ ] ê¸°ì¡´ ë°©ë²•ë“¤(ìœ ì‚¬ë„ ê¸°ë°˜, ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜)ì˜ í•œê³„ íŒŒì•…  
- [ ] TopicKì˜ í•µì‹¬ í˜ì‹ ì  3ê°€ì§€ ì„¤ëª… ê°€ëŠ¥
- [ ] "í† í”½ ì»¤ë²„ë¦¬ì§€"ê°€ ë¬´ì—‡ì¸ì§€ ì§ê´€ì  ì„¤ëª… ê°€ëŠ¥

#### ğŸ¯ í•µì‹¬ ì§ˆë¬¸ê³¼ ë‹µë³€

**Q1: ì´ ë…¼ë¬¸ì„ í•œ ì¤„ë¡œ ìš”ì•½í•˜ë©´?**
```
A: "In-Context Learningì—ì„œ ì‹œì—°ì„ ì„ íƒí•  ë•Œ, í…ŒìŠ¤íŠ¸ ì…ë ¥ì´ í•„ìš”ë¡œ í•˜ëŠ” í† í”½ë“¤ì„ 
   ì²´ê³„ì ìœ¼ë¡œ ì»¤ë²„í•˜ë©´ì„œë„ ëª¨ë¸ì´ ì•½í•œ ë¶€ë¶„ì„ ìš°ì„ ì ìœ¼ë¡œ ë³´ê°•í•˜ëŠ” ë°©ë²•"
```

**Q2: ê¸°ì¡´ ë°©ë²•ì˜ í•œê³„ 3ê°€ì§€ëŠ”?**
```
A: 1. ìœ ì‚¬ë„ ê¸°ë°˜: ëª¨ë¸ì˜ ì§€ì‹ ìƒíƒœë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
   2. ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜: í…ŒìŠ¤íŠ¸ ì‹œê°„ì— ëŠë¦¬ê³ , ë‹¤ì–‘ì„± ë¶€ì¡±
   3. ê³µí†µ ë¬¸ì œ: ì„¸ë¶„í™”ëœ í† í”½ ë ˆë²¨ì—ì„œì˜ ì²´ê³„ì  ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±
```

**Q3: TopicKì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ”?**
```
A: 1. í† í”½ ë ˆë²¨ì—ì„œ "ë¬´ì—‡ì´ í•„ìš”í•œì§€" íŒŒì•… (required topics)
   2. ëª¨ë¸ì´ "ë¬´ì—‡ì„ ì˜ ëª¨ë¥´ëŠ”ì§€" í‰ê°€ (topical knowledge)
   3. "ì•„ì§ ì•ˆ ë‹¤ë£¬ ì¤‘ìš”í•œ í† í”½" ìš°ì„  ì„ íƒ (cumulative coverage)
```

#### ğŸ“ ì§ê´€ì  ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

**ë¹„ìœ : ìš”ë¦¬ ë ˆì‹œí”¼ ì„ íƒí•˜ê¸°**
```python
ìƒí™© = "ì˜¤ëŠ˜ ì €ë…ì— ì´íƒˆë¦¬ì•„ ìš”ë¦¬ë¥¼ ë§Œë“¤ê³  ì‹¶ì–´ìš”"

# ê¸°ì¡´ ë°©ë²•ë“¤
ìœ ì‚¬ë„_ê¸°ë°˜ = "ì´íƒˆë¦¬ì•„ ìš”ë¦¬" í‚¤ì›Œë“œë¡œ ë¹„ìŠ·í•œ ë ˆì‹œí”¼ë“¤ë§Œ ì°¾ê¸°
ë¶ˆí™•ì‹¤ì„±_ê¸°ë°˜ = "ë‚´ê°€ ì‹¤íŒ¨í•  í™•ë¥ ì´ ë†’ì€ ë ˆì‹œí”¼" ìœ„ì£¼ë¡œ ì„ íƒ

# TopicK ë°©ë²•  
required_topics = ["íŒŒìŠ¤íƒ€", "í† ë§ˆí† ì†ŒìŠ¤", "ì¹˜ì¦ˆ", "í—ˆë¸Œ"]
my_knowledge = {"íŒŒìŠ¤íƒ€": 0.9, "í† ë§ˆí† ì†ŒìŠ¤": 0.3, "ì¹˜ì¦ˆ": 0.7, "í—ˆë¸Œ": 0.1}
ì„ íƒ_ìš°ì„ ìˆœìœ„ = "í—ˆë¸Œ â†’ í† ë§ˆí† ì†ŒìŠ¤ â†’ ì¹˜ì¦ˆ â†’ íŒŒìŠ¤íƒ€" ìˆœìœ¼ë¡œ ë ˆì‹œí”¼ ì„ íƒ
```

#### ğŸ” ì´í•´ë„ ì²´í¬
```python
# ìê°€ ì§„ë‹¨ í…œí”Œë¦¿
ì§ˆë¬¸_level_1 = [
    "TopicKê°€ í•´ê²°í•˜ë ¤ëŠ” í•µì‹¬ ë¬¸ì œëŠ”?",
    "ê¸°ì¡´ similarity-based ë°©ë²•ì˜ ê°€ì¥ í° ë¬¸ì œëŠ”?", 
    "í† í”½ ì»¤ë²„ë¦¬ì§€ë€ ë¬´ì—‡ì¸ê°€?",
    "'cumulative coverage'ê°€ ì™œ í•„ìš”í•œê°€?"
]

ë‹µë³€_ì˜ˆì‹œ = {
    "ë¬¸ì œ": "ICL ì‹œì—° ì„ íƒì—ì„œ ê´€ë ¨ì„±ê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ë³´ì¥í•˜ê¸°",
    "ê¸°ì¡´_í•œê³„": "ëª¨ë¸ì˜ ì§€ì‹ ìƒíƒœë¥¼ ëª¨ë¥´ê³  í‘œë©´ì  ìœ ì‚¬ë„ë§Œ ê³ ë ¤",
    "í† í”½_ì»¤ë²„ë¦¬ì§€": "í…ŒìŠ¤íŠ¸ ì…ë ¥ì´ ìš”êµ¬í•˜ëŠ” ì„¸ë¶€ ì§€ì‹ ì˜ì—­ë“¤ì„ ì–¼ë§ˆë‚˜ í¬ê´„í•˜ëŠ”ê°€",
    "ëˆ„ì _ì»¤ë²„ë¦¬ì§€": "ì¤‘ë³µì„ í”¼í•˜ê³  ìƒˆë¡œìš´ ì§€ì‹ ì˜ì—­ì„ ê³„ì† ì¶”ê°€í•˜ê¸° ìœ„í•´"
}
```

---

### Level 2: êµ¬ì¡° íŒŒì•… (45ë¶„) - TopicK ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì´í•´

#### âœ… í•™ìŠµ ëª©í‘œ
- [ ] TopicKì˜ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì´í•´
- [ ] í† í”½ ì˜ˆì¸¡ê¸°(Topic Predictor)ì˜ ì—­í• ê³¼ êµ¬ì¡° íŒŒì•…
- [ ] 3ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì˜ ìƒí˜¸ì‘ìš© ì´í•´
- [ ] ë°ì´í„° í”Œë¡œìš°ë¥¼ ë‹¨ê³„ë³„ë¡œ ì¶”ì  ê°€ëŠ¥

#### ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„

**Stage 1: Topical Knowledge Assessment**
```python
# ì…ë ¥: í›„ë³´ ì‹œì—° í’€ D = {(xâ‚,yâ‚), (xâ‚‚,yâ‚‚), ..., (xâ‚™,yâ‚™)}
# ê³¼ì •:
1. Topic_Mining(D) â†’ T = {topicâ‚, topicâ‚‚, ..., topicâ‚˜}
2. Topic_Matching(ê° ì‹œì—°, T) â†’ Tâ‚ (ê° ì‹œì—°ì˜ í•µì‹¬ í† í”½ë“¤)
3. Topic_Predictor_Training(ì„ë² ë”©, Tâ‚) â†’ f(eâ‚) = tÌ‚â‚
4. Topical_Knowledge_Estimation() â†’ tÌ‚_LM

# ì¶œë ¥: í›ˆë ¨ëœ í† í”½ ì˜ˆì¸¡ê¸° f()ì™€ ëª¨ë¸ ì§€ì‹ ë²¡í„° tÌ‚_LM
```

**Stage 2: Topic Coverage-based Retrieval**  
```python
# ì…ë ¥: í…ŒìŠ¤íŠ¸ ì…ë ¥ x, í›„ë³´ í’€ D, ì˜ˆì¸¡ê¸° f()
# ê³¼ì •:
For k rounds:
    1. Required_Topics(x) â†’ tÌ‚_x
    2. For each candidate d:
       - Covered_Topics(d) â†’ tÌ‚_d  
       - Relevance_Score(x, d) = âŸ¨tÌ‚_x âŠ˜ tÌ‚_LM, tÌ‚_dâŸ©
    3. Select_Best_Candidate() â†’ dáµ¢
    4. Update_Cumulative_Coverage() â†’ ì¤‘ë³µ í† í”½ ì œê±°

# ì¶œë ¥: ì„ íƒëœ kê°œ ì‹œì—° {dâ‚, dâ‚‚, ..., dâ‚–}
```

#### ğŸ“Š ì°¨ì› ë¶„ì„ (Tensor Dimension Tracking)

```python
# ë°ì´í„° í”Œë¡œìš° ì°¨ì› ì¶”ì 
batch_size = 32
seq_len = 128  
embed_dim = 768
num_topics = 1000
num_candidates = 10000

# Stage 1: Knowledge Assessment
embeddings = [batch_size, embed_dim]           # [32, 768]
topic_distributions = [batch_size, num_topics] # [32, 1000] 
topical_knowledge = [num_topics]               # [1000]

# Stage 2: Retrieval  
test_embedding = [embed_dim]                   # [768]
test_topics = [num_topics]                     # [1000]
candidate_embeddings = [num_candidates, embed_dim]    # [10000, 768]
candidate_topics = [num_candidates, num_topics]       # [10000, 1000]
relevance_scores = [num_candidates]                   # [10000]

print("âœ… ëª¨ë“  ì°¨ì›ì´ ì¼ì¹˜í•˜ë©´ êµ¬í˜„ ì„±ê³µ!")
```

#### ğŸ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì‹¬í™” ì´í•´

**1. Topic Predictor êµ¬ì¡°**
```python
class TopicPredictor(nn.Module):
    """3-layer MLP: embedding â†’ topic distribution"""
    
    def __init__(self, d_model=768, hidden=512, num_topics=1000):
        # Layer 1: [768] â†’ [512] + ReLU + Dropout
        # Layer 2: [512] â†’ [512] + ReLU + Dropout  
        # Layer 3: [512] â†’ [1000] + Sigmoid
        
    def forward(self, x):
        # x: [batch, 768] â†’ output: [batch, 1000]
        # ê° í† í”½ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ ì¶œë ¥
```

**2. Topical Knowledge ê³„ì‚°**
```python
# ìˆ˜ì‹: tÌ‚_LM,t = Î£(tÌ‚_d,t Ã— zero_shot(d)) / Î£(tÌ‚_d,t)
def compute_topical_knowledge():
    """ëª¨ë¸ì´ ê° í† í”½ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì•„ëŠ”ì§€ í‰ê°€"""
    
    for each_topic_t:
        ê´€ë ¨_ì‹œì—°ë“¤ = find_demonstrations_with_topic_t()
        ì •í™•ë„_í•©ê³„ = sum(í† í”½ê°€ì¤‘ì¹˜ Ã— zero_shot_ì •í™•ë„ for ì‹œì—° in ê´€ë ¨_ì‹œì—°ë“¤)
        ê°€ì¤‘ì¹˜_í•©ê³„ = sum(í† í”½ê°€ì¤‘ì¹˜ for ì‹œì—° in ê´€ë ¨_ì‹œì—°ë“¤)
        
        tÌ‚_LM[t] = ì •í™•ë„_í•©ê³„ / ê°€ì¤‘ì¹˜_í•©ê³„
        
    return tÌ‚_LM  # [num_topics] - ê° í† í”½ë³„ ëª¨ë¸ ì§€ì‹ ìˆ˜ì¤€
```

**3. Cumulative Coverage ë©”ì»¤ë‹ˆì¦˜**
```python
def update_coverage(new_demo, previous_demos):
    """ìƒˆ ì‹œì—°ì´ ê¸°ì¡´ ì„ íƒê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” í† í”½ ê¸°ì—¬ë„ë§Œ ê³„ì‚°"""
    
    # ê¸°ì¡´ ì‹œì—°ë“¤ì˜ í‰ê·  ì„ë² ë”©
    prev_avg = mean([embed(d) for d in previous_demos])
    
    # ìƒˆ ì‹œì—° í¬í•¨í•œ ì „ì²´ í‰ê·   
    new_avg = mean([embed(d) for d in previous_demos + [new_demo]])
    
    # ì¦ë¶„ ê¸°ì—¬ë„ = ìƒˆë¡œìš´ ì „ì²´ ì»¤ë²„ë¦¬ì§€ - ê¸°ì¡´ ì»¤ë²„ë¦¬ì§€
    incremental = max(0, f(new_avg) - f(prev_avg))
    
    return incremental  # ì‹¤ì œ ìƒˆë¡œìš´ ê¸°ì—¬ë¶„ë§Œ ë°˜í™˜
```

#### ğŸ¯ ì´í•´ë„ ì²´í¬
```python
ì§ˆë¬¸_level_2 = [
    "í† í”½ ì˜ˆì¸¡ê¸°ì˜ ì…ë ¥ê³¼ ì¶œë ¥ ì°¨ì›ì€?",
    "tÌ‚_LM ë²¡í„°ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ê°€?",
    "ëˆ„ì  ì»¤ë²„ë¦¬ì§€ëŠ” ì–´ë–»ê²Œ ì¤‘ë³µì„ ë°©ì§€í•˜ëŠ”ê°€?",
    "ì „ì²´ ì‹œìŠ¤í…œì—ì„œ LLM ì¶”ë¡ ì´ ì–¸ì œ í•„ìš”í•œê°€?"
]

ë‹µë³€_ê°€ì´ë“œ = {
    "ì˜ˆì¸¡ê¸°_ì°¨ì›": "ì…ë ¥ [768] â†’ ì¶œë ¥ [num_topics], ê° í† í”½ í™•ë¥ ",
    "tÌ‚_LM_ì˜ë¯¸": "ëª¨ë¸ì´ ê° í† í”½ì— ëŒ€í•´ ê°€ì§„ prior knowledge ìˆ˜ì¤€",
    "ì¤‘ë³µ_ë°©ì§€": "í‰ê·  ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ ì»¤ë²„ëœ í† í”½ ì œì™¸",
    "LLM_ì¶”ë¡ ": "ì‚¬ì „ ë‹¨ê³„ì—ì„œ topical knowledge ê³„ì‚° ì‹œì—ë§Œ"
}
```

---

### Level 3: ê¹Šì€ ì´í•´ (60ë¶„) - ì„¤ê³„ ì² í•™ê³¼ ìˆ˜í•™ì  ê·¼ê±°

#### âœ… í•™ìŠµ ëª©í‘œ
- [ ] "ì™œ ì´ë ‡ê²Œ ì„¤ê³„í–ˆëŠ”ê°€?" ì§ˆë¬¸ì— ëŒ€í•œ ìˆ˜í•™ì  ê·¼ê±° ì´í•´
- [ ] ê° ì„¤ê³„ ê²°ì •ì˜ trade-off ë¶„ì„ ê°€ëŠ¥
- [ ] Ablation study ê²°ê³¼ í•´ì„ ë° ì¤‘ìš”ë„ íŒŒì•…
- [ ] í•œê³„ì ê³¼ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ ê°€ëŠ¥

#### ğŸ¤” ì„¤ê³„ ì² í•™ ê¹Šì´ íŒŒê¸°

**Q: ì™œ í† í”½ ëª¨ë¸ë§ì„ ì‚¬ìš©í–ˆëŠ”ê°€?**

```python
# ì´ë¡ ì  ì •ë‹¹í™” (ë…¼ë¬¸ Section 3.2.3)
"""
ëª©í‘œ: H(x|d) ìµœì†Œí™” (í…ŒìŠ¤íŠ¸ ì…ë ¥ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ê°ì†Œ)
  â‰¡ p(x|d) ìµœëŒ€í™”

ê¸°ì¡´ ConE: p(x|d)ë¥¼ ì§ì ‘ LLMìœ¼ë¡œ ê³„ì‚° (ë¹„ì‹¸ê³  ëŠë¦¼)
TopicK: í† í”½ ëª¨ë¸ë§ìœ¼ë¡œ ë¶„í•´

p(x|d) = Î£â‚œ p(x|t) Ã— p(t|d)  
       = Î£â‚œ [p(t|x) Ã— p(x) / p(t)] Ã— p(t|d)
       = p(x) Ã— Î£â‚œ [p(t|x) Ã— p(t|d) / p(t)]
              â†‘        â†‘        â†‘
         required  covered  topical
          topics   topics  knowledge
"""

ì¥ì _ë¶„ì„ = {
    "íš¨ìœ¨ì„±": "LLM ì¶”ë¡  ì—†ì´ lightweight predictorë¡œ ëŒ€ì²´",
    "í•´ì„ì„±": "í† í”½ ë ˆë²¨ì—ì„œ ì™œ ê·¸ ì‹œì—°ì´ ì„ íƒë˜ì—ˆëŠ”ì§€ ëª…í™•",
    "í™•ì¥ì„±": "ìƒˆë¡œìš´ ë„ë©”ì¸ì— topic miningë§Œìœ¼ë¡œ ì ìš© ê°€ëŠ¥"
}
```

**Q: ì™œ 3ê°œ ì»´í¬ë„ŒíŠ¸(required, covered, knowledge) ëª¨ë‘ í•„ìš”í•œê°€?**

```python
# ê° ì»´í¬ë„ŒíŠ¸ ì œê±° ì‹œ ë¬¸ì œì  ë¶„ì„

def ablation_analysis():
    return {
        "w/o_required_topics": {
            "ë¬¸ì œ": "í…ŒìŠ¤íŠ¸ ì…ë ¥ê³¼ ê´€ë ¨ ì—†ëŠ” ì‹œì—° ì„ íƒ ê°€ëŠ¥",
            "ì˜ˆì‹œ": "ìˆ˜í•™ ë¬¸ì œì¸ë° ì—­ì‚¬ ê´€ë ¨ ì‹œì—° ì„ íƒ",
            "ì„±ëŠ¥ì €í•˜": "ê´€ë ¨ì„± ë¶€ì¡±ìœ¼ë¡œ ICL íš¨ê³¼ ê°ì†Œ"
        },
        
        "w/o_covered_topics": {
            "ë¬¸ì œ": "ì‹œì—°ì´ ì‹¤ì œë¡œ ì–´ë–¤ ì§€ì‹ì„ ì œê³µí•˜ëŠ”ì§€ ëª¨ë¦„", 
            "ì˜ˆì‹œ": "í† í”½ ë ˆì´ë¸”ë§Œ ê°™ê³  ë‚´ìš©ì´ ë¹ˆì•½í•œ ì‹œì—° ì„ íƒ",
            "ì„±ëŠ¥ì €í•˜": "ì •ë³´ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ í•™ìŠµ íš¨ê³¼ ì €í•˜"
        },
        
        "w/o_topical_knowledge": {
            "ë¬¸ì œ": "ëª¨ë¸ì´ ì´ë¯¸ ì˜ ì•„ëŠ” ì˜ì—­ì— ì¤‘ë³µ íˆ¬ì",
            "ì˜ˆì‹œ": "ëª¨ë¸ì´ ì™„ë²½í•œ ê¸°ì´ˆ ì‚°ìˆ ì¸ë° ë˜ ê¸°ì´ˆ ì‹œì—° ì„ íƒ", 
            "ì„±ëŠ¥ì €í•˜": "ì•½í•œ ë¶€ë¶„ ë³´ê°• ê¸°íšŒ ìƒì‹¤"
        }
    }

# ë…¼ë¬¸ì˜ ì‹¤ì œ ablation ê²°ê³¼ì™€ ë¹„êµ
ì‹¤í—˜_ê²°ê³¼ = {
    "TopicK": {"Common": 46.19, "QNLI": 62.51, "MedMCQA": 41.80},
    "w/o Core Topic": {"Common": 44.72, "QNLI": 62.03, "MedMCQA": 41.17}, # -1.47
    "w/o Soft Label": {"Common": 45.21, "QNLI": 62.38, "MedMCQA": 41.56}, # -0.98  
    "w/o Cumulative Coverage": {"Common": 44.41, "QNLI": 61.47, "MedMCQA": 40.12} # -1.78
}

# ê²°ë¡ : Cumulative Coverageê°€ ê°€ì¥ ì¤‘ìš”í•œ ì»´í¬ë„ŒíŠ¸
```

**Q: ì™œ distinctiveness-aware soft labelì„ ì‚¬ìš©í–ˆëŠ”ê°€?**

```python
def compare_training_signals():
    """Binary label vs Soft label ë¹„êµ"""
    
    example_demo = "Herbivores are animals that eat plants"
    nearby_demos = [
        "Animals eat food for survival",      # ì¼ë°˜ì 
        "Plants provide energy to organisms", # ì¼ë°˜ì   
        "Specialized digestive systems in ruminants" # êµ¬ì²´ì 
    ]
    
    # Binary label (naive)
    binary_target = {
        "herbivore": 1.0,  # ë‹¨ìˆœíˆ ìˆìŒ/ì—†ìŒë§Œ êµ¬ë¶„
        "animal": 1.0,
        "plant": 1.0
    }
    
    # Distinctiveness-aware soft label  
    soft_target = {
        "herbivore": 0.9,  # ë‹¤ë¥¸ ì‹œì—°ì—ì„œ ë“œë¬¼ê²Œ ë“±ì¥ â†’ ë†’ì€ ê°€ì¤‘ì¹˜
        "animal": 0.3,     # ë§ì€ ì‹œì—°ì—ì„œ ë“±ì¥ â†’ ë‚®ì€ ê°€ì¤‘ì¹˜
        "plant": 0.6       # ì¤‘ê°„ ì •ë„ ë¹ˆë„ â†’ ì¤‘ê°„ ê°€ì¤‘ì¹˜
    }
    
    return "êµ¬ë³„ì ì¸ í† í”½ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ â†’ ë” ì •í™•í•œ í† í”½ ì˜ˆì¸¡"
```

#### âš–ï¸ Trade-off ë¶„ì„

**1. ì •í™•ì„± vs íš¨ìœ¨ì„±**
```python
trade_offs = {
    "ConE (ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜)": {
        "ì •í™•ì„±": "ë†’ìŒ - ì‹¤ì œ LLM ê¸°ë°˜ í‰ê°€", 
        "íš¨ìœ¨ì„±": "ë‚®ìŒ - 37x ëŠë¦¼",
        "í™•ì¥ì„±": "ë‚®ìŒ - closed-source LLM ì ìš© ë¶ˆê°€"
    },
    
    "TopicK": {
        "ì •í™•ì„±": "ë†’ìŒ - ConEì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ìŒ",
        "íš¨ìœ¨ì„±": "ë†’ìŒ - lightweight predictor", 
        "í™•ì¥ì„±": "ë†’ìŒ - ëª¨ë“  LLMì— ì ìš© ê°€ëŠ¥"
    }
}

# ê²°ë¡ : TopicKëŠ” ê±°ì˜ ëª¨ë“  ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•œ trade-off ë‹¬ì„±
```

**2. ì¼ë°˜ì„± vs ë„ë©”ì¸ íŠ¹í™”**
```python
domain_analysis = {
    "ì¼ë°˜_ë„ë©”ì¸": {
        "CommonsenseQA": "similarity-based ë°©ë²•ë„ ì˜ ì‘ë™",
        "SciQ": "TopicKì˜ ìƒëŒ€ì  ìš°ìœ„ ì‘ìŒ",
        "ì´ìœ ": "í‘œë©´ì  ìœ ì‚¬ë„ë¡œë„ ì¶©ë¶„í•œ ê²½ìš° ë§ìŒ"
    },
    
    "ì „ë¬¸_ë„ë©”ì¸": {
        "MedMCQA": "TopicKì˜ ëŒ€í­ ê°œì„  (ìµœëŒ€ 6.38%)",  
        "Law": "TopicKì˜ ì§€ì†ì  ìš°ìœ„",
        "ì´ìœ ": "ì„¸ë¶„í™”ëœ ì „ë¬¸ ì§€ì‹ì´ ì¤‘ìš” â†’ í† í”½ ì»¤ë²„ë¦¬ì§€ íš¨ê³¼ ê·¹ëŒ€í™”"
    }
}
```

#### ğŸš¨ í•œê³„ì ê³¼ ì‹¤íŒ¨ ì¼€ì´ìŠ¤

**1. ì‹œìŠ¤í…œì  í•œê³„**
```python
limitations = {
    "í† í”½_ì •ì˜_ì˜ì¡´ì„±": {
        "ë¬¸ì œ": "topic mining í’ˆì§ˆì— ë”°ë¼ ì „ì²´ ì„±ëŠ¥ ì¢Œìš°",
        "ì™„í™”": "ë„ë©”ì¸ë³„ ì „ë¬¸ í† í”½ ì„¸íŠ¸ ì‚¬ìš©",
        "í–¥í›„": "hierarchical topic êµ¬ì¡° ë„ì…"
    },
    
    "ëª¨ë¸_í¬ê¸°_ì œì•½": {
        "ë¬¸ì œ": "0.5B~8B ëª¨ë¸ì—ì„œë§Œ í‰ê°€",
        "ì™„í™”": "closed-source ëŒ€í˜• ëª¨ë¸ë¡œ ê²€ì¦",
        "í–¥í›„": "100B+ ëª¨ë¸ì—ì„œì˜ íš¨ê³¼ ê²€ì¦ í•„ìš”"
    },
    
    "flat_topic_êµ¬ì¡°": {
        "ë¬¸ì œ": "ê³„ì¸µì  í† í”½ ê´€ê³„ ë¬´ì‹œ",
        "ì™„í™”": "ê´€ë ¨ í† í”½ ê°„ ìœ ì‚¬ë„ ê³ ë ¤",
        "í–¥í›„": "topical taxonomy í™œìš©"
    }
}
```

**2. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„**
```python
failure_cases = {
    "í† í”½_ë¶ˆê· í˜•": {
        "ìƒí™©": "ì¼ë¶€ í† í”½ì´ ê³¼ë„í•˜ê²Œ dominant",
        "ê²°ê³¼": "ì†Œìˆ˜ í† í”½ë§Œ ë°˜ë³µ ì„ íƒ",
        "í•´ê²°": "í† í”½ë³„ selection quota ë„ì…"
    },
    
    "ëƒ‰ì‹œë™_ë¬¸ì œ": {
        "ìƒí™©": "topical knowledge ì¶”ì •ì´ ë¶€ì •í™•í•œ ì´ˆê¸° ë‹¨ê³„", 
        "ê²°ê³¼": "ì˜ëª»ëœ ìš°ì„ ìˆœìœ„ë¡œ ì‹œì—° ì„ íƒ",
        "í•´ê²°": "ë” ë§ì€ demonstrationìœ¼ë¡œ ì‚¬ì „ calibration"
    },
    
    "ê·¹ë‹¨ì _ë„ë©”ì¸": {
        "ìƒí™©": "ë§¤ìš° ìƒˆë¡œìš´ ë„ë©”ì¸ (í† í”½ mining ì‹¤íŒ¨)",
        "ê²°ê³¼": "ì˜ë¯¸ìˆëŠ” í† í”½ ì¶”ì¶œ ë¶ˆê°€",
        "í•´ê²°": "manual topic seed ì œê³µ"
    }
}
```

#### ğŸ¯ ì´í•´ë„ ì²´í¬
```python
ì§ˆë¬¸_level_3 = [
    "TopicKì˜ ì´ë¡ ì  ì •ë‹¹í™”ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ë¼",
    "ê° ablationì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ ê°€ì¥ í° ì»´í¬ë„ŒíŠ¸ëŠ”?",
    "ì¼ë°˜ ë„ë©”ì¸ vs ì „ë¬¸ ë„ë©”ì¸ì—ì„œì˜ ì„±ëŠ¥ ì°¨ì´ ì´ìœ ëŠ”?",
    "TopicKê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ 3ê°€ì§€ëŠ”?"
]

ê¹Šì€_ì´í•´_ì§€í‘œ = {
    "ìˆ˜ì‹_ì´í•´": "p(x|d) ë¶„í•´ë¥¼ í† í”½ ëª¨ë¸ë§ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥",
    "ablation_í•´ì„": "Cumulative Coverage > Core Topic > Soft Label ìˆœì„œ",
    "ë„ë©”ì¸_ì°¨ì´": "ì „ë¬¸ ì˜ì—­ì¼ìˆ˜ë¡ ì„¸ë¶„í™”ëœ í† í”½ ì»¤ë²„ë¦¬ì§€ ì¤‘ìš”ì„± ì¦ê°€", 
    "ì‹¤íŒ¨_ì¼€ì´ìŠ¤": "í† í”½ ë¶ˆê· í˜•, ëƒ‰ì‹œë™ ë¬¸ì œ, ê·¹ë‹¨ì  ë„ë©”ì¸ ë“±"
}
```

---

### Level 4: ì‹¤ì „ ì ìš© (90ë¶„) - êµ¬í˜„ ë° í™•ì¥ ì•„ì´ë””ì–´

#### âœ… í•™ìŠµ ëª©í‘œ
- [ ] ì‹¤ì œ ë™ì‘í•˜ëŠ” TopicK ë¯¸ë‹ˆ êµ¬í˜„ ì™„ì„±
- [ ] ë‹¤ë¥¸ ë„ë©”ì¸ì— ì ìš©í•˜ëŠ” êµ¬ì²´ì  ë°©ë²• ì œì•ˆ
- [ ] ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ í™•ì¥ ì•„ì´ë””ì–´ 3ê°€ì§€ ë„ì¶œ
- [ ] ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­ê³¼ ìµœì í™” íŒ ì •ë¦¬

#### ğŸ› ï¸ ì‹¤ì „ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

**Step 1: ë¯¸ë‹ˆ TopicK êµ¬í˜„**
```python
# í•„ìˆ˜ êµ¬í˜„ í•­ëª© ì²´í¬ë¦¬ìŠ¤íŠ¸
êµ¬í˜„_ì²´í¬ë¦¬ìŠ¤íŠ¸ = {
    "âœ… í† í”½ ì˜ˆì¸¡ê¸°": "3-layer MLP + sigmoid activation",
    "âœ… í† í”½ë³„ ì§€ì‹ ì¶”ì •": "zero-shot accuracy ê¸°ë°˜ weighted average", 
    "âœ… ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°": "âŸ¨tÌ‚_x âŠ˜ tÌ‚_LM, tÌ‚_dâŸ© ìˆ˜ì‹ êµ¬í˜„",
    "âœ… ëˆ„ì  ì»¤ë²„ë¦¬ì§€": "mean pooling + incremental update",
    "âœ… ë°˜ë³µì  ì„ íƒ": "k-round greedy selection with diversity",
    "âœ… ì„±ëŠ¥ í‰ê°€": "topic coverage, diversity, entropy ë©”íŠ¸ë¦­"
}

# ê²€ì¦ ë°©ë²•
def validate_implementation():
    """êµ¬í˜„ì´ ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
    
    test_cases = [
        "herbivore ì§ˆë¬¸ì— herbivore ì‹œì—° ìš°ì„  ì„ íƒ",
        "ì´ë¯¸ ì„ íƒëœ í† í”½ê³¼ ì¤‘ë³µ ì‹œ í˜ë„í‹° ì ìš©",
        "ëª¨ë¸ì´ ì•½í•œ í† í”½ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬",
        "ì˜ë¯¸ì  ìœ ì‚¬ë„ì™€ í† í”½ ê´€ë ¨ì„± ì ì ˆíˆ ì¡°í•©"
    ]
    
    for case in test_cases:
        result = run_test_case(case) 
        assert result.passes, f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {case}"
    
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ - êµ¬í˜„ ê²€ì¦ ì™„ë£Œ!")
```

**Step 2: ì„±ëŠ¥ ìµœì í™”**
```python
optimization_tips = {
    "ë©”ëª¨ë¦¬_ìµœì í™”": {
        "ë°°ì¹˜_ì²˜ë¦¬": "ëŒ€ìš©ëŸ‰ í›„ë³´ í’€ì„ 32ê°œì”© ë‚˜ëˆ„ì–´ ì²˜ë¦¬",
        "ì‚¬ì „_í•„í„°ë§": "ìƒìœ„ 300ê°œë¡œ í›„ë³´ ì¶•ì†Œ (ë…¼ë¬¸ ì–¸ê¸‰)",
        "ì„ë² ë”©_ìºì‹±": "ë™ì¼ í…ìŠ¤íŠ¸ ì¬ê³„ì‚° ë°©ì§€"
    },
    
    "ì†ë„_ìµœì í™”": {
        "ë³‘ë ¬_ì²˜ë¦¬": "í† í”½ ì˜ˆì¸¡ì„ GPU batchë¡œ ê°€ì†",
        "ì¡°ê¸°_ì¢…ë£Œ": "ê´€ë ¨ì„± ì ìˆ˜ ì„ê³„ê°’ ë¯¸ë‹¬ ì‹œ ì¡°ê¸° ì¤‘ë‹¨",
        "ê·¼ì‚¬_ê³„ì‚°": "ì „ì²´ í† í”½ ëŒ€ì‹  top-k í† í”½ë§Œ ê³„ì‚°"
    },
    
    "í’ˆì§ˆ_ìµœì í™”": {
        "í•˜ì´í¼íŒŒë¼ë¯¸í„°_íŠœë‹": "Î», coverage threshold ë“± ê·¸ë¦¬ë“œ ì„œì¹˜",
        "ì•™ìƒë¸”": "ì—¬ëŸ¬ í† í”½ ì˜ˆì¸¡ê¸°ì˜ weighted ensemble",
        "ì ì‘ì _ì„ íƒ": "ë„ë©”ì¸ë³„ ìµœì  kê°’ ìë™ ê²°ì •"
    }
}
```

#### ğŸŒ ë„ë©”ì¸ë³„ ì ìš© ì „ëµ

**1. ì»´í“¨í„° ë¹„ì „ + VLM**
```python
cv_adaptation = {
    "í† í”½_ì •ì˜": "visual concepts (objects, scenes, attributes)",
    "í† í”½_ì¶”ì¶œ": "CLIP ê¸°ë°˜ visual-semantic embedding",
    "ì‹œì—°_í˜•íƒœ": "image-text pairs for visual reasoning",
    "ì ìš©_ì˜ˆì‹œ": "VQAì—ì„œ ì‹œê°ì  ê°œë… ì»¤ë²„ë¦¬ì§€ ìµœì í™”"
}

def adapt_to_vision():
    """Vision-Language Modelì— TopicK ì ìš©"""
    
    # ì´ë¯¸ì§€ì˜ visual topics ì¶”ì¶œ
    visual_topics = extract_visual_concepts(image, clip_model)
    
    # í…ìŠ¤íŠ¸ì˜ semantic topicsì™€ ê²°í•©  
    multimodal_topics = combine_topics(visual_topics, text_topics)
    
    # ì‹œê°-ì–¸ì–´ ì‹œì—° ì„ íƒ
    selected_demos = topicK_select(
        test_input=(image, question),
        candidates=visual_qa_pairs, 
        topic_space=multimodal_topics
    )
    
    return selected_demos
```

**2. ì½”ë“œ ìƒì„± ëª¨ë¸**
```python
code_adaptation = {
    "í† í”½_ì •ì˜": "programming concepts (algorithms, data structures, APIs)",
    "í† í”½_ì¶”ì¶œ": "AST parsing + semantic analysis", 
    "ì‹œì—°_í˜•íƒœ": "problem-solution code pairs",
    "ì ìš©_ì˜ˆì‹œ": "coding interviewì—ì„œ ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì»¤ë²„ë¦¬ì§€"
}

def adapt_to_coding():
    """Code generationì— TopicK ì ìš©"""
    
    # ë¬¸ì œì—ì„œ ìš”êµ¬ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ í† í”½ ì¶”ì¶œ
    required_topics = analyze_coding_problem(problem_description)
    # ["dynamic_programming", "tree_traversal", "hash_table"]
    
    # ì½”ë“œ ì˜ˆì œë“¤ì˜ êµ¬í˜„ í† í”½ ë¶„ì„
    code_topics = analyze_code_examples(example_solutions)
    
    # ì•Œê³ ë¦¬ì¦˜ íŒ¨í„´ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ì„ íƒ
    selected_examples = topicK_select(
        test_input=coding_problem,
        candidates=coding_examples,
        topic_space=algorithmic_concepts
    )
```

**3. ê³¼í•™ì  ë¬¸ì„œ ë¶„ì„**
```python
scientific_adaptation = {
    "í† í”½_ì •ì˜": "scientific concepts (methods, theories, domains)",
    "í† í”½_ì¶”ì¶œ": "scientific entity extraction + knowledge graph",
    "ì‹œì—°_í˜•íƒœ": "paper abstracts + key findings",
    "ì ìš©_ì˜ˆì‹œ": "literature reviewì—ì„œ ë°©ë²•ë¡  ì»¤ë²„ë¦¬ì§€"
}

def adapt_to_science():
    """Scientific document analysisì— TopicK ì ìš©"""
    
    # ì—°êµ¬ ì§ˆë¬¸ì—ì„œ í•„ìš”í•œ ë°©ë²•ë¡ /ê°œë… ì¶”ì¶œ
    required_methods = extract_scientific_concepts(research_question)
    
    # ë…¼ë¬¸ë“¤ì˜ ê¸°ì—¬ ë°©ë²•ë¡  ë¶„ì„
    paper_contributions = analyze_papers(candidate_papers)
    
    # ê³¼í•™ì  ë°©ë²•ë¡  ë‹¤ì–‘ì„± ê¸°ë°˜ ë…¼ë¬¸ ì„ íƒ
    selected_papers = topicK_select(
        test_input=research_question,
        candidates=academic_papers,
        topic_space=scientific_concepts
    )
```

#### ğŸ’¡ ì°½ì˜ì  í™•ì¥ ì•„ì´ë””ì–´

**1. Hierarchical TopicK**
```python
hierarchical_extension = {
    "ì•„ì´ë””ì–´": "ê³„ì¸µì  í† í”½ êµ¬ì¡°ë¡œ ë‹¤ë‹¨ê³„ ì„ íƒ",
    "êµ¬í˜„": """
    Level 1: ê±°ì‹œì  ë„ë©”ì¸ (biology, physics, chemistry)
    Level 2: ì¤‘ê°„ ì˜ì—­ (genetics, ecology, molecular biology) 
    Level 3: ì„¸ë¶€ í† í”½ (DNA replication, food webs, enzyme kinetics)
    
    ì„ íƒ ì „ëµ: ìƒìœ„ ë ˆë²¨ë¶€í„° ì»¤ë²„ë¦¬ì§€ ë³´ì¥ í›„ í•˜ìœ„ë¡œ ì„¸ë¶„í™”
    """,
    "ì¥ì ": "í† í”½ ê°„ ê´€ê³„ í™œìš©, ì²´ê³„ì  ì§€ì‹ êµ¬ì„±"
}
```

**2. Dynamic TopicK**
```python
dynamic_extension = {
    "ì•„ì´ë””ì–´": "ëŒ€í™”/ì„¸ì…˜ ì§„í–‰ì— ë”°ë¼ í† í”½ ì¤‘ìš”ë„ ë™ì  ì¡°ì •",
    "êµ¬í˜„": """
    ì´ˆê¸°: ê· ë“±í•œ í† í”½ ê°€ì¤‘ì¹˜
    ì¤‘ê°„: ì‚¬ìš©ì í”¼ë“œë°±/ì„±ëŠ¥ì— ë”°ë¼ í† í”½ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸  
    ìµœì¢…: ê°œì¸í™”ëœ í† í”½ ì„ í˜¸ë„ ë°˜ì˜
    
    ì—…ë°ì´íŠ¸: tÌ‚_LM â† tÌ‚_LM + Î± Ã— (ì‹¤ì œì„±ëŠ¥ - ì˜ˆìƒì„±ëŠ¥) Ã— tÌ‚_d
    """,
    "ì¥ì ": "ê°œì¸í™”, ì ì‘ì  í•™ìŠµ, ì¥ê¸° ê¸°ì–µ"
}
```

**3. Multi-Modal TopicK**
```python
multimodal_extension = {
    "ì•„ì´ë””ì–´": "í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì¤‘ ëª¨ë‹¬ í† í”½ í†µí•©",
    "êµ¬í˜„": """
    í…ìŠ¤íŠ¸_í† í”½ = topic_predictor_text(text_embedding)
    ì´ë¯¸ì§€_í† í”½ = topic_predictor_vision(image_embedding)  
    ì˜¤ë””ì˜¤_í† í”½ = topic_predictor_audio(audio_embedding)
    
    í†µí•©_í† í”½ = weighted_fusion(í…ìŠ¤íŠ¸_í† í”½, ì´ë¯¸ì§€_í† í”½, ì˜¤ë””ì˜¤_í† í”½)
    """,
    "ì¥ì ": "í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸, ë‹¤ì¤‘ ê°ê° í•™ìŠµ"
}
```

#### âš ï¸ ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­

**1. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬**
```python
data_quality_checklist = {
    "í† í”½_ë§ˆì´ë‹": [
        "ë„ë©”ì¸ë³„ ì „ë¬¸ ìš©ì–´ì‚¬ì „ ì¤€ë¹„",
        "ë¶ˆìš©ì–´/ë…¸ì´ì¦ˆ í† í”½ í•„í„°ë§",
        "í† í”½ ë¼ë²¨ì˜ ì¼ê´€ì„± ê²€ì¦"
    ],
    "ì‹œì—°_í’ˆì§ˆ": [
        "ì¤‘ë³µ/ìœ ì‚¬í•œ ì‹œì—° ì œê±°",
        "ì‹œì—° ê¸¸ì´ í‘œì¤€í™”",
        "ì •ë‹µ ë ˆì´ë¸” ì •í™•ì„± ê²€ì¦"
    ],
    "í‰ê°€_ê¸°ì¤€": [
        "ë„ë©”ì¸ ì „ë¬¸ê°€ ê²€í† ",
        "A/B í…ŒìŠ¤íŠ¸ë¡œ ì„±ëŠ¥ ë¹„êµ",
        "ì‚¬ìš©ì ë§Œì¡±ë„ ì¡°ì‚¬"
    ]
}
```

**2. í™•ì¥ì„± ê³ ë ¤ì‚¬í•­**
```python
scalability_considerations = {
    "í† í”½_ìˆ˜_ì¦ê°€": {
        "ë¬¸ì œ": "num_topics ì¦ê°€ ì‹œ ë©”ëª¨ë¦¬/ê³„ì‚° ë¹„ìš© ê¸‰ì¦",
        "í•´ê²°": "sparse representation, topic clustering"
    },
    
    "í›„ë³´_í’€_í™•ëŒ€": {
        "ë¬¸ì œ": "millions of candidates ì²˜ë¦¬ ì‹œ ì†ë„ ì €í•˜",
        "í•´ê²°": "hierarchical indexing, approximate search"
    },
    
    "ì‹¤ì‹œê°„_ìš”êµ¬ì‚¬í•­": {
        "ë¬¸ì œ": "< 100ms ì‘ë‹µ ìš”êµ¬ ì‹œ í’ˆì§ˆ vs ì†ë„ trade-off",
        "í•´ê²°": "pre-computed embeddings, model distillation"
    }
}
```

**3. ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…**
```python
monitoring_framework = {
    "ì„±ëŠ¥_ë©”íŠ¸ë¦­": [
        "ì„ íƒëœ ì‹œì—°ë“¤ì˜ í‰ê·  ê´€ë ¨ì„± ì ìˆ˜",
        "í† í”½ ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨ (í™œì„± í† í”½ / ì „ì²´ í† í”½)",
        "ì‹œì—° ê°„ ë‹¤ì–‘ì„± ì§€ìˆ˜ (1 - avg_similarity)"
    ],
    
    "í’ˆì§ˆ_ì§€í‘œ": [
        "ìµœì¢… ICL ì •í™•ë„ í–¥ìƒ í­",
        "ì‚¬ìš©ìê°€ ì„ íƒí•œ ì‹œì—° vs TopicK ì„ íƒ ì¼ì¹˜ë„", 
        "ë„ë©”ì¸ ì „ë¬¸ê°€ í‰ê°€ ì ìˆ˜"
    ],
    
    "ë””ë²„ê¹…_ë„êµ¬": [
        "í† í”½ë³„ ê¸°ì—¬ë„ ì‹œê°í™”",
        "ì„ íƒ ê³¼ì • step-by-step ì¶”ì ",
        "ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìë™ ë¶„ë¥˜"
    ]
}
```

#### ğŸ¯ ìµœì¢… ì´í•´ë„ ì²´í¬

```python
ì§ˆë¬¸_level_4 = [
    "TopicKë¥¼ ì»´í“¨í„° ë¹„ì „ì— ì ìš©í•˜ëŠ” êµ¬ì²´ì  ë°©ë²•ì€?",
    "ê³„ì¸µì  í† í”½ êµ¬ì¡° ë„ì… ì‹œ ì˜ˆìƒ íš¨ê³¼ì™€ êµ¬í˜„ ë°©ë²•ì€?", 
    "ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œ TopicK ì ìš© ì‹œ ì£¼ìš” ë³‘ëª©ì ì€?",
    "í† í”½ ë§ˆì´ë‹ í’ˆì§ˆì´ ì „ì²´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
]

mastery_indicators = {
    "êµ¬í˜„_ëŠ¥ë ¥": "ì‘ë™í•˜ëŠ” ë¯¸ë‹ˆ TopicK ì™„ì„± + ì„±ëŠ¥ ê²€ì¦",
    "í™•ì¥_ì‚¬ê³ ": "ìƒˆë¡œìš´ ë„ë©”ì¸ ì ìš© ë°©ë²• êµ¬ì²´ì  ì œì•ˆ ê°€ëŠ¥",
    "ì‹¤ë¬´_ê°ê°": "í’ˆì§ˆ/ì†ë„/í™•ì¥ì„± trade-off ì´í•´ ë° í•´ê²°ì±… ì œì‹œ",
    "ì°½ì˜_ì ìš©": "ë…¼ë¬¸ í•œê³„ ê·¹ë³µí•˜ëŠ” ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ë„ì¶œ"
}

# ìµœì¢… í‰ê°€: ëª¨ë“  ì§€í‘œì—ì„œ 80% ì´ìƒ ë‹¬ì„± ì‹œ Level 4 ì™„ë£Œ
```

## ğŸ§  ë©”íƒ€ í•™ìŠµ: ë…¼ë¬¸ ë¶„ì„ ìŠ¤í‚¬ í–¥ìƒ

### ğŸ“Š í•™ìŠµ íš¨ê³¼ ì¸¡ì •
```python
learning_metrics = {
    "ì´í•´_ê¹Šì´": "ìˆ˜ì‹ì˜ ë¬¼ë¦¬ì  ì˜ë¯¸ê¹Œì§€ ì„¤ëª… ê°€ëŠ¥í•œê°€?",
    "ì ìš©_ëŠ¥ë ¥": "ë‹¤ë¥¸ ë¬¸ì œì— ì•„ì´ë””ì–´ ë³€í˜• ì ìš© ê°€ëŠ¥í•œê°€?", 
    "ë¹„íŒ_ì‚¬ê³ ": "ë…¼ë¬¸ì˜ í•œê³„ì™€ ê°œì„ ì  ì œì‹œ ê°€ëŠ¥í•œê°€?",
    "êµ¬í˜„_ì‹¤ë ¥": "í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì„ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„ ê°€ëŠ¥í•œê°€?"
}

# ê° ë ˆë²¨ë³„ ì˜ˆìƒ í•™ìŠµ ì‹œê°„
time_allocation = {
    "Level 1 (30ë¶„)": "ê¸°ë³¸ ì•„ì´ë””ì–´ íŒŒì•… - ì „ì²´ì˜ 30%",
    "Level 2 (45ë¶„)": "ì‹œìŠ¤í…œ êµ¬ì¡° ì´í•´ - ì „ì²´ì˜ 35%", 
    "Level 3 (60ë¶„)": "ìˆ˜í•™ì  ê·¼ê±° ë¶„ì„ - ì „ì²´ì˜ 20%",
    "Level 4 (90ë¶„)": "ì‹¤ì „ êµ¬í˜„ ë° í™•ì¥ - ì „ì²´ì˜ 15%"
}
```

### ğŸ”„ ë°˜ë³µ í•™ìŠµ ê°€ì´ë“œ
```python
revision_strategy = {
    "1ì£¼_í›„": "í•µì‹¬ ì•„ì´ë””ì–´ 3ê°€ì§€ë¥¼ 5ë¶„ ë‚´ ì„¤ëª… ê°€ëŠ¥í•œì§€ ì²´í¬",
    "1ê°œì›”_í›„": "ë‹¤ë¥¸ ë…¼ë¬¸ê³¼ ë¹„êµí•˜ì—¬ TopicKì˜ ë…ì°½ì„± í‰ê°€",
    "3ê°œì›”_í›„": "ì‹¤ì œ í”„ë¡œì íŠ¸ì— TopicK ì•„ì´ë””ì–´ ì ìš© ì‹œë„"
}
```

ì´ ë‹¨ê³„ë³„ í•™ìŠµ ê²½ë¡œë¥¼ í†µí•´ TopicK ë…¼ë¬¸ì„ ì™„ì „íˆ ì´í•´í•˜ê³  ì‹¤ì „ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ê¹Œì§€ ë„ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!