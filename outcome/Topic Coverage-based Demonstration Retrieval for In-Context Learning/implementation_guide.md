# Topic Coverage-based Demonstration Retrieval for In-Context Learning - êµ¬í˜„ ê°€ì´ë“œ

## ğŸ” ë‹¨ê³„ë³„ ë¯¸ë‹ˆ êµ¬í˜„

### Step 1: ê¸°ë³¸ í† í”½ ì˜ˆì¸¡ê¸° êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple

class TopicPredictor(nn.Module):
    """ê²½ëŸ‰ í† í”½ ì˜ˆì¸¡ê¸° - ì‹œì—° ì„ë² ë”©ì„ í† í”½ ë¶„í¬ë¡œ ë³€í™˜"""
    
    def __init__(self, input_dim: int = 768, hidden_dim: int = 512, num_topics: int = 1000):
        super().__init__()
        self.num_topics = num_topics
        
        # 3-layer MLP as mentioned in paper
        self.layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_topics),
            nn.Sigmoid()  # Output probabilities for each topic
        )
        
    def forward(self, embeddings):
        # embeddings: [batch_size, input_dim] = [32, 768]
        topic_probs = self.layers(embeddings)  # [batch_size, num_topics]
        return topic_probs

# ì˜ˆì‹œ ì‚¬ìš©ë²•
embedding_model = SentenceTransformer('all-mpnet-base-v2')
topic_predictor = TopicPredictor(input_dim=768, num_topics=1000)

# ìƒ˜í”Œ ì‹œì—°ë“¤
demonstrations = [
    "Herbivores are animals that primarily eat plants",
    "Carnivores hunt and eat other animals for survival", 
    "Photosynthesis converts sunlight into chemical energy"
]

# ì„ë² ë”© ìƒì„±
embeddings = embedding_model.encode(demonstrations, convert_to_tensor=True)
print(f"ì„ë² ë”© ì°¨ì›: {embeddings.shape}")  # [3, 768]

# í† í”½ ë¶„í¬ ì˜ˆì¸¡
with torch.no_grad():
    topic_distributions = topic_predictor(embeddings)
    print(f"í† í”½ ë¶„í¬ ì°¨ì›: {topic_distributions.shape}")  # [3, 1000]
    print(f"ì²« ë²ˆì§¸ ì‹œì—°ì˜ ìƒìœ„ 5ê°œ í† í”½: {torch.topk(topic_distributions[0], 5)}")
```

### Step 2: í† í”½ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°ê¸° êµ¬í˜„

```python
class TopicCoverageCalculator:
    """í† í”½ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
    
    def __init__(self, topic_predictor: TopicPredictor):
        self.topic_predictor = topic_predictor
        self.topical_knowledge = None  # tÌ‚_LM - ëª¨ë¸ì˜ í† í”½ë³„ ì§€ì‹
        
    def estimate_topical_knowledge(self, demonstrations: List[str], 
                                 embeddings: torch.Tensor, 
                                 zero_shot_accuracies: List[float]):
        """ë…¼ë¬¸ì˜ ìˆ˜ì‹ (5) êµ¬í˜„: í† í”½ë³„ ëª¨ë¸ ì§€ì‹ í‰ê°€"""
        
        with torch.no_grad():
            topic_distributions = self.topic_predictor(embeddings)  # [N, num_topics]
            
        # tÌ‚_LM,t = (âˆ‘_d tÌ‚_d,t Â· zero-shot(d)) / (âˆ‘_d tÌ‚_d,t)
        zero_shot_tensor = torch.tensor(zero_shot_accuracies).unsqueeze(1)  # [N, 1]
        
        # ë¶„ì: weighted sum of zero-shot accuracies
        numerator = torch.sum(topic_distributions * zero_shot_tensor, dim=0)  # [num_topics]
        
        # ë¶„ëª¨: sum of topic weights  
        denominator = torch.sum(topic_distributions, dim=0)  # [num_topics]
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        denominator = torch.clamp(denominator, min=1e-8)
        
        self.topical_knowledge = numerator / denominator  # [num_topics]
        
        print(f"í† í”½ë³„ ëª¨ë¸ ì§€ì‹ ë²”ìœ„: {self.topical_knowledge.min():.3f} ~ {self.topical_knowledge.max():.3f}")
        return self.topical_knowledge
    
    def compute_relevance_score(self, test_embedding: torch.Tensor, 
                              candidate_embedding: torch.Tensor) -> float:
        """ë…¼ë¬¸ì˜ ìˆ˜ì‹ (6) êµ¬í˜„: í† í”½ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜"""
        
        if self.topical_knowledge is None:
            raise ValueError("ë¨¼ì € estimate_topical_knowledge()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”!")
            
        with torch.no_grad():
            # í…ŒìŠ¤íŠ¸ ì…ë ¥ê³¼ í›„ë³´ ì‹œì—°ì˜ í† í”½ ë¶„í¬ ì˜ˆì¸¡
            test_topics = self.topic_predictor(test_embedding.unsqueeze(0))[0]  # [num_topics]
            candidate_topics = self.topic_predictor(candidate_embedding.unsqueeze(0))[0]  # [num_topics]
            
            # r(x, d) = âŸ¨tÌ‚_x âŠ˜ tÌ‚_LM, tÌ‚_dâŸ©
            knowledge_weighted_test = test_topics / torch.clamp(self.topical_knowledge, min=1e-8)
            relevance_score = torch.dot(knowledge_weighted_test, candidate_topics)
            
            return relevance_score.item()

# ì˜ˆì‹œ ì‚¬ìš©ë²•
calculator = TopicCoverageCalculator(topic_predictor)

# ëª¨ì˜ zero-shot ì •í™•ë„ (ì‹¤ì œë¡œëŠ” LLMìœ¼ë¡œ ì¸¡ì •)
zero_shot_scores = [0.85, 0.72, 0.91]  # ê° ì‹œì—°ì— ëŒ€í•œ zero-shot ì„±ëŠ¥

# í† í”½ë³„ ëª¨ë¸ ì§€ì‹ ì¶”ì •
topical_knowledge = calculator.estimate_topical_knowledge(
    demonstrations, embeddings, zero_shot_scores
)

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
test_question = "Non-human organisms that mainly consume plants are known as what?"
test_embedding = embedding_model.encode([test_question], convert_to_tensor=True)[0]

# ê° í›„ë³´ ì‹œì—°ì— ëŒ€í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
for i, demo in enumerate(demonstrations):
    score = calculator.compute_relevance_score(test_embedding, embeddings[i])
    print(f"ì‹œì—° {i+1} ê´€ë ¨ì„± ì ìˆ˜: {score:.3f}")
    print(f"ë‚´ìš©: {demo[:50]}...")
    print()
```

### Step 3: ëˆ„ì  í† í”½ ì»¤ë²„ë¦¬ì§€ êµ¬í˜„

```python
class CumulativeTopicCoverage:
    """ë…¼ë¬¸ì˜ ìˆ˜ì‹ (7) êµ¬í˜„: ëˆ„ì  í† í”½ ì»¤ë²„ë¦¬ì§€ë¡œ ë‹¤ì–‘ì„± ë³´ì¥"""
    
    def __init__(self, topic_predictor: TopicPredictor, embedding_model):
        self.topic_predictor = topic_predictor
        self.embedding_model = embedding_model
        self.selected_demonstrations = []
        self.selected_embeddings = []
        
    def update_coverage(self, new_demonstration: str, new_embedding: torch.Tensor) -> torch.Tensor:
        """ìƒˆë¡œìš´ ì‹œì—° ì¶”ê°€ ì‹œ ì»¤ë²„ë¦¬ì§€ ì—…ë°ì´íŠ¸"""
        
        if len(self.selected_demonstrations) == 0:
            # ì²« ë²ˆì§¸ ì‹œì—°ì¸ ê²½ìš°
            self.selected_demonstrations.append(new_demonstration)
            self.selected_embeddings.append(new_embedding)
            
            with torch.no_grad():
                return self.topic_predictor(new_embedding.unsqueeze(0))[0]
        
        # ì´ì „ ì‹œì—°ë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
        prev_embeddings = torch.stack(self.selected_embeddings)  # [num_prev, embed_dim]
        prev_mean_embedding = torch.mean(prev_embeddings, dim=0)  # [embed_dim]
        
        # ìƒˆ ì‹œì—° í¬í•¨í•œ ì „ì²´ í‰ê·  ì„ë² ë”©
        all_embeddings = torch.stack(self.selected_embeddings + [new_embedding])
        combined_mean_embedding = torch.mean(all_embeddings, dim=0)
        
        with torch.no_grad():
            # ì´ì „ ì»¤ë²„ë¦¬ì§€ì™€ ìƒˆ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
            prev_coverage = self.topic_predictor(prev_mean_embedding.unsqueeze(0))[0]
            combined_coverage = self.topic_predictor(combined_mean_embedding.unsqueeze(0))[0]
            
            # tÌ‚_d â† (tÌ‚_{dâˆªD'_x} - tÌ‚_{D'_x})
            incremental_coverage = torch.clamp(combined_coverage - prev_coverage, min=0)
            
        # ì„ íƒëœ ì‹œì—° ëª©ë¡ ì—…ë°ì´íŠ¸
        self.selected_demonstrations.append(new_demonstration)
        self.selected_embeddings.append(new_embedding)
        
        return incremental_coverage
    
    def get_coverage_diversity_score(self) -> float:
        """í˜„ì¬ ì„ íƒëœ ì‹œì—°ë“¤ì˜ ë‹¤ì–‘ì„± ì ìˆ˜"""
        if len(self.selected_demonstrations) < 2:
            return 0.0
            
        # ëª¨ë“  ì‹œì—° ìŒ ê°„ì˜ í† í”½ ë¶„í¬ ìœ ì‚¬ë„ ê³„ì‚°
        with torch.no_grad():
            all_embeddings = torch.stack(self.selected_embeddings)
            topic_distributions = self.topic_predictor(all_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = F.cosine_similarity(
                topic_distributions.unsqueeze(1), 
                topic_distributions.unsqueeze(0), 
                dim=2
            )
            
            # ëŒ€ê°ì„  ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„ (ë‹¤ì–‘ì„±ì˜ ì—­ì§€í‘œ)
            mask = ~torch.eye(len(self.selected_demonstrations), dtype=bool)
            avg_similarity = similarities[mask].mean()
            
            # ë‹¤ì–‘ì„± ì ìˆ˜ = 1 - í‰ê·  ìœ ì‚¬ë„
            diversity_score = 1.0 - avg_similarity.item()
            
        return diversity_score

# ì˜ˆì‹œ ì‚¬ìš©ë²•
coverage_tracker = CumulativeTopicCoverage(topic_predictor, embedding_model)

# ìˆœì°¨ì  ì‹œì—° ì„ íƒ ì‹œë®¬ë ˆì´ì…˜
candidate_demos = [
    "Herbivores are animals that primarily eat plants",
    "Carnivores hunt and eat other animals for survival",
    "Omnivores eat both plants and animals",
    "Photosynthesis is the process plants use to make food",
    "Food chains show energy flow in ecosystems"
]

# ê° ì‹œì—°ì˜ ëˆ„ì  ê¸°ì—¬ë„ ê³„ì‚°
for i, demo in enumerate(candidate_demos):
    demo_embedding = embedding_model.encode([demo], convert_to_tensor=True)[0]
    incremental_coverage = coverage_tracker.update_coverage(demo, demo_embedding)
    diversity_score = coverage_tracker.get_coverage_diversity_score()
    
    print(f"\nì‹œì—° {i+1} ì¶”ê°€ í›„:")
    print(f"ë‚´ìš©: {demo}")
    print(f"ì¦ë¶„ ì»¤ë²„ë¦¬ì§€ í•©ê³„: {incremental_coverage.sum():.3f}")
    print(f"ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.3f}")
    print(f"ìƒìœ„ í† í”½ ê¸°ì—¬ë„: {torch.topk(incremental_coverage, 3)}")
```

### Step 4: ì „ì²´ TopicK ì‹œìŠ¤í…œ í†µí•©

```python
class TopicKRetriever:
    """TopicK ì „ì²´ ì‹œìŠ¤í…œ êµ¬í˜„"""
    
    def __init__(self, embedding_model_name: str = 'all-mpnet-base-v2', 
                 num_topics: int = 1000):
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.topic_predictor = TopicPredictor(input_dim=768, num_topics=num_topics)
        self.coverage_calculator = None
        self.is_trained = False
        
    def train_topic_predictor(self, demonstrations: List[str], 
                            topic_labels: List[Dict[str, float]], 
                            epochs: int = 100, lr: float = 1e-4):
        """í† í”½ ì˜ˆì¸¡ê¸° í•™ìŠµ - ë…¼ë¬¸ì˜ ìˆ˜ì‹ (4) êµ¬í˜„"""
        
        # ì‹œì—°ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        embeddings = self.embedding_model.encode(demonstrations, convert_to_tensor=True)
        
        # í† í”½ ë ˆì´ë¸”ì„ í…ì„œë¡œ ë³€í™˜
        topic_targets = []
        for labels in topic_labels:
            target = torch.zeros(self.topic_predictor.num_topics)
            for topic_idx, weight in labels.items():
                target[int(topic_idx)] = weight
            topic_targets.append(target)
        
        targets = torch.stack(topic_targets)  # [num_demos, num_topics]
        
        # í•™ìŠµ ì„¤ì •
        optimizer = torch.optim.Adam(self.topic_predictor.parameters(), lr=lr)
        criterion = nn.BCELoss()
        
        self.topic_predictor.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            predictions = self.topic_predictor(embeddings)
            
            # ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ (ë…¼ë¬¸ ìˆ˜ì‹ 4)
            loss = criterion(predictions, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}")
        
        self.is_trained = True
        print("í† í”½ ì˜ˆì¸¡ê¸° í•™ìŠµ ì™„ë£Œ!")
        
    def retrieve_demonstrations(self, test_input: str, 
                              candidate_pool: List[str], 
                              k: int = 8,
                              lambda_weight: float = 0.5) -> List[Tuple[str, float]]:
        """TopicK ë©”ì¸ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜"""
        
        if not self.is_trained:
            raise ValueError("í† í”½ ì˜ˆì¸¡ê¸°ë¥¼ ë¨¼ì € í•™ìŠµì‹œí‚¤ì„¸ìš”!")
        
        # ì„ë² ë”© ìƒì„±
        test_embedding = self.embedding_model.encode([test_input], convert_to_tensor=True)[0]
        candidate_embeddings = self.embedding_model.encode(candidate_pool, convert_to_tensor=True)
        
        # í† í”½ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        if self.coverage_calculator is None:
            # ëª¨ì˜ zero-shot ì •í™•ë„ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLMìœ¼ë¡œ ì¸¡ì •)
            mock_zero_shot = np.random.uniform(0.6, 0.9, len(candidate_pool))
            self.coverage_calculator = TopicCoverageCalculator(self.topic_predictor)
            self.coverage_calculator.estimate_topical_knowledge(
                candidate_pool, candidate_embeddings, mock_zero_shot
            )
        
        # ëˆ„ì  ì»¤ë²„ë¦¬ì§€ ì¶”ì ê¸° ì´ˆê¸°í™”
        coverage_tracker = CumulativeTopicCoverage(self.topic_predictor, self.embedding_model)
        
        selected_indices = []
        remaining_indices = list(range(len(candidate_pool)))
        
        # ë°˜ë³µì  ì„ íƒ (kê°œê¹Œì§€)
        for step in range(k):
            best_score = -float('inf')
            best_idx = None
            
            for idx in remaining_indices:
                # í† í”½ ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜
                topic_score = self.coverage_calculator.compute_relevance_score(
                    test_embedding, candidate_embeddings[idx]
                )
                
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ì ìˆ˜
                semantic_score = F.cosine_similarity(
                    test_embedding.unsqueeze(0), 
                    candidate_embeddings[idx].unsqueeze(0)
                ).item()
                
                # ìµœì¢… ì ìˆ˜: r(x,d) + Î» * cos(e_x, e_d)
                total_score = topic_score + lambda_weight * semantic_score
                
                # ëˆ„ì  ì»¤ë²„ë¦¬ì§€ ê³ ë ¤ (2ë‹¨ê³„ë¶€í„°)
                if step > 0:
                    # í˜„ì¬ê¹Œì§€ ì„ íƒëœ ì‹œì—°ë“¤ê³¼ì˜ í† í”½ ì¤‘ë³µ í˜ë„í‹° ì¶”ê°€
                    coverage_penalty = self._calculate_coverage_penalty(
                        idx, selected_indices, candidate_embeddings, coverage_tracker
                    )
                    total_score -= coverage_penalty
                
                if total_score > best_score:
                    best_score = total_score
                    best_idx = idx
            
            # ìµœê³  ì ìˆ˜ ì‹œì—° ì„ íƒ
            if best_idx is not None:
                selected_indices.append(best_idx)
                remaining_indices.remove(best_idx)
                
                # ì»¤ë²„ë¦¬ì§€ ì—…ë°ì´íŠ¸
                coverage_tracker.update_coverage(
                    candidate_pool[best_idx], 
                    candidate_embeddings[best_idx]
                )
                
                print(f"Step {step+1}: ì„ íƒëœ ì‹œì—° {best_idx} (ì ìˆ˜: {best_score:.3f})")
                print(f"ë‚´ìš©: {candidate_pool[best_idx][:60]}...")
                print()
        
        # ê²°ê³¼ ë°˜í™˜
        results = [(candidate_pool[idx], self.coverage_calculator.compute_relevance_score(
                    test_embedding, candidate_embeddings[idx])) 
                   for idx in selected_indices]
        
        return results
    
    def _calculate_coverage_penalty(self, candidate_idx: int, 
                                   selected_indices: List[int], 
                                   embeddings: torch.Tensor,
                                   coverage_tracker: CumulativeTopicCoverage) -> float:
        """í† í”½ ì¤‘ë³µì— ëŒ€í•œ í˜ë„í‹° ê³„ì‚°"""
        
        if len(selected_indices) == 0:
            return 0.0
        
        with torch.no_grad():
            # í›„ë³´ ì‹œì—°ì˜ í† í”½ ë¶„í¬
            candidate_topics = self.topic_predictor(embeddings[candidate_idx].unsqueeze(0))[0]
            
            # ì´ë¯¸ ì„ íƒëœ ì‹œì—°ë“¤ì˜ í‰ê·  í† í”½ ë¶„í¬
            selected_embeddings = embeddings[selected_indices]
            selected_topics = self.topic_predictor(selected_embeddings)
            avg_selected_topics = torch.mean(selected_topics, dim=0)
            
            # í† í”½ ì˜¤ë²„ë© ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
            overlap = F.cosine_similarity(
                candidate_topics.unsqueeze(0), 
                avg_selected_topics.unsqueeze(0)
            ).item()
            
            # ì˜¤ë²„ë©ì´ ë†’ì„ìˆ˜ë¡ ë†’ì€ í˜ë„í‹°
            penalty = max(0, overlap - 0.3) * 0.5  # ì„ê³„ê°’ 0.3, í˜ë„í‹° ìŠ¤ì¼€ì¼ 0.5
            
        return penalty

# ì „ì²´ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ
def main_example():
    """TopicK ì „ì²´ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    retriever = TopicKRetriever(num_topics=100)  # ì˜ˆì‹œìš©ìœ¼ë¡œ ì‘ì€ í† í”½ ìˆ˜
    
    # í›„ë³´ ì‹œì—° í’€
    candidate_pool = [
        "Herbivores are animals that primarily eat plants and vegetation for energy.",
        "Carnivores are predators that hunt and consume other animals as their main food source.", 
        "Omnivores have a diverse diet that includes both plant and animal matter.",
        "Photosynthesis is the biological process by which plants convert sunlight into chemical energy.",
        "Cellular respiration breaks down glucose to produce ATP energy in living organisms.",
        "Food chains illustrate the flow of energy from producers to various levels of consumers.",
        "Decomposers like bacteria and fungi break down dead organic matter in ecosystems.",
        "Symbiosis describes close relationships between different species that can be beneficial.",
        "Evolution explains how species change and adapt over time through natural selection.",
        "Biodiversity refers to the variety of life forms found in different ecosystems."
    ]
    
    # ëª¨ì˜ í† í”½ ë ˆì´ë¸” ìƒì„± (ì‹¤ì œë¡œëŠ” topic miningìœ¼ë¡œ ìƒì„±)
    topic_labels = []
    for i in range(len(candidate_pool)):
        # ê° ì‹œì—°ë§ˆë‹¤ 3-5ê°œì˜ ê´€ë ¨ í† í”½ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        labels = {}
        num_topics = np.random.randint(3, 6)
        topic_indices = np.random.choice(100, num_topics, replace=False)
        weights = np.random.uniform(0.3, 1.0, num_topics)
        
        for topic_idx, weight in zip(topic_indices, weights):
            labels[str(topic_idx)] = weight
        
        topic_labels.append(labels)
    
    # í† í”½ ì˜ˆì¸¡ê¸° í•™ìŠµ
    print("í† í”½ ì˜ˆì¸¡ê¸° í•™ìŠµ ì¤‘...")
    retriever.train_topic_predictor(candidate_pool, topic_labels, epochs=50)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = "Non-human organisms that mainly consume plants are known as what?"
    
    # ì‹œì—° ê²€ìƒ‰ ì‹¤í–‰
    print(f"\ní…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_question}")
    print("\nTopicKë¡œ ì„ íƒëœ ì‹œì—°ë“¤:")
    print("=" * 60)
    
    selected_demos = retriever.retrieve_demonstrations(
        test_question, candidate_pool, k=5
    )
    
    for i, (demo, score) in enumerate(selected_demos):
        print(f"{i+1}. ê´€ë ¨ì„± ì ìˆ˜: {score:.3f}")
        print(f"   ë‚´ìš©: {demo}")
        print()

if __name__ == "__main__":
    main_example()
```

## ğŸ“Š ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸

```python
def evaluate_topic_coverage(selected_demonstrations: List[str], 
                          topic_predictor: TopicPredictor,
                          embedding_model) -> Dict[str, float]:
    """ì„ íƒëœ ì‹œì—°ë“¤ì˜ í† í”½ ì»¤ë²„ë¦¬ì§€ í’ˆì§ˆ í‰ê°€"""
    
    embeddings = embedding_model.encode(selected_demonstrations, convert_to_tensor=True)
    
    with torch.no_grad():
        topic_distributions = topic_predictor(embeddings)  # [num_demos, num_topics]
        
        # 1. í† í”½ ì»¤ë²„ë¦¬ì§€ (í™œì„±í™”ëœ í† í”½ì˜ ìˆ˜)
        avg_distribution = torch.mean(topic_distributions, dim=0)
        active_topics = (avg_distribution > 0.1).sum().item()  # ì„ê³„ê°’ 0.1
        coverage_ratio = active_topics / topic_distributions.shape[1]
        
        # 2. í† í”½ ë‹¤ì–‘ì„± (ì‹œì—° ê°„ í† í”½ ë¶„í¬ì˜ ë‹¤ì–‘ì„±)
        pairwise_similarities = F.cosine_similarity(
            topic_distributions.unsqueeze(1),
            topic_distributions.unsqueeze(0), 
            dim=2
        )
        # ëŒ€ê°ì„  ì œì™¸
        mask = ~torch.eye(len(selected_demonstrations), dtype=bool)
        avg_similarity = pairwise_similarities[mask].mean().item()
        diversity_score = 1.0 - avg_similarity
        
        # 3. í† í”½ ì—”íŠ¸ë¡œí”¼ (ë¶„í¬ì˜ ê· ë“±ì„±)
        topic_entropy = -torch.sum(avg_distribution * torch.log(avg_distribution + 1e-8)).item()
        
        # 4. í† í”½ ì§‘ì¤‘ë„ (ìƒìœ„ í† í”½ë“¤ì´ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘)
        sorted_topics, _ = torch.sort(avg_distribution, descending=True)
        top_k_ratio = sorted_topics[:10].sum().item()  # ìƒìœ„ 10ê°œ í† í”½ì˜ ë¹„ì¤‘
        
    return {
        "topic_coverage_ratio": coverage_ratio,
        "topic_diversity": diversity_score, 
        "topic_entropy": topic_entropy,
        "top_k_concentration": top_k_ratio
    }

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì˜ˆì‹œ
def monitor_performance():
    """í•™ìŠµ ë° ê²€ìƒ‰ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    print("=== ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ===")
    
    # í† í”½ ì˜ˆì¸¡ê¸° ì„±ëŠ¥ ì²´í¬
    sample_demos = [
        "Herbivores eat plants",
        "Carnivores eat meat", 
        "Plants use photosynthesis"
    ]
    
    embedding_model = SentenceTransformer('all-mpnet-base-v2')
    topic_predictor = TopicPredictor(num_topics=100)
    
    metrics = evaluate_topic_coverage(sample_demos, topic_predictor, embedding_model)
    
    print("í† í”½ ì»¤ë²„ë¦¬ì§€ ë©”íŠ¸ë¦­:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.3f}")
    
    # ì˜ˆìƒ ì„±ëŠ¥ ê¸°ì¤€ì 
    print("\nì˜ˆìƒ ì„±ëŠ¥ ê¸°ì¤€:")
    print("  topic_coverage_ratio: > 0.15 (15% ì´ìƒì˜ í† í”½ í™œì„±í™”)")
    print("  topic_diversity: > 0.3 (30% ì´ìƒì˜ ë‹¤ì–‘ì„±)")
    print("  topic_entropy: > 2.0 (ì ì ˆí•œ ë¶„í¬ ê· ë“±ì„±)")
    print("  top_k_concentration: < 0.8 (ê³¼ë„í•œ ì§‘ì¤‘ ë°©ì§€)")

if __name__ == "__main__":
    monitor_performance()
```

## ğŸ¯ ìµœì í™” íŒê³¼ ì‹¤ë¬´ ê³ ë ¤ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ìµœì í™”
```python
# ëŒ€ìš©ëŸ‰ í›„ë³´ í’€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
def batch_process_embeddings(texts: List[str], batch_size: int = 32):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì„ë² ë”© ìƒì„±"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_embeddings = embedding_model.encode(batch, convert_to_tensor=True)
        embeddings.append(batch_embeddings)
    return torch.cat(embeddings, dim=0)
```

### 2. ì†ë„ ìµœì í™”
```python
# ìƒìœ„ 300ê°œ í›„ë³´ë¡œ ì‚¬ì „ í•„í„°ë§ (ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰)
def fast_prefiltering(test_embedding: torch.Tensor, 
                     candidate_embeddings: torch.Tensor, 
                     top_k: int = 300) -> torch.Tensor:
    """ë¹ ë¥¸ ì‚¬ì „ í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ê³µê°„ ì¶•ì†Œ"""
    similarities = F.cosine_similarity(
        test_embedding.unsqueeze(0), candidate_embeddings
    )
    _, top_indices = torch.topk(similarities, min(top_k, len(similarities)))
    return top_indices
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ
```python
hyperparams = {
    "learning_rate": [1e-5, 1e-4, 1e-3],          # í† í”½ ì˜ˆì¸¡ê¸° í•™ìŠµë¥ 
    "lambda_weight": [0.1, 0.5, 1.0],             # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜  
    "coverage_threshold": [0.1, 0.2, 0.3],        # í† í”½ í™œì„±í™” ì„ê³„ê°’
    "diversity_penalty": [0.3, 0.5, 0.8]          # ë‹¤ì–‘ì„± í˜ë„í‹° ê°•ë„
}
```

ì´ êµ¬í˜„ ê°€ì´ë“œë¥¼ í†µí•´ TopicKì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì‹¤ì œ ì‘ë™í•˜ëŠ” ì½”ë“œë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!